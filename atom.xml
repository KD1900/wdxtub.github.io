<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小土刀</title>
  <subtitle>Agony is my triumph</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wdxtub.com/"/>
  <updated>2016-12-08T15:59:08.000Z</updated>
  <id>http://wdxtub.com/</id>
  
  <author>
    <name>wdxtub</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【血战钢锯岭】再救一个</title>
    <link href="http://wdxtub.com/2016/12/08/hacksaw-ridge/"/>
    <id>http://wdxtub.com/2016/12/08/hacksaw-ridge/</id>
    <published>2016-12-08T14:44:06.000Z</published>
    <updated>2016-12-08T15:59:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>『看到战友的笑容，我觉得这就是最好的回报』。</p>
<a id="more"></a>
<hr>
<p><img src="/images/14812111986039.jpg" alt=""></p>
<p>作为从小在部队大院长大的孩子，对于战争片有着天然的执着，这种感觉可能只能跟同在部队大院长大的孩子共享。还记得小时候早晚必有的号声，虽然早已因为『扰民』而取消，但时至今日，听到类似的集结号，还是会有心潮澎湃的感觉。</p>
<p>考虑到我是坚定的无神论者，对影片中的信仰问题就不做过多的讨论，不过无论信什么，只要不是倒行逆施的信仰，我至少是不反对的。</p>
<p>先前觉得电影节奏有些拖沓，但仔细想想，前面的爱情戏码越是老派越是温情，才更能凸现后面战争的残忍。尤其是男主的懵懂青涩，真的让人有种回到过去的感觉。而训练营的部分就有点用力不足了，我感觉完全可以缩短十分钟，很多桥段有说不出的怪异感，作为战争部分的预热，显然是有些暖场气温不够的。</p>
<p>但是战争戏一下子把前面的温情与矛盾完全抹去，血与肉，爆炸与火焰，简单粗暴，爽吗？并不爽，反而让人对战争产生深深的厌恶，我见过的最可怕的战争场景大概就是这里了，几乎全程都紧紧抓着外套捂着嘴吧。</p>
<p>正当我纳闷怎么表现男主拯救世界的时候，日军气势汹汹的反扑配合上无差别炮火掩护，留下了各种伤兵和清理战场的日军。于是，战地最强『奶妈』就登场了。但即使是这样『硬点』的英雄，在第一次完成救人之后，仍旧是迷茫的。我想任何课本和学校都无法教会人们如何去面对这样的情景。</p>
<p>于是男主每次喊出『再一个』『再一个』『再一个』的时候，我都特别为他捏一把汗，毕竟《美国狙击手》最后的结局实在太令人难过了。</p>
<p>钢锯岭拿下来了，英雄也凯旋归来了。</p>
<p>我在回家的路上不由得暗念：好男儿报国在今朝！</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;『看到战友的笑容，我觉得这就是最好的回报』。&lt;/p&gt;
    
    </summary>
    
      <category term="Movie" scheme="http://wdxtub.com/categories/Movie/"/>
    
    
      <category term="战争" scheme="http://wdxtub.com/tags/%E6%88%98%E4%BA%89/"/>
    
      <category term="信仰" scheme="http://wdxtub.com/tags/%E4%BF%A1%E4%BB%B0/"/>
    
  </entry>
  
  <entry>
    <title>计算机科学名人堂</title>
    <link href="http://wdxtub.com/2016/12/07/cs-hall-of-fame/"/>
    <id>http://wdxtub.com/2016/12/07/cs-hall-of-fame/</id>
    <published>2016-12-07T13:29:07.000Z</published>
    <updated>2016-12-08T14:38:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>纵观计算机发展历史，可真是『天才引导的历程』，一路走来，星光熠熠。本文介绍计算机科学中那些可以入选『名人堂』的人物。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.12.07: 完成初稿</li>
</ul>
<p>收集了计算机领域比较重要的各大奖项的历年获奖人，大家可以从他们的工作中发现时代发展的轨迹。</p>
<h2 id="图灵奖得主"><a href="#图灵奖得主" class="headerlink" title="图灵奖得主"></a>图灵奖得主</h2><p>图灵奖(ACM A.M. Turing Award)是计算机协会（ACM）于1966年设立的奖项，专门奖励对计算机事业作出重要贡献的个人。其名称取自世界计算机科学的先驱、英国科学家、英国曼彻斯特大学教授艾伦·图灵（A.M. Turing），这个奖设立目的之一是纪念这位现代计算机科学的奠基者。获奖者必须是在计算机领域具有持久而重大的先进性的技术贡献。大多数获奖者是计算机科学家。是计算机界最负盛名的奖项，有“计算机界诺贝尔奖”之称。</p>
<ul>
<li>1966年 - 艾伦·佩利(Alan J. Perlis) - 高级程序设计技巧，编译器构造</li>
<li>1967年 - 莫里斯·威尔克斯(Maurice V. Wilkes) - 存储程序式计算机EDSAC，程序库</li>
<li>1968年 - 理查德·卫斯里·汉明(Richard Hamming) - 数值方法，自动编码系统，错误检测和纠错码</li>
<li>1969年 - 马文·闵斯基(Marvin Minsky) - 人工智能</li>
<li>1970年 - 詹姆斯·维尔金森(James H. Wilkinson) - 数值分析，线性代数，倒退错误分析</li>
<li>1971年 - 约翰·麦卡锡(John McCarthy) - 人工智能</li>
<li>1972年 - 艾兹格·迪科斯彻(Edsger Dijkstra) - 程序设计语言的科学与艺术</li>
<li>1973年 - 查理士·巴赫曼(Charles W. Bachman) - 数据库技术</li>
<li>1974年 - 高德纳(Donald E. Knuth) - 算法分析、程序设计语言的设计、程序设计</li>
<li>1975年 - 艾伦·纽厄尔(Allen Newell) 赫伯特·西蒙(Herbert A. Simon) - 人工智能，人类认知心理学和列表处理</li>
<li>1976年 - 迈克尔·拉宾(Michael O. Rabin) 达纳·斯科特(Dana S. Scott) - 非确定性自动机</li>
<li>1977年 - 约翰·巴克斯(John Backus) - 高级编程系统，程序设计语言规范的形式化定义</li>
<li>1978年 - 罗伯特·弗洛伊德(Robert W. Floyd) - 设计高效可靠软件的方法学</li>
<li>1979年 - 肯尼斯·艾佛森(Kenneth E. Iverson) - 程序设计语言和数学符号，互动系统的设计，运用 APL 进行教学，程序设计语言的理论与实践</li>
<li>1980年 - 东尼·霍尔(C. Antony R. Hoare) - 程序设计语言的定义与设计</li>
<li>1981年 - 埃德加·科德(Edgar F. Codd) - 数据库系统，尤其是关系型数据库</li>
<li>1982年 - 史提芬·库克(Stephen A. Cook) - 计算复杂度</li>
<li>1983年 - 肯·汤普逊(Ken Thompson) 丹尼斯·里奇(Dennis M. Ritchie) - UNIX 操作系统和 C 语言</li>
<li>1984年 - 尼古拉斯·沃斯(Niklaus Wirth) - 程序设计语言设计、程序设计</li>
<li>1985年 - 理查德·卡普(Richard M. Karp) - 算法理论，尤其是 NP-完全性理论</li>
<li>1986年 - 约翰·霍普克罗夫特(John Hopcroft) 罗伯特·塔扬(Robert Tarjan) - 算法和数据结构的设计与分析</li>
<li>1987年 - 约翰·科克(John Cocke) - 编译理论，大型系统的体系结构，及精简指令集（RISC）计算机的开发</li>
<li>1988年 - 伊凡·苏泽兰(Ivan Sutherland) - 计算机图形学</li>
<li>1989年 - 威廉·卡亨(William Morton Kahan) - 数值分析</li>
<li>1990年 - 费尔南多·考巴托(Fernando J. Corbató) - CTSS 和 Multics</li>
<li>1991年 - 罗宾·米尔纳(Robin Milner) - LCF，ML语言，CCS</li>
<li>1992年 - 巴特勒·兰普森(Butler W. Lampson) - 分布式，个人计算环境</li>
<li>1993年 - 尤里斯·哈特马尼斯(Juris Hartmanis) 理查德·斯特恩斯(Richard E. Stearns) - 计算复杂度理论</li>
<li>1994年 - 爱德华·费根鲍姆(Edward Feigenbaum) 拉吉·瑞迪(Raj Reddy) - 大规模人工智能系统</li>
<li>1995年 - 曼纽尔·布卢姆(Manuel Blum) - 计算复杂度理论，及其在密码学和程序校验上的应用</li>
<li>1996年 - 阿米尔·伯努利(Amir Pnueli) - 时序逻辑，程序与系统验证</li>
<li>1997年 - 道格拉斯·恩格尔巴特(Douglas Engelbart) - 互动计算</li>
<li>1998年 - 詹姆斯·尼古拉·格雷(James Gray) - 数据库与事务处理</li>
<li>1999年 - 弗雷德里克·布鲁克斯(Frederick P. Brooks,Jr.) - 计算机体系结构，操作系统，软件工程</li>
<li>2000年 - 姚期智(Andrew Chi-Chih Yao) - 计算理论，包括伪随机数生成，密码学与通信复杂度</li>
<li>2001年 - 奥利-约翰·达尔(Ole-Johan Dahl) 克利斯登·奈加特(Kristen Nygaard) - 面向对象编程</li>
<li>2002年 - 罗纳德·李维斯特(Ronald L. Rivest) 阿迪·萨莫尔(Adi Shamir) 伦纳德·阿德曼(Leonard M. Adleman) - 公钥密码学（RSA加密算法）</li>
<li>2003年 - 艾伦·凯(Alan Kay) - 面向对象编程</li>
<li>2004年 - 文特·瑟夫(Vinton G. Cerf) 罗伯特·卡恩(Robert E. Kahn) - TCP/IP协议</li>
<li>2005年 - 彼得·诺尔(Peter Naur) - Algol 60 语言</li>
<li>2006年 - 法兰西斯·艾伦(Frances E. Allen) - 优化编译器</li>
<li>2007年 - 爱德蒙·克拉克(Edmund M. Clarke) 艾伦·爱默生(Allen Emerson) 约瑟夫·斯发基斯(Joseph Sifakis) - 开发自动化方法检测计算机硬件和软件中的设计错误</li>
<li>2008年 - 芭芭拉·利斯科夫(Barbara Liskov) - 编程语言和系统设计的实践与理论</li>
<li>2009年 - 查尔斯·萨克尔(Charles Thacker) - 帮助设计、制造第一款现代PC</li>
<li>2010年 - 莱斯利·瓦伦特(Leslie Valiant) - 对众多计算理论所做的变革性的贡献\</li>
<li>2011年 - 犹大·伯尔(Judea Pearl) - 人工智能</li>
<li>2012年 - 沙菲·戈德瓦塞尔(Shafi Goldwasser) 西尔维·奥麦克林(Silvio Micali) - 由于在密码学和复杂理论领域做出创举性工作</li>
<li>2013年 - 莱斯利·兰伯特(Leslie Lamport) - 在提升计算机系统的可靠性及稳定性领域的杰出贡献</li>
<li>2014年 - 迈克尔·斯通布雷克 - 对现代数据库的概念和实践作出的根本性贡献</li>
<li>2015年 - 惠特菲尔德·迪菲(Whitfield Diffie) 马丁·赫尔曼(Martin E. Hellman) - 发明迪菲-赫尔曼密钥交换，对公开密钥加密技术有重大贡献</li>
</ul>
<h2 id="王选奖得主"><a href="#王选奖得主" class="headerlink" title="王选奖得主"></a>王选奖得主</h2><p>2005年设立。旨在表彰计算机领域取得重大理论、技术突破或获得重大科研成果的个人，是为纪念已故的王选院士而设。　CCF王选奖奖励的人应具备如下条件之一：</p>
<ol>
<li>在基础研究或应用基础研究方面有得到国内外同行公认的重大理论突破或做出原创性的研究成果；</li>
<li>有重大的技术发明或技术突破，通过原理性样机或系统，展示了新思想、新技术或新方法，被后来的研究者采纳，并最终影响了工业界。 </li>
<li>把新技术首次应用于某一领域，并在应用中效果显著，得到了一定的推广，形成了对产业或社会的重要影响。</li>
<li>在与信息化建设相关的计算机系统重大工程方面取得了创新性成果，产生了良好的经济或社会效益。</li>
</ol>
<p>历年得主</p>
<ul>
<li>2006年 - 胡伟武（龙芯2号增强型通用处理器） 赵有健（IPv6核心路由器） 朱建生（中国铁路客票发售和预订系统v5.0项目）</li>
<li>2007年 - 应明生（通讯并发系统中的拓扑结构、随机性与噪音） 唐卫清（PDSOFT 计算机辅助工厂设计系统）</li>
<li>2008年 - 孙育宁（信息设备资源共享协同服务国际标准研制）</li>
<li>2009年 - 孟小峰（纯XML数据库技术研究）</li>
<li>2010年 - 高文（北京大学教授 音视频研究、标准的制定和应用） 刘积仁（东北大学教授 软件新技术研发和研究成果在行业领域的推广与应用）</li>
<li>2011年 - 廖湘科（国防科技大学教授 高性能计算机和操作系统的研究及应用） 孙凝晖（中科院计算所研究员 集群高性能计算机领域的研究和应用）</li>
<li>2012年 - 刘庆峰（安徽科大讯飞 计算机语音技术领域的创新努力和突出贡献）</li>
<li>2013年 - 李彦宏（百度 全球最大的中文搜索引擎） 李晓明（北京大学教授 搜索引擎理论方面）</li>
<li>2014年 - 赵沁平（北京航空航天大学教授 建立了我国第一个基于广域网络的分布式虚拟环境） 雷军（小米科技董事长兼CEO 在手机硬件、人机交互方面有大量创新）</li>
<li>2015年 - 李建中（哈尔滨工业大学教授 海量数据的计算研究） 刘迎建（汉王科技公司 手写汉字识别技术）</li>
<li>2016年 - 李星（清华大学教授 互联网和下一代互联网科技领域的主要开拓者）</li>
</ul>
<h2 id="计算机先驱奖得主"><a href="#计算机先驱奖得主" class="headerlink" title="计算机先驱奖得主"></a>计算机先驱奖得主</h2><p>计算机先驱奖(Computer Pioneer Award)是 IEEE 计算机协会于 1981 年设立的奖项，兼顾了理论与实践，设计与工程实现，硬件与软件，系统与部件。奖励那些在概念真正流行起来的 15 年前就开始做出巨大贡献的人。</p>
<ul>
<li>1981年<ul>
<li>Jeffrey Chuan Chu - 电子计算机逻辑设计</li>
</ul>
</li>
<li>1982年<ul>
<li>Harry D. Huskey - 并行计算机 SWAC</li>
<li>Arthur Burks - 电子计算机逻辑设计</li>
</ul>
</li>
<li>1984年<ul>
<li>John Vincent Atanasoff - 有序串行内存的电子计算机</li>
<li>Jerrier A. Haddad - IBM 701</li>
<li>Nicholas C. Metropolis - 使用 ENIAC 解决了核能问题</li>
<li>Nathaniel Rochester - IBM 702 的架构师</li>
<li>Willem L. van der Poel - 串行电脑 ZEBRA</li>
</ul>
</li>
<li>1985年<ul>
<li>John G. Kemeny - BASIC 语言</li>
<li>John McCarthy - LISP 语言与人工智能</li>
<li>Alan Perlis    - 计算机语言翻译</li>
<li>Ivan Sutherland - 图像化绘图板</li>
<li>David J. Wheeler - 汇编语言编程</li>
<li>Heinz Zemanek - Mailüfterl 计算机及编程语言</li>
</ul>
</li>
<li>1986年<ul>
<li>Cuthbert C. Hurd - 计算</li>
<li>Peter Naur - 计算机语言开发</li>
<li>James H. Pomerene - IAS 和 Harvest 电脑</li>
<li>Adriann van Wijngaarden - ALGOL 68 </li>
</ul>
</li>
<li>1987年<ul>
<li>Robert E. Everett - Whirlwind</li>
<li>Reynold B. Johnson - RAMAC</li>
<li>Arthur L. Samuel - 自适应非数值处理</li>
<li>Niklaus E. Wirth    - Pascal 语言</li>
</ul>
</li>
<li>1988年<ul>
<li>Friedrich L. Bauer - 计算机栈</li>
<li>Marcian E. Hoff, Jr. - 芯片上的微处理器</li>
</ul>
</li>
<li>1989年<ul>
<li>John Cocke    - 指令流水线和 RISC 概念</li>
<li>James A. Weidenhammer - 高速 I/O 机制</li>
<li>Ralph L. Palmer - IBM 604 电子计算器</li>
<li>Mina S. Rees &amp; Marshall C. Yovits &amp; F. Joachim Weyl &amp; Gordon D. Goldstein - 自 1946 年开始进行的 ONR 计算机研发</li>
</ul>
</li>
<li>1990年<ul>
<li>Werner Buchholz - 计算机架构</li>
<li>C.A.R. Hoare - 编程语言定义</li>
</ul>
</li>
<li>1991年<ul>
<li>Bob O. Evans - 兼容电脑</li>
<li>Robert W. Floyd - 编译器</li>
<li>Thomas E. Kurtz - BASIC 语言</li>
</ul>
</li>
<li>1992年<ul>
<li>Stephen W. Dunwell - Project stretch</li>
<li>Douglas C. Engelbart - 人机交互</li>
</ul>
</li>
<li>1993年<ul>
<li>Erich Bloch    - 高速计算</li>
<li>Jack S. Kilby - 合作发明了集成电路</li>
<li>Willis H. Ware - 设计了 IAS 和 JOHNNIAC 电脑</li>
</ul>
</li>
<li>1994年<ul>
<li>Gerrit A. Blaauw    - IBM System/360 系列</li>
<li>Harlan B. Mills - 结构化编程</li>
<li>Dennis M. Ritchie &amp; Ken L. Thompson - Unix</li>
</ul>
</li>
<li>1995年 <ul>
<li>Gerald Estrin - 早期计算机</li>
<li>David Evans - 计算机图形学</li>
<li>Butler Lampson - 个人电脑</li>
<li>Marvin Minsky - 人工智能</li>
<li>Kenneth Olsen - 迷你电脑</li>
</ul>
</li>
<li>1996年（今年主要是冷战结束后给老对手颁发安慰奖）<ul>
<li>Angel Angelov - 保加利亚的计算机科学</li>
<li>Richard F. Clippinger - 在 Aberdeen Proving Ground 把 ENIAC 转化成一个程序</li>
<li>Edgar Frank Codd - 数据库管理的抽象模型</li>
<li>Norber Fristacky - 数码设备</li>
<li>Victor M. Glushkov - 计算机架构的数字自动化</li>
<li>Jozef Gruska - 计算理论和有组织的活动</li>
<li>Jiri Horejs - 信息论与计算机科学</li>
<li>Lubomir Georgiev Iliev - 第一个保加利亚的计算机，抽象数学和软件</li>
<li>Robert E. Kahn - TCP/IP 协议和 Internet</li>
<li>László Kalmár - 1956 逻辑机器，在匈牙利设计 MIR 电脑</li>
<li>Antoni Kilinski - 波兰的商业化电脑，大学的计算机科学</li>
<li>László Kozma - 1930 relay machines, 战前匈牙利的早期电脑</li>
<li>Sergey A. Lebedev - 苏联的计算机事业</li>
<li>Alexey A. Lyaponov - 苏联的控制论和编程</li>
<li>Romuald W. Marczynski - 波兰的数字计算机和计算机架构</li>
<li>Grigore C. Moisil -  逻辑交换电路</li>
<li>Ivan Plander - 斯洛伐克的计算机硬件技术</li>
<li>Arnold Reitsakas    - Estonia’s computer age</li>
<li>Antonin Svoboda - 捷克斯洛伐克的计算机研究</li>
</ul>
</li>
<li>1997年<ul>
<li>Homer (Barney) Oldfield - 银行应用 ERMA 和计算机制造</li>
<li>Francis Elizabeth (Betty) Snyder-Holberton - Univac 电脑的排序生成器</li>
</ul>
</li>
<li>1998年<ul>
<li>Irving John (Jack) Good - Colossus 和 Manchester Mark I</li>
</ul>
</li>
<li>1999年<ul>
<li>Herbert Freeman - SPEEDAC of Sperry Corporation，计算机图形学和图像处理</li>
</ul>
</li>
<li>2000年<ul>
<li>Harold W. Lawson - 指针变量的发明</li>
<li>Gennady Stolyarov - Minsk 系列电脑的软件</li>
<li>Georgy Lopato - Minsk 系列电脑的硬件</li>
</ul>
</li>
<li>2001年<ul>
<li>Vernon Schatz - 电子资金转移，使得银行可以进行数字交易</li>
<li>William H. Bridge - GE DATANET-30 计算机和交流技术</li>
</ul>
</li>
<li>2002年<ul>
<li>Per Brinch Hansen - 操作系统和并发编程</li>
<li>Robert W. Bemer - ASCII 码和转义序列</li>
</ul>
</li>
<li>2003年<ul>
<li>Martin Richards - 系统软件可移植化</li>
</ul>
</li>
<li>2004年<ul>
<li>Frances E. Allen - 编译器优化的理论与实践</li>
</ul>
</li>
<li>2006年<ul>
<li>Mamoru Hosaka - 日本的计算事业</li>
<li>Arnold M. Spielberg - 实时数据获取和记录</li>
</ul>
</li>
<li>2008年<ul>
<li>Betty Jean Jeanings Bartik - ENAIC 的程序员</li>
<li>Edward J. McCluskey - 五十年来设计综合数字系统</li>
<li>Carl A. Petri - Petri 网络理论 (1962) 和并行/分布式计算</li>
</ul>
</li>
<li>2009年<ul>
<li>Jean E. Sammet - 最早的编程语言开发者</li>
<li>Lynn Conway - 对超标量架构的突出贡献，包括多发射动态指令调度</li>
</ul>
</li>
<li>2011年<ul>
<li>David Kuck - 并行架构</li>
</ul>
</li>
<li>2012年<ul>
<li>Cleve Moler    - 创造 MATLAB</li>
</ul>
</li>
<li>2013年<ul>
<li>Edward Feigenbaum - 知识库系统的开发和应用</li>
</ul>
</li>
<li>2014年<ul>
<li>Linus Torvalds - Linux 内核和开源</li>
</ul>
</li>
<li>2015年<ul>
<li>Michael J. Flynn    - 超过五十多年的 TCCA 和 SIGARCH 的老领导 </li>
<li>Peter M. Kogge - 多核处理器开发和计算机控制流水线</li>
</ul>
</li>
<li>2016年<ul>
<li>E. Grady Booch - 对象建模和 UML</li>
</ul>
</li>
</ul>
<h2 id="计算机历史博物馆-Fellow-Award-得主"><a href="#计算机历史博物馆-Fellow-Award-得主" class="headerlink" title="计算机历史博物馆 Fellow Award 得主"></a>计算机历史博物馆 Fellow Award 得主</h2><p>奖励那些改变世界的人，包括：计算、网络、软件、硬件、存储、编程语言等等</p>
<ul>
<li>1987年<ul>
<li>Grace Murray Hopper - 开发编程语言</li>
</ul>
</li>
<li>1995年<ul>
<li>Jay W. Forrester - 内存技术，早期计算机系统设计和开发</li>
</ul>
</li>
<li>1996年<ul>
<li>Mitch Kapor - 开发 Lotus 1-2-3</li>
<li>Ken Olsen - DEC 公司的共同创始人，为 Minicomputer 做出贡献</li>
</ul>
</li>
<li>1997年<ul>
<li>John Backus - FORTRAN，计算机系统理论和软件管理</li>
<li>Dennis Ritchie &amp; Ken Thompson - UNIX 和 C</li>
</ul>
</li>
<li>1998年<ul>
<li>Gene Amdahl - 为计算机架构和设计做的基本的工作，项目管理和领导力</li>
<li>Donald Knuth - 算法，TeX 语言和在数学和计算机科学领域的贡献</li>
<li>Gordon Moore - 设计和制造半导体设备，Fairchild 和 Intel 的共同创始人</li>
<li>Steve Wozniak - Apple 共同创始人，设计了 Apple I</li>
</ul>
</li>
<li>1999年<ul>
<li>Alan Kay - 个人电脑和人机界面</li>
<li>John McCarthy - 人工智能，分时系统</li>
<li>Konrad Zuse - 发明了第一个程序控制的高级语言 Plankalkul</li>
</ul>
</li>
<li>2000年<ul>
<li>Frances Allen - 并行电脑程序编译和优化</li>
<li>Vinton Cerf - 创造和发展了 Internet</li>
<li>Tom Kilburn - 随机访问存储，虚拟内存和 Multiprogramming</li>
</ul>
</li>
<li>2001年<ul>
<li>Frederick P. Brooks - 计算机架构，操作系统，软件工程</li>
<li>Jean Sammet - 编程语言及其历史</li>
<li>Sir Maurice V. Wilkes - 早期计算机设计</li>
</ul>
</li>
<li>2002年<ul>
<li>John Cocke - 精简指令集，程序优化</li>
<li>Charles Geschke &amp; John Warnock- 商业电脑，计算机图形和打印</li>
<li>Carver Mead - 自动化</li>
</ul>
</li>
<li>2003年<ul>
<li>Gordon Bell - 微型电脑的进化</li>
<li>Sir Tim Berners-Lee - 开发 WWW</li>
<li>David Wheeler - 架构设计，计算机测试</li>
</ul>
</li>
<li>2004年<ul>
<li>Erich Bloch - 工程管理</li>
<li>Dan Bricklin &amp; Bob Frankston- VisiCalc 电子表单</li>
<li>Bob Evans - 硬件，软件项目管理</li>
<li>Niklaus Wirth - 编程语言核算法</li>
</ul>
</li>
<li>2005年<ul>
<li>Paul Baran - Internet 架构</li>
<li>Douglas C. Engelbart - 人机交互，发明鼠标</li>
<li>Alan F. Shugart - 现代磁盘驱动器</li>
<li>Ivan E. Sutherland - Sketchpad 应用，计算机图形学</li>
</ul>
</li>
<li>2006年<ul>
<li>Sir Antony Hoare - 发明快速排序，编程语言</li>
<li>Robert Kahn - 在科学研究中利用网络</li>
<li>Butler Lampson - 工作站，操作系统，计算机安全，文档发布</li>
<li>Marvin Minsky - 人工智能，神经网络，机器人</li>
</ul>
</li>
<li>2007年<ul>
<li>Morris Chang - 开发独立半导体制作产业</li>
<li>John Hennessy - 工程教育，计算机架构</li>
<li>David Patterson - 工程教育，计算机架构</li>
<li>Charles (Chuck) Thacker - 领导开发 Xerox PARC Alto</li>
</ul>
</li>
<li>2008年<ul>
<li>Jean Bartik - 在 ENIAC 上编程</li>
<li>Bob Metcalfe - 发明、标准化核商业化以太网</li>
<li>Linus Torvalds - 创造 Linux 内核，开源</li>
</ul>
</li>
<li>2009年<ul>
<li>Donald Chamberlin - SQL 与数据库架构</li>
<li>Robert Everett - MIT Whirlwind 和 SAGE 计算机系统</li>
<li>Federico Faggin &amp; Marcian Hoff &amp; Stan Mazor &amp; Masatoshi Shima - 开发 Intel 4004，世界上第一个商业处理器</li>
</ul>
</li>
<li>2011年<ul>
<li>Whitfield Diffie &amp; Martin Hellman &amp; Ralph Merkle - 公钥加密</li>
<li>Bill Joy - BSD Unix 系统</li>
</ul>
</li>
<li>2012年<ul>
<li>Fernando Corbato - 分时系统，Multics 操作系统</li>
<li>Edward Feigenbaum - 人工智能，专家系统</li>
<li>Steve Furber &amp; Sophie Wilson- ARM 处理器架构</li>
</ul>
</li>
<li>2013年<ul>
<li>Edwin Catmull - 计算机图形，动画，电影制作</li>
<li>Harry D. Huskey - 计算系统</li>
<li>Robert W. Taylor - 计算机网络，在线信息和交流系统 </li>
</ul>
</li>
<li>2014年<ul>
<li>Lynn Conway - 集成电路设计</li>
<li>John Crawford - 工业标准微处理器架构</li>
<li>Irwin Jacobs - 数字电话通信技术</li>
</ul>
</li>
<li>2015年<ul>
<li>Charles W. Bachman - 早期数据库系统开发</li>
<li>Evelyn Berezin - 计算机设计</li>
<li>Bjarne Stroustrup - 创造 C++ 编程语言</li>
</ul>
</li>
<li>2016年<ul>
<li>David Cutler - 计算机架构，编译器，操作系统，软件工程</li>
<li>Lee Felsenstein - 早期个人电脑时代的影响力</li>
<li>Philip Moorby - 发明和提高 Verilog 这一硬件描述语言</li>
</ul>
</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/Computer_Pioneer_Award" target="_blank" rel="external">Computer Pioneer Award</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E5%9B%BE%E7%81%B5%E5%A5%96" target="_blank" rel="external">图灵奖</a></li>
<li><a href="http://www.computerhistory.org/fellowawards/hall/" target="_blank" rel="external">Hall of Fellows</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;纵观计算机发展历史，可真是『天才引导的历程』，一路走来，星光熠熠。本文介绍计算机科学中那些可以入选『名人堂』的人物。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="计算机" scheme="http://wdxtub.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="名人" scheme="http://wdxtub.com/tags/%E5%90%8D%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>【小土刀的播客】零 Podcast 制作指南</title>
    <link href="http://wdxtub.com/2016/12/07/podcast-guide/"/>
    <id>http://wdxtub.com/2016/12/07/podcast-guide/</id>
    <published>2016-12-07T05:22:33.000Z</published>
    <updated>2016-12-07T13:18:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>我筹备一年多的播客项目终于要启动了！</p>
<a id="more"></a>
<hr>
<h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>播客(Podcast)是类似广播的网络声讯节目，如果博客(Blog)是文字的游戏，那么播客(Podcast) 则是声音的游戏。五花八门的节目类型，真的是用耳朵去『看』世界。</p>
<p>最初的想法其实是做一档访谈视频节目，找我不同阶段的同学作嘉宾，内容是各自从前对生活的想法，现在的生活和未来的展望。一方面增进了同学感情，一方面大家都可以知道彼此在想什么，可能很多年之后回头看，是特别有趣的事儿。</p>
<p>如果说博客依赖的就是键盘，那么一步跨越到视频节目，绝对是会拉伤的。首先至少得有俩机位吧，得有俩麦克风吧，得有个场地吧，得有一俩工作人员吧。这么一想，现阶段基本是没有机会开展了。</p>
<p>但是播客就不同了，音频毕竟还是要比视频好处理多了，也没有太多时空上的限制，哪怕网络不稳定什么的，后期剪辑一下就好。什么场地啊化妆啊灯光啊都可以省了，就我一个业余主持人兼工作人员，还是凡事从简得好。</p>
<p>因为形式的转变，内容也有些变化。不过变来变去，都是一个非常『个人化』的节目。会聊我的童年、少年、青年，会找各行各业的朋友来聊聊自己的生活和工作。一直觉得，让年轻人了解其他人在做什么，了解不同的生活可能性是特别有意义的事情。世界给我们的条条框框已经很多，希望能在仅有的空间中，活得更自由些。</p>
<h2 id="一份不靠谱的播客制作指南"><a href="#一份不靠谱的播客制作指南" class="headerlink" title="一份不靠谱的播客制作指南"></a>一份不靠谱的播客制作指南</h2><p>注：本指南非常随性，更加详细靠谱的可以在参考链接中找到。</p>
<p>在这个硬件软件网络成本都很低的时代，做一『』播客节目基本没有什么难度，但是做一『档』播客节目就很难了，因为一档节目最需要的恰恰是无论哪个时代都很难的东西：坚持、知识与见解。</p>
<p>那么我打算怎么做呢？</p>
<ul>
<li>名字：第一步，节目名，Logo 和宣传语得定一下</li>
<li>主题：需要能够持续产生话题的内容。录之前理清思路<ul>
<li>回忆专辑：我和我的小伙伴的童年少年青年小学中学大学研究生</li>
<li>身边专辑：我和我的程序员小伙伴的日常吐槽</li>
<li>九零后专辑：我和我的九零后朋友们聊聊各行各业的工作</li>
</ul>
</li>
<li>时长：剪辑后约 45 - 60 分，不然会被说太短吧</li>
<li>结构：每期节目的具体内容编排<ul>
<li>片头（简单自我介绍，节目基本信息，联系方式，捐赠地址等等）</li>
<li>如果有听众来信，那么选读</li>
<li>如果有微博评论，那么选读</li>
<li>嘉宾自我介绍（我也许会插嘴吐槽）</li>
<li>正题部分（就是随意跟着我事先列的简略提纲聊一聊）</li>
<li>总结（总结下中心思想，升华下主题）</li>
<li>片尾（暂时还没想好）</li>
</ul>
</li>
<li>硬件：iPhone + 原配的耳机</li>
<li>网络存储：荔枝 FM</li>
<li>编辑器：GarageBand</li>
<li>运营：随缘运营法<ul>
<li>公众号：搞一下，还是要推送一下的</li>
<li>节目邮箱：会申请一个的</li>
<li>捐赠网址：就直接链接到我的博客了</li>
<li>微博：直接用我自己的微博</li>
<li>抽奖：那还是要抽的，不过准备礼物和发货有点略麻烦</li>
<li>时间轴：每期节目对应一篇博客，里面会附上节目内容的相关链接</li>
<li>周末更新：看心情</li>
<li>关注热点：考虑下，一般来说不关注</li>
<li>浏览各类信息：不浏览</li>
<li>阅读量：不在意</li>
<li>粉丝数量：不在意</li>
<li>交朋友：在意，希望能交到更多各行各业的朋友</li>
</ul>
</li>
</ul>
<p>这么看来，只要没有太多杂念，做个播客节目还是不难滴。</p>
<h2 id="推荐播客"><a href="#推荐播客" class="headerlink" title="推荐播客"></a>推荐播客</h2><p>我一直在寻找一个解决方案，来利用好上班/上学通勤的这段时间，毕竟每天也有一两个小时呢。阅读文字条件不允许，歌曲总有听厌的时候，网络电台早上都是天气交通新闻。一番寻寻觅觅，播客可谓完美解决了这个问题。</p>
<p>转眼间接触播客也快五年了，和身边朋友聊天的时候发现其实这还是颇为小众的信息源。考虑自己目前也正在筹备播客节目，所以希望能有更多的朋友来听听看，感受另一种信息获取的方式，说不定就喜欢上了呢？</p>
<p>均可在 Podcast 应用中搜索并订阅。</p>
<p><strong>内核恐慌</strong></p>
<blockquote>
<p>《内核恐慌》(Kernel Panic) 是由 IPN 出品、吴涛和 Rio 做的播客，首播于 2014 年 10 月。号称硬核，可也没什么干货。想听的人听，不想听的人就别听。</p>
</blockquote>
<p>吴涛和 Rio 大概是我最喜欢的节目主持人了！声音很好听，跑题能力也是一流，但是字里行间能听出来对技术满满的热情与追求。节目中聊的话题五花八门，有种老司机带带我的感觉，从键盘到字体，从文件系统到如何找女朋友，从编码到 WWDC。总而言之，如果你是一个自认为比较有趣的技术人，那么内核恐慌绝对不应该错过。</p>
<p><strong>太医来了</strong></p>
<blockquote>
<p>《太医来了》由 IPN 出品、由前骨科医生初洋和妇产科医生田吉顺主持，是中文互联网第一档医生谈话类播客。节目里没有老专家讲养生，只有几个医生聊聊医院里的事儿，顺便给大家做做科普。</p>
</blockquote>
<p>田吉顺是知乎上的高质量的答主之一，如果你觉得光看图文回答不过瘾，那么这档节目就是听医生说事儿的最佳选择。医疗和健康问题，还是要多去听听看医生怎么说，而不是听养生节目忽悠。</p>
<p><strong>味之道</strong></p>
<blockquote>
<p>《味之道》是由 IPN 出品、由席妙雅主持的美食播客，首播于 2014 年 6 月。美味是感性的体验，但同时需要开放的心态和眼界，以及精准的判断力。用味道来检验美食，不谈风月，不聊情怀。</p>
</blockquote>
<p>作为一个吃货，怎么吃，怎么好吃，都是必修课！如果平日工作太忙，不妨就听听专业人士的说法，毕竟咱们还可以按图索骥刻舟求剑嘛（这里贬义褒用）</p>
<p><strong>机核网 GADIO 游戏广播</strong></p>
<blockquote>
<p>国内首家游戏网络电台 最新最全的游戏新闻 热门游戏报道 游戏心得分享 以及业内最专业的游戏人访谈 涵盖所有游戏主机</p>
</blockquote>
<p>非常接地气且专业的游戏节目！素质高话题有趣涉猎范围广，唯一低的就是节操！如果你喜欢游戏，那么从这里可以找到游戏背后更有趣的故事。</p>
<p><strong>Teahour.fm</strong></p>
<blockquote>
<p>Teahour.fm 由Terry，玎玎, Daniel, Kevin 和滚滚主持，会专注程序员感兴趣的话题，包括 Web 设计和开发，移动应用设计和开发，创业以及一切 Geek 的话题。</p>
</blockquote>
<p>这一档界面虽然更新比较慢，但是基本每次都会请 IT/互联网业界的大牛来分享经验和历程，对于我这种初出茅庐的新兵蛋子来说，简直就是快速升级的十全大补丸。唯一的问题在于主持人有点多，我常常听着听着就混乱了，建议还是带上嘉宾三人以内比较好。</p>
<p><strong>一刻 talks</strong></p>
<blockquote>
<p>「一刻talks」正在邀请世界上的思想领袖与实干家来分享他们的事业、故事创意和想法，这些来自世界各地不同领域的专业人才和深藏不露的绝妙素人，将带来极具前瞻性的话题，把思想精华浓缩在15分钟，讲述知识与创意的精髓，碰撞思想的火花。目前已累积1500则以上的精彩演讲，主题包罗万象，科学家、艺术家、哲学家、探险家、心理学家、商业领袖等各路有想法的人纷纷登台，观点“响亮”，要给世界一点言色！</p>
</blockquote>
<p>简单来说，可以理解为音频版的 TED，我一直觉得，即使是做技术，也需要有视野和大局观，说不定就发现新世界了呢对不对！</p>
<p><strong>静雅思听</strong></p>
<blockquote>
<p>我们将城市大众，尤其是白领年轻人关心的一些热点问题，生活性、知识性及思想性的内容， 以MP3的形式提供给公众，让城市听众免费从我们的网站上下载下来，放在电脑或MP3中，在自己方便的时候收听，在所有现代城市人适合听，但不适合看的场合来听。内容涉及 ：历史、军事、健康保健、法制、情感、文化、汽车、旅游、生活常识、经济管理、 心理、医疗、营养、职业发展、教育、地理、科技等各个方面</p>
</blockquote>
<p>静雅思听是我最早接触的播客之一，也是我听得最久，甚至会专门下载来收藏的唯一的播客。内容非常丰富，天南海北五花八门，基本上来说，是听不完的，不过挑自己感兴趣的就好嘛！如果硬要说缺点，就是有些太专业了，像节目，不像前面几个是主持人唠唠嗑的感觉。播客这个事儿丰俭由人，挑自己喜欢的就好</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://www.qdaily.com/cooperation/articles/yidian/32869.html" target="_blank" rel="external">长假第 5 天，感觉现充的点份播客制作指南</a></li>
<li><a href="https://zhuanlan.zhihu.com/haohaoshuohua" target="_blank" rel="external">好好说话</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我筹备一年多的播客项目终于要启动了！&lt;/p&gt;
    
    </summary>
    
      <category term="Life" scheme="http://wdxtub.com/categories/Life/"/>
    
    
      <category term="播客" scheme="http://wdxtub.com/tags/%E6%92%AD%E5%AE%A2/"/>
    
      <category term="好望角" scheme="http://wdxtub.com/tags/%E5%A5%BD%E6%9C%9B%E8%A7%92/"/>
    
  </entry>
  
  <entry>
    <title>【你的名字。】忘却</title>
    <link href="http://wdxtub.com/2016/12/04/your-name/"/>
    <id>http://wdxtub.com/2016/12/04/your-name/</id>
    <published>2016-12-04T08:42:35.000Z</published>
    <updated>2016-12-04T09:54:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>梦里相逢人不见，若知是梦何须醒。纵然梦里常幽会，怎比真如见一回。我很开心，新海诚终于学会讲故事了，动人的故事。</p>
<a id="more"></a>
<hr>
<p><img src="/images/14808417657233.jpg" alt=""></p>
<p>依然是招牌式的天空，回顾新海诚的几部重要作品，就会发现十几年间，一切都已改变，一切也未曾改变。《星之声(2002)》的成功使得新海诚崭露头角；两年后《云的彼端，约定的地方(2004)》却只能碎片化无头绪地讲故事；三年后《秒速五厘米(2007)》干脆放弃长篇故事，用三小节讲述了生活的焦虑和选择，回归现实并取得了重大成功；四年后《追逐繁星的孩子(2011)》却魔幻现实过了头，像是宫崎骏和庵野秀明没睡醒弄出来的；两年后《言叶之庭》终于说了一个起承转合的故事，但是一直以来的时空感没有了。</p>
<p>终于《你的名字。》把所有的一切融合了起来，用新海诚自己的话说就是：『每次制作新作品的时候，我都要想，这必须是一部此刻的自己做不出来的作品，并且又要想，这部作品如今的我肯定能制作出来。我想将这十几年经历的事，感受的东西，都在这部作品中反映出来。到了成年人的年纪，就想将自己曾经得到的东西，送给与曾经的自己相似的人。』</p>
<p>看完之后不得不祝贺，新海诚做到了，找到了自己的方向，叙事清晰，节奏感也很好，为难自己，却不再折磨观众。</p>
<blockquote>
<p>隐约雷鸣 阴霾天空 但盼风雨来 能留你在此<br>隐约雷鸣 阴霾天空 即使天无雨 我亦留此地</p>
</blockquote>
<p>以《万叶集》为引子，前作中我特别喜欢的语文老师在这个名为糸守的小镇中教着短歌，这次的主题是『黄昏之时』，黑板上写着的是『长月黄昏后，伫立露沾身；莫问我为谁，我自待伊人』。</p>
<p>糸守镇的特产是结绳，代表着时间的流动，扭曲、缠绕、还原、连接，也正是一直以来新海诚作品的主题：分离、找寻与相逢。</p>
<p>交换身体，平行世界，时空流转。星空，云海，匆匆行人，鳞次栉比的高楼，往来穿梭的电车。还是熟悉的配方，还是原来的味道。</p>
<p>但这次又有些不同，从《云的彼端，约定的地方》中的高塔、《秒速五厘米》中的樱花、《追逐繁星的孩子》中的宝石，到《言叶之庭》中的鞋子，贯穿作品系列的意象又多了一个——跨越时空的结绳发带。新海诚终于把前几作精彩的部分抽出来，融合到了一个完整的故事中，要是我一个人在家看，恐怕是早已泪如雨下吧。</p>
<p>最后说三个小细节吧。</p>
<ol>
<li>三叶去供奉的身体和《追逐繁星的孩子》中的生死门非常类似，都是巨大圆环中央。正当我以为又会是一个生死相隔的故事的时候，还是通过平行世界圆了我想要大团圆的梦。但是细细想来，聚散离合生死无常，我们也只能慢慢去学着接受吧。</li>
<li>在彗星碎块摧毁小镇之后，三叶的消失让我想到《合金装备 V 幻痛》中的 Quiet 。她离开之后，曾经的一切痕迹仿佛都像不曾存在过一样。这种曾经的存在都不存在的感觉是非常让人绝望的，就好像自己的一半被暮然抽走一样。没有办法生活在电影里的我们，也只能慢慢去学着接受吧。</li>
<li>从《言叶之庭》开始有意融入的日本传统文化为作品增色不少，直接把意境这一栏填充满了。什么时候我们的电影也能做这样的文化输出呢？</li>
</ol>
<p>翻阅影评时看到了电影宣传小册子上的一段话：『这是一部献给所有正值青春期的年轻人和内心仍旧怀抱青春期残片的大人们的电影。我赌上了自己的全部来完成它的制作，希望大家能乐在其中。』</p>
<p>仿佛我又找回了被丢在角落里快要忘却的信念，要更加满怀憧憬地冲向下一段未知的旅程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;梦里相逢人不见，若知是梦何须醒。纵然梦里常幽会，怎比真如见一回。我很开心，新海诚终于学会讲故事了，动人的故事。&lt;/p&gt;
    
    </summary>
    
      <category term="Movie" scheme="http://wdxtub.com/categories/Movie/"/>
    
    
      <category term="电影" scheme="http://wdxtub.com/tags/%E7%94%B5%E5%BD%B1/"/>
    
      <category term="新海诚" scheme="http://wdxtub.com/tags/%E6%96%B0%E6%B5%B7%E8%AF%9A/"/>
    
      <category term="找寻" scheme="http://wdxtub.com/tags/%E6%89%BE%E5%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>【追逐繁星的孩子】天亮了</title>
    <link href="http://wdxtub.com/2016/12/04/go-after-stars/"/>
    <id>http://wdxtub.com/2016/12/04/go-after-stars/</id>
    <published>2016-12-04T04:33:12.000Z</published>
    <updated>2016-12-04T08:39:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>转型对于任何人来说都不是简单的事情，这是新海诚又一次挑战剧情片，好坏且不说，这种尝试本身就值得肯定。</p>
<a id="more"></a>
<hr>
<p><img src="/images/14808261944849.jpg" alt=""></p>
<p>经历了《云的彼端，约定的地方(2004)》《秒速五厘米(2007)》和《言叶之庭(2013)》的洗礼，这部 2011 年的作品精准而清晰地插入了 2007 和 2013 之间的缝隙。每次新海诚出了剧情片之后，都要出一部文艺片回回血。所以不难想象，《秒速五厘米》大获成功之后，肯定又要来一部剧情片了，这就是本文的主角——《追逐繁星的孩子》。</p>
<p>之前的影评说过『长于细腻的新海诚，似乎一直不太能找到叙事的方法』，吸取了《云的彼端，约定的地方(2004)》的教训，这次总算是把故事说清楚了。但是给人的感觉却不再新海诚了，因为在寻寻觅觅之后才发现，宫崎骏老爷子早已在山的那边很久了。我想，这对于创意工作者来说，是特别难过的事情，就好像《伟大的悲剧》中英国人斯科特到达南极点却发现挪威人阿蒙森早已拔得头筹那样。不过话说回来，所有的初学者不都是从模仿开始的嘛。（插一句：我很高兴在《你的名字》中看到的进步）</p>
<p>实话说，除了标准的新海诚式天空和质量极高的配乐（依然是和天门合作），画风的改变一时间让我有些难以适应。但是随着剧情的展开，那种熟悉的感觉又回来了。然后我意识到，这些感觉已经深深和不同的意象结合起来了，或者说，正是如此才有了『意象』这个词。</p>
<p>熟悉的轨道、电车与火车，一方面代表着被拦住的路，一方面却又代表着通向远方的路。随风摇曳的风铃代表着思念，我却不由自主想到了<a href="http://wdxtub.com/2014/09/11/feng-ling-zhong-de-dao-sheng-clip/">《风铃中的刀声》</a>。曾经的守护者因为失去了『心』而变成了怪物，面目可憎的夷族因为见不得阳光不得不蛰伏在地下，老师和瞬的弟弟最终选择了不停流浪的生活，明日菜却回到了原来的世界。</p>
<p>最令我震撼的其实是明日菜突如其来的那句『只是太寂寞了』。</p>
<p>很直白，很露骨，却没有任何办法反驳。有这样一个帅气的小英雄在危难之中出现，还送上了一个吻，从小独立坚强的明日菜仿佛看到了新世界的亮光，于是再也不肯放下。正因为她孤独，所以才会不顾一切去追寻。看着丧妻的老师即使失去眼睛也要再见妻子一面，才意识到也许所有的执念的根源都来自于寂寞吧。</p>
<p>因为寂寞，所以有了分离、孤独和找寻。生与死的分离，地上与地下的分离，有得必有失；黑暗中蹒跚前进的孤独，爬下通向生死门的孤独，在追逐中消逝；拿着宝石对不可思议音乐的找寻，牺牲一切只求一个结果的找寻，也许只是大梦一场。</p>
<p>突然，幸福和悲伤的感觉都一起涌上来。我才意识到，分离、孤独和找寻固然是重要的主题，但我却一直漏掉了最重要的主题——相逢。</p>
<p>因为向往没有去过的地上，瞬用生命的代价为明日菜留下了一个吻，这是一种相逢。因为对亡妻的思念，老师十年来苦苦追寻终于在生死门前再次见到了夫人，这是一种相逢。因为经历了奇幻且不可思议的冒险，明日菜对着妈妈笑容，也是一种相逢。</p>
<blockquote>
<p>生死门虽繁星灿烂，但活着的人才是最重要。<br>我们会幸福，会痛苦，但一定要活下去，缅怀逝者，珍惜生者，承受着丧失，继续存活，人就是这样的生物啊。我们是如此渺小，可正是因为太过渺小，才要愈发珍惜。</p>
</blockquote>
<p>天亮了。明日菜，你要走了吗？</p>
<p>是的，再见。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转型对于任何人来说都不是简单的事情，这是新海诚又一次挑战剧情片，好坏且不说，这种尝试本身就值得肯定。&lt;/p&gt;
    
    </summary>
    
      <category term="Movie" scheme="http://wdxtub.com/categories/Movie/"/>
    
    
      <category term="告别" scheme="http://wdxtub.com/tags/%E5%91%8A%E5%88%AB/"/>
    
      <category term="电影" scheme="http://wdxtub.com/tags/%E7%94%B5%E5%BD%B1/"/>
    
      <category term="新海诚" scheme="http://wdxtub.com/tags/%E6%96%B0%E6%B5%B7%E8%AF%9A/"/>
    
  </entry>
  
  <entry>
    <title>【云的彼端，约定的地方】醒来</title>
    <link href="http://wdxtub.com/2016/12/04/cloud-other-side/"/>
    <id>http://wdxtub.com/2016/12/04/cloud-other-side/</id>
    <published>2016-12-04T00:31:02.000Z</published>
    <updated>2016-12-04T02:08:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>逆着时间走，秒速五厘米上映的三年前，新海诚披着科幻的皮，还是在诉说那个悲伤却圆满的故事。不由得想起古龙的那句话『故事情节的变化有穷尽时，只有情感的冲突才永远能激动人心』。</p>
<a id="more"></a>
<hr>
<p><img src="/images/14808114689811.jpg" alt=""></p>
<p>从《言叶之庭》到《秒速五厘米》再到《云的彼端，约定的地方》，倒序连着看新海诚的三部作品，才意识到这些所有作品本来就只是他为自己制作的，只是恰好击中了大家心底柔软的部分罢了。这样一想，片中大量的意识流、留白和跳跃也就理所当然了，毕竟在想象的世界中，很多东西一点都不重要，不需要画，也不需要说。</p>
<p>当然，毕竟要上映毕竟要挣钱，所以影片中还是努力想要把故事说清楚的，只是长于细腻的新海诚，似乎一直不太能找到叙事的方法。稍微回顾一下几部作品令人记忆深刻的地方，就会发现凡是涉及稍微长一些的故事主线的时，大约都是以简单的『几年后』一笔带过的。反而是那些小场景的设置，令人拍手叫绝。从《言叶之庭》的雨中的凉亭和楼梯的拐角，到《秒速五厘米》的铁轨和杂货店，再到《云的彼端，约定的地方》的工厂和病房，充盈的细节与恰到好处的节奏把控，像靶靶十环的开弓箭，把记忆中美好的与糟糕的串在一起，在击中靶心的那一刻因为惯性全部融汇在一起，像火箭发射后巨大的烟雾，逐渐消散后，发现天依然很蓝。</p>
<p>《云的彼端，约定的地方》的科幻设定有不少硬伤，或者说，科幻在这里的作用仅仅是为了烘托气氛制造冲突，并不是故事的重点。看到各种吐槽设定的影评时，我反而觉得他们因为不能够接受他人的天马行空而错过了更重要的东西。平行宇宙的故事已经太多，研究所和黑科技也颇有《新世纪福音战士》的感觉，但这些再离谱，也就相当于背景板的几棵树，所有的背景设定都是为了营造一场分离，一种孤独，一次找寻。</p>
<p>分离是什么？也许是《言叶之庭》中无法跨越的师生差距，也许是《秒速五厘米》中因为大雪停运的电车，也许是《云的彼端，约定的地方》中三年时间带来的天翻地覆。</p>
<p>孤独是什么？也许是《言叶之庭》中做好却没法送出去的鞋子，也许是《秒速五厘米》中被风吹走的装满思念的信，也许是《云的彼端，约定的地方》中一个人在平行世界中逡巡。</p>
<p>找寻是什么？也许是《言叶之庭》中每一个期盼下雨的清晨，也许是《秒速五厘米》中没有收件人的短信，也许是《云的彼端，约定的地方》中最终在天空飞翔的自制飞行器。</p>
<p>突然意识到，新海诚只是一直在一次又一次诉说着相同的故事，但是每次依然动人。我不由得想知道这其中的奥秘。最后发现其实哪有什么奥秘，不过是心里装着美好的希冀，自然而然的真情流露罢了。还是古龙写的好：</p>
<blockquote>
<p>中原的四月，正如三月的江南，莺飞草长，正是春光最艳，春色最浓的时候，只可惜这时候春又偏偏已将去了。夕阳最美时，也总是将近黄昏。世上有很多事都是这样子的，尤其是一些特别辉煌美好的事。所以你不必伤感，也不用惋惜，纵然到江湖去赶上了春，也不必留住它。因为这就是人生，有些事你留也留不住。你一定要先学会忍受它的无情，才会懂得享受它的温柔。</p>
</blockquote>
<p>云的彼端，约定的地方是一座高塔。故事的最后，这座高塔却已不复存在了。没有了约定的地方，生活也将重新开始了。约定的从来不是一个地方，而是共同的渴望。</p>
<p>醒来吧！浪子为君歌一曲，劝君切莫把泪流。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://movie.douban.com/review/1928206/" target="_blank" rel="external">剧情详解 - 新海诚从来都喜欢把观众当作已知剧情群体</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;逆着时间走，秒速五厘米上映的三年前，新海诚披着科幻的皮，还是在诉说那个悲伤却圆满的故事。不由得想起古龙的那句话『故事情节的变化有穷尽时，只有情感的冲突才永远能激动人心』。&lt;/p&gt;
    
    </summary>
    
      <category term="Movie" scheme="http://wdxtub.com/categories/Movie/"/>
    
    
      <category term="电影" scheme="http://wdxtub.com/tags/%E7%94%B5%E5%BD%B1/"/>
    
      <category term="新海诚" scheme="http://wdxtub.com/tags/%E6%96%B0%E6%B5%B7%E8%AF%9A/"/>
    
      <category term="科幻" scheme="http://wdxtub.com/tags/%E7%A7%91%E5%B9%BB/"/>
    
  </entry>
  
  <entry>
    <title>【秒速五厘米】One More Time</title>
    <link href="http://wdxtub.com/2016/12/03/speed-5cm-second/"/>
    <id>http://wdxtub.com/2016/12/03/speed-5cm-second/</id>
    <published>2016-12-03T13:15:18.000Z</published>
    <updated>2016-12-03T14:53:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>后知后觉的我，在电影上映了快十年才第一次看了这部作品。也许是心境不同了吧，看到澄田在第二话结尾蜷缩着入睡，我终于能够说服自己，残留着夏天味道的十月下旬，已经永远地过去了。</p>
<a id="more"></a>
<hr>
<p><img src="/images/14807714020993.jpg" alt=""></p>
<p>最初接触新海诚的动画片是《言叶之庭》，缓慢的节奏，绝美的画面，文艺的台词配上抒情的音乐，从各个细节体现出日本人细腻的心思，因此是一部适合一个人慢慢看慢慢品味的电影。或者说，与其说是看电影，更不如说是看自己的记忆，那些埋藏在心底的瞬间，虽然有快乐有痛苦，但是拥有这样的回忆本身，就是一种幸福。</p>
<p>《秒速五厘米》的题材则与《言叶之庭》有很大的不同，虽然刚看了开头就已经猜到了结局，但是第二话『太空人』着实给了我太多太多的惊喜。仿佛看到过去和曾经的那个脸通红心过速的自己，一面想哭，一面想笑。想哭是因为很多事情即使再勇敢再努力也没有办法改变，那不如一开始就深埋心底；想笑是因为那个为了心底认定的事情拼命努力的自己，着实让我很开心。</p>
<p>不由得想起几年前写给自己的一封信，要好的朋友读完后曾说过那是我写得最好的一篇文章。那当然应该是最好的，不是每一篇文章都可以写两三年。想在再读当年的文字，依然有种莫名的亲切感。特别幼稚，特别天真，但是特别开心，那种透过纸面都能感受到的开心。一方面我努力让自己心理变得更强大更踏实，一方面却又憧憬着怀念着那样不知所措心烦意乱的自己。用理性武装自己可以百毒不侵，但却也少了许多趣味。</p>
<p>意识到自己在逐渐『异化』后，便很努力很努力想要进入『人化』的轨道，就好像我的 Github 签名那样——『一个死理性派的人化』。我变得像躲在墙角假装凑巧的澄田，变得像拿到新玩具迫不及待想要分享的小朋友，变得像希望回程的路能够再长一点的学生，变得像走过一个又一个灯柱但依然凑不齐勇气的自己。</p>
<p>很多当时的话，放到现在同样应景，真不知道是该开心还是不开心：</p>
<blockquote>
<p>命运的车轮忘了装刹车，一旦启动，便无法停下，甚至连转变方向，都很困难。于是我们在某一点交汇，在某一点分离。</p>
<p>等待是煎熬。时间却是良药。多么可笑的结论。</p>
<p>满怀希望，却又瞬间落空，才意识到，本是一场空。我在自己编织的梦境里已度过了太久，久得分不清什么是现实，什么是梦境。</p>
</blockquote>
<p>缘深缘浅，相聚别离，后会无期。</p>
<blockquote>
<p>One more time 季节啊 请不要变化<br>One more time 那嬉笑玩闹的岁月</p>
</blockquote>
<p>别给自己太多烦恼，过些日子就会好的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;后知后觉的我，在电影上映了快十年才第一次看了这部作品。也许是心境不同了吧，看到澄田在第二话结尾蜷缩着入睡，我终于能够说服自己，残留着夏天味道的十月下旬，已经永远地过去了。&lt;/p&gt;
    
    </summary>
    
      <category term="Movie" scheme="http://wdxtub.com/categories/Movie/"/>
    
    
      <category term="电影" scheme="http://wdxtub.com/tags/%E7%94%B5%E5%BD%B1/"/>
    
      <category term="新海诚" scheme="http://wdxtub.com/tags/%E6%96%B0%E6%B5%B7%E8%AF%9A/"/>
    
      <category term="爱情" scheme="http://wdxtub.com/tags/%E7%88%B1%E6%83%85/"/>
    
  </entry>
  
  <entry>
    <title>【不周山之计算机基础】计算机网络协议指南</title>
    <link href="http://wdxtub.com/2016/12/03/bzs-network-protocol-guide/"/>
    <id>http://wdxtub.com/2016/12/03/bzs-network-protocol-guide/</id>
    <published>2016-12-03T06:33:06.000Z</published>
    <updated>2016-12-04T00:29:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>如果说计算机科学中最重要的概念是抽象，那么『分层』绝对有资格争榜眼，再不济也是一个探花。计算机网络是最能代表『分层』思想的概念，本文我们就来了解一下计算机网络中的各种协议。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.12.03: 完成初稿</li>
</ul>
<h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><ul>
<li><a href="http://wdxtub.com/2016/12/03/bzs-linux-concept-guide/">Linux 概念指南</a></li>
<li><a href="http://wdxtub.com/2016/12/03/bzs-network-protocol-guide/">计算机网络协议指南</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>了解计算机网络的几种分层模型</li>
<li>了解数据链路层、网络层、传输层和应用层的常见协议及其特性</li>
<li>学会利用命令查看基本的网络信息</li>
</ol>
<h2 id="分层模型"><a href="#分层模型" class="headerlink" title="分层模型"></a>分层模型</h2><p>网络通信是基于分层模型的。虽然不同的协议有不同的层级划分（甚至同一种协议也有不同的层级划分），但是都离不开分层。分层的好处有很多，最重要的是能够比较好控制具体实现的复杂度，由于每层之间由事先约定的接口通信，并不需要关心其他层的实现细节。</p>
<p>比较出名的分层模型有 <a href="https://zh.wikipedia.org/wiki/OSI%E6%A8%A1%E5%9E%8B" target="_blank" rel="external">OSI 分层模型</a>和 <a href="https://zh.wikipedia.org/wiki/TCP/IP%E5%8D%8F%E8%AE%AE%E6%97%8F" target="_blank" rel="external">TCP/IP 分层模型</a>，本文以 TCP/IP 的四层模型进行讲解，他们分别是：</p>
<ul>
<li>网络接口层(link layer): 以太网, Wi-Fi</li>
<li>网络互连层(internet layer): IP, ARP</li>
<li>传输层(transport layer): TCP, UDP, RTP, SCTP</li>
<li>应用层(application layer): HTTP, FTP, DNS</li>
</ul>
<p>下面就开始分别介绍这四层的基本概念和重要的协议。</p>
<h2 id="网络接口层"><a href="#网络接口层" class="headerlink" title="网络接口层"></a>网络接口层</h2><p>网络接口层的基础是光纤、电缆或者电磁波等真实存在的物理媒介。对于数字应用来说，我们只需要两种物理信号来分别表示 0 和 1，比如用高电压表示 1，低电压表示 0，就构成了简单的物理层协议。针对某种媒介，电脑可以有相应的接口，用来接收物理信号，并解读成为 0/1 序列</p>
<p>在这些介质中，信息以『帧(frame)』为单位传输。所谓的帧，是一段有限的 0/1 序列。连接层协议的功能就是识别 0/1 序列中所包含的帧。比如说，根据一定的 0/1 组合识别出帧的起始和结束。在帧中，有收信地址(Source, SRC)和送信地址(Destination, DST)，还有能够探测错误的校验序列(Frame Check Sequence)。当然，帧中最重要的最重要是所要传输的数据(payload)。这些数据往往符合更高层协议，供网络的上层使用。与数据相配套，帧中也有数据的类型(Type)信息。连接层协议不关心数据中到底包含什么。帧就像是一个信封，把数据包裹起来。</p>
<h3 id="深入理解帧-Frame"><a href="#深入理解帧-Frame" class="headerlink" title="深入理解帧(Frame)"></a>深入理解帧(Frame)</h3><p><img src="/images/14808113173928.jpg" alt=""></p>
<p>帧的结构很简单，一共三部分：<code>头部 | 数据(Payload) | 尾部</code>，发送时由头发送到尾，其中：</p>
<ul>
<li>头部: <code>DST SRC Type</code></li>
<li>数据: <code>Payload(Data)</code></li>
<li>尾部: <code>Pad FCS Extension</code></li>
</ul>
<p>更正：<code>Preamble</code> 和 <code>SFD</code> 如上图所示是 packet 的一部分，具体参考 <a href="https://en.m.wikipedia.org/wiki/Ethernet_frame" target="_blank" rel="external">这里</a></p>
<p>我知道看到这些奇奇怪怪的单词大家一定一头雾水，不要紧张，这就来一个一个解释：</p>
<ul>
<li>Preamble: 翻译过来叫序言，是一帧的前 7 个字节，每个字节都是 <code>10101010</code>，所以合起来大概样子是 <code>10101010101010101010101010101010101010101010101010101010</code>。这是要干嘛？对表！为什么要对表！因为不同的网卡发送/接收的频率可能是不一样的，所以就通过这些 1010 来让彼此找到节奏，这个过程叫做时钟复原(recover the clock)</li>
<li>SFD: 起始信号(Start Frame Delimiter)，估计是 <code>0xAB</code></li>
<li>DST: 目的地(Destination)，对方的 MAC 地址（物理设备自带的序号）</li>
<li>SRC: 起始地(Source)，本机的 MAC 地址（只能在同一个以太网中识别）</li>
<li>Type: 用来描述数据部分的类型，如果是 IPv4 则为 <code>0x0800</code>，如果是 ARP 则为 <code>0x0806</code></li>
<li>FCS: 校验序列(Frame Check Sequence)，检测传输过来的数据是否出错，采用 CRC 算法</li>
</ul>
<p>数据部分一般包含更高层协议的数据，比如 IP 包，但是其实具体里面是帧是并不在意的。另外数据尾部可能有一串用来填充的 0，因为数据有一个最小长度限制。</p>
<h3 id="集线器与交换机"><a href="#集线器与交换机" class="headerlink" title="集线器与交换机"></a>集线器与交换机</h3><p>我们现在把想要发送的数据打包成帧，然后要怎么发送呢？老一点的方式是通过集线器，而目前基本上都是通过交换机。</p>
<p>集线器的每个端口都会连接一台计算机，一旦有一台电脑把帧发送到集线器，就会自动转发到其他所有的端口，由每台计算机进行检测，如果不是 DST 的话，就忽略这一帧。这种机制实际上有两个问题：</p>
<ol>
<li>两台计算机的通信在同一个以太网上是公开的，甚至都不需要去截获，帧自己就会推送过来。虽然信息本身可以加密，但是能够知道发送信息的时间和规律，也并不是特别安全的做法</li>
<li>因为每次都需要广播消息，所以实际上集线器不能同时发送信息，一旦冲突，就等待一段时间再进行发送</li>
</ol>
<p>交换机解决了这两个问题，一是记录各个设备的 MAC 地址，按需发送的同时也允许多路同时通信。正因为如此，交换机取代集线器也就非常理所当然了。</p>
<h2 id="网络互联层"><a href="#网络互联层" class="headerlink" title="网络互联层"></a>网络互联层</h2><p>如果我们想让一台连接 Wifi 的计算和一台连接以太网的计算机通信，只使用网络接口层的『帧』是万万做不到的，因为有线和无线并没有办法进行信号的传输，所有我们需要一个桥梁，也就是路由器(router)。路由器可以在 Wifi 和 以太网之间发送/接收数据，这样一来就把中间欠缺的环节补上了。但是还有一个问题，我们来看看整个通信过程：</p>
<p>Computer1(Wifi) &lt;-&gt; Router(Wifi) &lt;-&gt; Router(Ethernet) &lt;-&gt; Computer2(Ethernet)</p>
<p>这里需要四个地址！但是一个帧里面只有 SRC(起点)和 DST(终点)的地址，所以我们就需要在前面的数据(payload)中添加信息，来完成信息的传输，于是便有了 IP 协议。</p>
<h3 id="IP-协议"><a href="#IP-协议" class="headerlink" title="IP 协议"></a>IP 协议</h3><p>网络互联层是互联网最重要的根基，通过几乎一统的 IP 协议构成了我们今天的互联网。更高层的协议，无论是 TCP 还是 UDP，必须通过网络层的 IP 数据包(datagram)来传递信息。操作系统也会提供该层的 socket，从而允许用户直接操作 IP 包。</p>
<p>IP 协议现在有 IPv4 和 IPv6 两个版本，比如下面就是 IPv4 的格式：</p>
<p><img src="/images/14551392855125.jpg" alt=""></p>
<p>其中 source 和 destination 的长度都是 4 字节，我们通常会把每个字节转化成一个 0-255 的十进制整数，记为类似 192.168.0.1 这样的形式。</p>
<p>IP地址是全球地址，它可以识别局域网和主机。这是通过将IP地址分类实现的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">IP class    From          To                Subnet Mask</div><div class="line">A           1.0.0.0       126.255.255.255    255.0.0.0</div><div class="line">B           128.0.0.0     191.255.255.255    255.255.0.0</div><div class="line">C           192.0.0.0     223.255.255.255    255.255.255.0</div></pre></td></tr></table></figure>
<p>每个 IP 地址的 32 位分为前后两部分，第一部分用来区分局域网，第二个部分用来区分该局域网的主机。子网掩码(Subnet Mask)告诉我们这两部分的分界线，比如 255.0.0.0 (也就是8个1和24个0)表示前 8 位用于区分局域网，后 24 位用于区分主机。由于 A、B、C 分类是已经规定好的，所以当一个IP地址属于 B 类范围时，我们就知道它的前 16 位和后 16 位分别表示局域网和主机。</p>
<p>而在具体传输的时候，需要依赖路由器中的路由表，在不同机器间传递的时候，改变的只有帧中的 SRC 和 DST，而 IP 包的内容在帧的 Payload 中是不会变化的。但是现在问题来了，我们怎么知道 IP 和 MAC 的对应关系呢？这里就是 ARP 协议发挥作用的时候了，它会把IP 地址与 MAC 地址的对应传播到局域网的每个主机和路由。而路由表除了可以手动进行编写外，也可以使用 RIP(Routing Information Protocol) 来根据距离进行生成。</p>
<p>IP 协议是 “Best Effort” 式的，也就是说是不可靠的（如果失败了就失败了），但于此同时也是的 IP 协议效率很高，至于更加可靠的传输方式，就要靠传输层实现了。</p>
<p>除了 IP 协议之外，网络互联层一个很重要的协议是 ICMP(Internet Control Message Protocol) 协议，它介于网络层和传输层，主要功能是传输网络诊断信息。我们常用的 <code>ping</code> 命令就是基于 ICMP 协议的。</p>
<h2 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h2><p>现在我们已经能在两台计算机之间进行通信了，但是问题来了，每个计算机有不同的进程，如果我想指定找远方计算机的某个进程要怎么办呢，IP 协议又不够用了。所以传输层协议(TCP, UDP)使用端口号来标注进程，在传输数据的时候，我们写上目的进程的端口。当数据到达另一台计算机时，会根据传输层协议，识别端口号，将信送给不同的进程。</p>
<p>传输层最重要的协议是 TCP 协议和 UDP 协议。TCP 协议复杂但传输可靠，UDP 协议简单但传输不可靠，接下来我们来简单了解一下这俩协议。</p>
<h3 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h3><p>UDP 协议其实并没有在 IP 协议的基础上做太多额外的工作，主要是为 IP 协议增加了端口的支持，头部如下：</p>
<p><img src="/images/14551394375473.jpg" alt=""></p>
<p>从上图就可以看到，UDP 的头部很短，只是额外包含了端口信息，可以看作是 IP 协议暴露在传输层上的接口。</p>
<h3 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h3><p>TCP(Transportation Control Protocol)协议与 IP 协议是一同产生的，或者说，他俩一开始就是同一个协议，后来才被拆开的。TCP 因为实现了『流』式通信，在特定场景中非常有用。具体『流』式通信的实现这里就不展开了，我们来看看 TCP 协议的头部大概就知道它要比 UDP 复杂多少了：</p>
<p><img src="/images/14551394884094.jpg" alt=""></p>
<p>在 TCP 传输数据之前，需要先进行连接，这个过程就是我们常说的 TCP 三次握手(three-way handshaking)。而结束联结的时候也需要进行双方互相确认的工作，就是我们常说的 TCP 四次挥手。</p>
<p>TCP 是一个非常复杂的协议，感兴趣的同学可以在参考链接中找到非常详细的说明，这里我就不再赘述了。</p>
<h2 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h2><p>传输层协议其实在大部分时候已经能够满足我们的需求，但是随着网络应用的增加，不同类型的应用也慢慢有了自己的一套规则，就是我们所说的应用层协议了，比如 HTTP, FTP, IMAP, DNS 等等。这里我们简单介绍一下 HTTP 协议。</p>
<p>HTTP 使用的是『请求(request)』-『回复(response)』机制，客户端向服务器发送请求，而服务器给客户端发送回复。具体回复的状态会在返回的信息中以状态码的形式进行表示。早期发送请求的时候，都需要重新进行 TCP 连接，网络开销很大，同时每次的通信也是无状态的(stateless)。现在 HTTP 协议允许 TCP 链接复用，但是这种无状态的特性依然保留了下来。</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>访问 wdxtub.com，看看在这个过程中数据是如何传输的</li>
<li>试着 ping 一些地址，看看返回的信息是什么</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文中我们简单了解了计算机网络的基本概念和常用协议，如果有需要，会在后面的系列中进行深入讲解。</p>
<p>（写此文时精神状态不佳，振作后再进行修订）</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://read.douban.com/column/1788114/" target="_blank" rel="external">协议森林 - 趣话网络协议</a></li>
<li><a href="http://www.cnblogs.com/vamei/p/3784866.html" target="_blank" rel="external">Mac OS X 网络诊断命令</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果说计算机科学中最重要的概念是抽象，那么『分层』绝对有资格争榜眼，再不济也是一个探花。计算机网络是最能代表『分层』思想的概念，本文我们就来了解一下计算机网络中的各种协议。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="网络协议" scheme="http://wdxtub.com/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>【不周山之计算机基础】Linux 概念指南</title>
    <link href="http://wdxtub.com/2016/12/03/bzs-linux-concept-guide/"/>
    <id>http://wdxtub.com/2016/12/03/bzs-linux-concept-guide/</id>
    <published>2016-12-03T00:19:09.000Z</published>
    <updated>2016-12-03T06:37:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>如果说计算机科学的三大浪漫是操作系统、编译原理和计算机图形学的话，谈及操作系统，Linux 就一定是那个程序员对它又爱又恨的存在。本文带大家了解 Linux 中的基本概念和原理，正所谓知其然也要知其所以然。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.12.03: 初稿完成</li>
</ul>
<h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><ul>
<li><a href="http://wdxtub.com/2016/12/03/bzs-linux-concept-guide/">Linux 概念指南</a></li>
<li><a href="http://wdxtub.com/2016/12/03/bzs-network-protocol-guide/">计算机网络协议指南</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>了解 Linux 的基本概念</li>
<li>理解 Linux 的架构和背后的设计思考</li>
<li>初步掌握文件系统的操作和原理</li>
<li>对管道、进程和进程间通信有简单的感性认识</li>
</ol>
<h2 id="当我们谈论-Linux-时我们在谈论什么"><a href="#当我们谈论-Linux-时我们在谈论什么" class="headerlink" title="当我们谈论 Linux 时我们在谈论什么"></a>当我们谈论 Linux 时我们在谈论什么</h2><p>Linux 的出现其实是一位大学生的心血来潮，Linus Torvalds（就是 Linux 之父）不满意当年学习操作系统时所使用的 Minix 系统，在其代码的基础上参考 Unix 的设计，写出了第一版 Linux 内核。之后 Linus 开源了代码，随着网络时代的大幕逐渐揭开，Linux 和 GNU 金风玉露一相逢，在开源协议下迅速发展成熟。</p>
<p>正所谓『不懂 Unix 的人注定最终还要重复发明一个蹩脚的 Unix』，通过学习 Linux 来掌握 Linux/Unix 的核心思想其实是非常有意义的。作为诸多天才的智慧结晶，能够从中偷师一星半点，也能受益匪浅。（新闻插播：2016.12.03 Solaris 操作系统将终止开发）</p>
<p>准确来说，Linux 其实只是一个内核，负责管理硬件和为上层应用提供接口。不过随着 Linux 概念的不断外延，现在提到 Linux，更多是指以 Linux 内核为基础配上各种应用的 Linux 发行版本（比如 Ubuntu, Debian 等等）。这个系列的文章不会过多着眼于各个发行版，而是专注于 Linux 内核和系统基本概念本身，比如操作系统中重要的抽象：文件系统、输入输出操作、进程、线程和进程间通信。</p>
<h2 id="常见-Linux-发行版简介"><a href="#常见-Linux-发行版简介" class="headerlink" title="常见 Linux 发行版简介"></a>常见 Linux 发行版简介</h2><p>因为 Linux 开源的特性，各种不同的发行版层出不穷，感兴趣的同学可以在 <a href="https://zh.wikipedia.org/wiki/Linux%E5%8F%91%E8%A1%8C%E7%89%88%E5%88%97%E8%A1%A8" target="_blank" rel="external">维基百科 - Linux 发行版列表</a> 这个条目中看到各式各样的发行版及简介，也可以在 <a href="http://distrowatch.com/" target="_blank" rel="external">distrowatch.com</a> 查看更加详细的排名，这之中比较流行的发行版有：</p>
<ul>
<li>ArchLinux，一个基于 KISS(Keep It Simple and Stupid) 的滚动更新的操作系统。</li>
<li>CentOS，从 Red Hat 发展而来的发行版，由志愿者维护，旨在提供开源的，并与 Red Hat 100%兼容的系统。比较稳定，不用动不动就升级。</li>
<li>Debian，一个强烈信奉自由软件，并由志愿者维护的系统。</li>
<li>Elementary OS：基于 Ubuntu，接口酷似 Mac OS X。</li>
<li>Fedora，是 Red Hat 的社区版，会经常引入新特性进行测试。</li>
<li>Gentoo，一个面向高级用户的发行版，所有软件的源代码需要自行编译。</li>
<li>Linux Mint，从 Ubuntu 派生并与 Ubuntu 兼容的系统。</li>
<li>openSUSE，最初由 Slackware 分离出来，现在由 Novell 维护。用起来还是比较生涩。</li>
<li>Red Hat Enterprise Linux，Fedora 的商业版，由 Red Hat 维护和提供技术支持。</li>
<li>Ubuntu，一个非常流行的桌面发行版，由 Canonical 维护。基本上日常常用的就是它了。</li>
</ul>
<p>因为手头上只有基于 Ubuntu 的虚拟机（包括 Win10 中自带的 Linux），所以接下来的示例都是基于 Ubuntu 14.04 LTS 的。</p>
<h2 id="按下开机键之后"><a href="#按下开机键之后" class="headerlink" title="按下开机键之后"></a>按下开机键之后</h2><p>虽然现在我们使用的云主机基本都已经预装好了 Linux，也不需要自己去操心开机，但是操作系统毕竟不是凭空出现的，了解从按下开机键到操作系统启动之间的过程有助于我们深入理解计算机系统。整个过程的步骤如下：</p>
<ol>
<li>按下开机键</li>
<li><strong>BIOS 步骤</strong>：计算机从主板的 BIOS(Basic Input/Output System) 中读取存储的程序</li>
<li><strong>MBR 步骤</strong>：该程序从存储设备中读取起始的 512 字节数据（称为主引导记录 Master Boot Record, MBR）</li>
<li><strong>Boot Loader 步骤</strong>：MBR 告诉计算机从哪个分区(Partition)来载入引导加载程序(Boot Loader)，Boot Loader 保存了操作系统的相关信息</li>
<li><strong>Kernel 步骤</strong>：Boot Loader 根据所存储的信息加载内核(Kernel)，内核主要的任务是管理计算机的硬件资源</li>
<li><strong>Init 步骤</strong>：内核会为自己预留内存空间，然后进行硬件检测，之后启动 init 进程（1 号进程），之后的操作会由 init 进程来接管</li>
<li><strong>初始化脚本步骤</strong>：如果没有进入单用户模式，就会为操作系统启动做各种初始化工作，包括计算机基本信息、文件系统、硬盘、清理临时文件、设置网络等等</li>
<li><strong>登录步骤</strong>：操作系统准备好之后，我们就可以用用户名和密码登录到计算机中，我们成为了一个用户，属于某个用户组</li>
</ol>
<h2 id="Linux-的架构"><a href="#Linux-的架构" class="headerlink" title="Linux 的架构"></a>Linux 的架构</h2><p>现在我们有了一个可以运行的 Linux 操作系统，具体它是怎么工作的呢？这就要从架构说起了。</p>
<p><img src="/images/14807342792897.jpg" alt=""></p>
<p>最底层是硬件，硬件之上是内核，前面说内核负责管理所有的硬件资源的意思是，所有的计算机操作都需要通过内核传递给硬件。如果接触过硬件的同学一定知道，硬件本身是颇为复杂的，即使有了内核代为管理，仍旧非常繁琐，所以在内核之上我们有了系统调用。我们不需要了解内核和硬件的细节，就可以通过系统调用来操作它们，系统调用是操作系统的最小组成单位，也就是说，计算机能做的所有操作，最终能且仅能分解成已有的系统调用。</p>
<p>我们可以看到，内核实际上是硬件的抽象，而系统调用是内核的抽象，在这之上的 shell 和 library 甚至应用程序其实是更高层次的抽象，正是通过这样一层一层的抽象，计算机才得以发展成为如今这么庞大却简洁的系统。</p>
<p>我们在命令行中输入 <code>man 2 syscalls</code> 就可以浏览系统调用的说明了，顺着列表往下滑，就可以看到一些我们常常使用的命令了，比如 <code>chmod</code>, <code>fork</code>, <code>kill</code> 等等。反应快的同学应该已经意识到了，这些命令不就是我们在 shell 中常常使用的嘛，原来它们就是系统调用！</p>
<p>现在最常用的 shell 叫做 bash，其他诸如 zsh, fish 等也各有各的拥趸。这里要具体说一下 shell 和终端(Terminal)的不同，在大型机时代，终端是一个硬件设备，用来进行输入输出，而随着计算机硬件的发展，终端已经慢慢从实体变成了一个概念。我们打开 Gnome Terminal 的 About 页面，就可以发现下面的介绍是这样写的：</p>
<p><img src="/images/14807358566249.jpg" alt=""></p>
<p>注意这个说法 “A terminal emulator for the GNOME desktop”，什么是 emulator 呢？中文翻译叫做仿真器，等于是说，这个程序是一个仿真终端的程序。与 emulator 相关的一个非常容易混淆的概念是 simulator（模拟器），他们的差别在于：</p>
<ul>
<li>仿真器。通过软件方式，精确地在一种处理器上仿真另一种处理器或者硬件的运行方式。其目的是完全仿真被仿真硬件在接收到各种外界信息的时候的反应。</li>
<li>模拟器。通过某种手段，来模拟某些东西。不一定要完全正确的原理，追求的只是尽可能的相像。</li>
</ul>
<p>我们找一个 Mac OS 上最流行的终端的介绍来看看，同样会发现，这是一个仿真器：</p>
<p><img src="/images/14807362345090.jpg" alt=""></p>
<p>所以可以这样理解，现代计算中的终端是一个用软件仿真的终端，我们在这上面输入输出的命令会传给具体执行这些命令的 shell 程序，再由 shell 程序执行对应的系统调用。重要的事情说三遍：终端不是 shell，终端不是 shell，终端不是 shell。</p>
<p>因为系统调用是操作系统的最小功能单位，所以一般来说提供的功能是非常零碎的，我们完成一个操作一般需要多个系统调用进行配合，于是 Linux 定义了一些 library（库），将常见的系统调用组合打包成各种功能。如果说系统调用是笔画的话，那么库函数大概就是偏旁部首了。一般来说 Linux/Unix 系统都会有 ISO C 标准库和 POSIX 标准库，用来保证不同平台的兼容性。</p>
<p>在 shell 和 library 的基础上，我们就可以构造各式各样强大的应用了，当然除了这两种方式外，也可以根据需要自己进行系统调用。</p>
<p>至此，我们就简单介绍了 Linux 架构中的各个层级：</p>
<ul>
<li>内核是软硬件的桥梁</li>
<li>系统调用是应用与内核的桥梁，一方面隐藏了内核的复杂性，另一方面提高了应用的可移植性</li>
<li>库实际上是系统调用组成的模块化功能</li>
<li>shell 实际上是一种方便我们操作计算机的机制</li>
</ul>
<p>在图形化界面出现之前，在命令行中输入命令是跟电脑交互的主要方式。而在图形化界面出现这么多年之后，命令行依然扮演者举足轻重的角色，一是因为简单粗暴，二是因为可以方便地自动化流程化。</p>
<h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>简单来说，文件系统是 0 与 1 的逻辑组织形式，常见的抽象是文件和目录。在 Linux 中，文件系统是一个树结构，树的根就是我们常常能看到的根目录 <code>/</code>，每一个分叉表示一个文件夹，如下图所示：</p>
<p><img src="/images/14807377801394.jpg" alt=""></p>
<p>文件名加上从根目录到该文件所在目录的目录名就构成了一个路径。对于目录来说，里面至少会包含两个条目：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">.       指向当前目录</div><div class="line">..      指向父目录</div></pre></td></tr></table></figure>
<p>当一个文件被放入到目录中，实际上就是建立了一个到该文件的硬链接(hard link)，当对这个文件的硬链接数目为零的时候，文件实际上就被删除了。不过现在基本都使用软链接(soft link)，类似于 windows 中的快捷方式，不会影响链接数目。</p>
<p>我们能对文件进行三种操作：</p>
<ul>
<li>读取 Read: 获取数据</li>
<li>写入 Write: 创建新文件或在旧文件中写入数据</li>
<li>运行 Execute: 文件是可执行的二进制代码，那么会被载入内存进行执行</li>
</ul>
<p>但是三种操作都有各自的权限，我们使用 <code>ls -l filename</code> 就可以看到详情，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">wdxtub@ubuntu:~/GO/bin$ ls -l -rwxrwxr-x 1 wdxtub wdxtub 11277064 Sep 14 10:35 bee</div><div class="line">wdxtub@ubuntu:~/GO$ ls -l bin</div><div class="line">drwxrwxr-x 2 wdxtub wdxtub 4096 Sep 14 10:35 bin</div></pre></td></tr></table></figure>
<p>这里简单介绍下各个字段的含义：</p>
<ul>
<li>第一个字符，如果是 <code>-</code> 表示常规文件，如果是 <code>d</code> 表示目录</li>
<li>后面的九个字符表示 owner, owner group 和 other 的权限，rwx 分别代表读取、写入和执行，如果是 <code>-</code> 则表示没有对应的权限</li>
<li>第二列的数字是 hard link 的数目</li>
<li>第三、四列是所属的用户和用户所在的用户组</li>
<li>第五列是文件大小，单位是字节 byte</li>
<li>最后的是上一次写入的时间</li>
</ul>
<p>文件系统的使用基本上就是这些内容，但是这样的一个文件系统到底是怎么实现的呢？这又要从存储设备说起了。前面提到，存储设备的前 512 字节是 MBR，用于开机启动，剩余的空间可能会被分为多个分区(partition)，每个分区有对应的分区表(partition table)来记录分区的相关信息（比如起始位置和分区大小）。需要注意的是，分区表并不保存在该分区中，不然万一分区挂了，连最关键的分区表都找不到了。</p>
<p>每个分区大概的样子是这样的：</p>
<p><img src="/images/14590056997684.jpg" alt=""></p>
<ul>
<li>Boot block 是为计算机启动而准备的，在 MBR 指定启动分区之后，就会把 Boot block 部分的程序读入内存执行。为了方便管理，即使该分区没有操作系统，仍然会预留 Boot block</li>
<li>Super block 存储文件系统的信息，比如类型、inode 数目和数据块的数目</li>
<li>inodes 是文件存储的关键，每个文件对应一个 inode，inode 中包含指向具体数据的指针，读取的时候根据这些指针进行数据读取即可</li>
<li>Data blocks 就是具体的数据了，我们通过 inode 中的指针来进行访问</li>
</ul>
<p>关于 inode 的具体实现细节这里因为篇幅所限就不展开了，会在系列后面的文章中进行介绍。</p>
<h2 id="管道与流"><a href="#管道与流" class="headerlink" title="管道与流"></a>管道与流</h2><p>在 Linux 中 “Everything is a stream of bytes”，用设计模式的话说其实这就是一个数据流导向的设计，信息在不同的应用之间流动，最终成为我们所需要的信息。Linux 在执行程序的时候，会自动打开三个流：</p>
<ul>
<li>标准输入(Standard Input)</li>
<li>标准输出(Standard Output)</li>
<li>标准错误(Standard Error)</li>
</ul>
<p>我们可以按需进行使用。而如果我们想把一个程序的标准输出作为另一个程序的标准输入，就需要使用管道(pipeline)了。而正是因为这样的机制，我们可以把诸多小功能组合成强大的应用，一个简单的例子是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">wdxtub@ubuntu:~$ cat hello.txt welcome to wdxtub.comwdxtub@ubuntu:~$ cat hello.txt | wc -w3</div></pre></td></tr></table></figure>
<h2 id="进程与进程组"><a href="#进程与进程组" class="headerlink" title="进程与进程组"></a>进程与进程组</h2><p>最基础的操作是指令，一堆指令在一起就是程序，而进程就是程序的具体实现，也就是把程序载入到内存中并执行的过程。操作系统的重要功能之一便是对进程进行从摇篮（分配内存空间）到坟墓（回收）的管理。我们先执行如下命令看看 <code>ps -eo pid,comm,cmd</code>（列出全部进程并展示 pid, command 和 cmd 信息）</p>
<p><img src="/images/14807437718754.jpg" alt=""></p>
<p>这里每一行都是一个进程，第一列是 pid，相当于身份证号；第二列是进程的简称；第三列是进程启动时候的命令。如果我们往上滚动，就会找到这样的一行 <code>1 init /sbin/init</code>，这个就是内核建立的唯一一个进程了，剩下的进程都是 init 通过 fork 方式创建的，也就是说，所有的其他进程都是 init 的子进程。</p>
<p>子进程终结的时候会通知父进程进行内存空间的回收，而如果父进程比子进程还早终结，那么这个子进程就会被过继给 init 进程，并由 init 进程通过调用 <code>wait</code> 函数进行回收。如果无法正确回收，那么这个子进程就成为了僵尸进程，所占据的内存空间就无法被访问了。</p>
<p>除了父子进程的关系外，还有一个进程组(process group)的概念：每个进程组中有多个进程，进程组的 pid 由进程组 leader 的 pid 决定。而多个进程组还可以组成一个会话(session)，会话使得前台和后台程序得以展示出来。当我们创建了多个终端窗口，实际上就创建了多个会话，每个会话都有其前台和后台进程。</p>
<h2 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h2><p>前面介绍了进程，但是进程之前如果想要交互怎么办？除了管道之外，有没有其他方法？当然有也必须要有。其中最简单的一种就是信号，所谓信号就是一个整数，一个由进程 A 发送给进程 B 的整数。因为一个整数所能携带的信息量有限，所以一般用于系统管理。</p>
<p>信号的传递机制也很简单，由内核，或者由其他进程经由内核往目标进程发送信号，实际上是在该进程对应的表中写入信号。当进程执行完系统调用退出内核的时候，就会查看这个信号，然后根据信号的不同执行不同的操作。</p>
<p>具体什么整数表示什么意思可以通过 <code>man 7 signal</code> 来查看，常见的有：</p>
<ul>
<li><code>SIGINT</code>: 当键盘按下 CTRL+C 从 shell 中发出信号，信号被传递给 shell 中前台运行的进程，对应该信号的默认操作是中断(INTERRUPT)该进程</li>
<li><code>SIGQUIT</code>: 当键盘按下 CTRL+\ 从 shell 中发出信号，信号被传递给 shell 中前台运行的进程，对应该信号的默认操作是退出(QUIT)该进程</li>
<li><code>SIGTSTP</code>: 当键盘按下 CTRL+Z 从 shell 中发出信号，信号被传递给 shell 中前台运行的进程，对应该信号的默认操作是暂停(STOP)该进程</li>
<li><code>SIGCONT</code>: 用于通知暂停的进程继续</li>
<li><code>SIGALRM</code>: 起到定时器的作用，通常是程序在一定的时间之后才生成该信号</li>
</ul>
<p>上面的介绍说『默认』操作，那么也就意味着我们是可以采取其他操作的，比方说直接无视掉，或者执行我们自定义的操作。</p>
<p>除了信号，消息队列(message queue)和共享内存(shared memory)也可以在进程间进行信息共享。不过因为这种机制比较复杂，尤其是涉及到同步的问题，所以在使用的时候需要多加注意。</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>试着自己安装一个 Linux 系统，尝试只使用终端来完成基本的文件夹查看操作</li>
<li>查看系统当前正在运行的进程</li>
<li>试着给某个进程发送一个信号</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这一讲中我们简单介绍了 Linux 系统中几个比较重要的概念，部分内容可能会比较难理解，这时候就要实际在电脑上试一试，配合关键词进行搜索咯。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://www.catb.org/~esr/writings/cathedral-bazaar/introduction/" target="_blank" rel="external">大教堂和市集(The Cathedral and the Bazaar)</a></li>
<li><a href="http://coolshell.cn/articles/2322.html" target="_blank" rel="external">Unix 传奇(上篇)</a></li>
<li><a href="http://coolshell.cn/articles/2324.html" target="_blank" rel="external">Unix 传奇(下篇)</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果说计算机科学的三大浪漫是操作系统、编译原理和计算机图形学的话，谈及操作系统，Linux 就一定是那个程序员对它又爱又恨的存在。本文带大家了解 Linux 中的基本概念和原理，正所谓知其然也要知其所以然。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="Linux" scheme="http://wdxtub.com/tags/Linux/"/>
    
      <category term="概念" scheme="http://wdxtub.com/tags/%E6%A6%82%E5%BF%B5/"/>
    
  </entry>
  
  <entry>
    <title>第二十五周 - 沧海一声笑</title>
    <link href="http://wdxtub.com/2016/12/02/roar-of-the-ocean/"/>
    <id>http://wdxtub.com/2016/12/02/roar-of-the-ocean/</id>
    <published>2016-12-02T13:32:57.000Z</published>
    <updated>2016-12-02T15:14:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>沧海一声笑，滔滔两岸潮，浮沉随浪只记今朝。苍天笑，纷纷世上潮，谁负谁胜出天知晓。</p>
<a id="more"></a>
<hr>
<p>为了给周记编链接地址，不得不求助词典来翻译『沧海一声笑』这个歌名，出来的结果也是颇让我哭笑不得，有 “See the world indifferently” - 冷眼看世界，有 “Laughter in the sea” - 海中的笑声，甚至还有 “The sea on voice laugh” - 绝对是机器翻译，最后只好选了一个稍微靠谱点的 “Roar of the Ocean”。</p>
<p>冬季运动计划进行中，因为天气变暖所以加上了跑步，于是继续浑身酸痛。跑了一段时间上坡，慢慢也已经习惯了，虽说是朝抵抗力最大的路径走，但蹬地的反作用力其实要比平地来得大，跑起来反而有另一种轻松。考虑到春节临近又要预备着每逢佳节胖三公斤，所以还是得先减一点，算是给自己多一些放纵的空间。</p>
<p>赶在十二月之前发布了博客的 Beta 版，基本上把自己之前所有的文章和资料进行了统一的梳理，把地基打牢了才好在这之上盖更宏伟的建筑。正像编辑所说的那样，编写著作要趁早，在这个过程中可以理清思路进而升华。不过在此之前还是要有一定的阅读量的，所以 kindle 这个东西嘛，早买早悟道呀。</p>
<p>自上个周末起沉浸在自己营造的牢笼中体验了好几天的矛盾与冲突，从前刻意去避免的情感波动在短短几天内似乎全都释放了出来。虽然这个释放的过程非常痛苦，但是起起伏伏反而激荡出了变化，也很高兴能在迷茫中找回自己。很多事情虽然没有那么好，但也没有那么糟。负能量这个东西大抵虚幻，不要做任何判断与决定，少些到处宣泄，其实该走的自然就会走。或者说，其实这样的体验才是最好的认识自己的机会。</p>
<p>睡前抽空看了一本书，叫《系统之美》，说的是输入、存量、输出、反馈这样的『系统』思维模式。换不同的角度去看待世界是蛮有意思的事情，如果用系统的思维来复盘自己的一天，那么早起对应于系统的预防机制，晚睡对应于系统的容错机制。这俩机制都很重要，预防的思路是事先把事情做好，尽量少出错；容错的思路是即使错了没关系，能有办法补救。很多时候在资源受限的情况下必须做出选择，不过我总是倾向于预防机制（看我的代码就更能体现出来了），但转过头来想想，容错也非常重要，以后还是要多多综合考虑。当然，一次就把事情做好是最好的。</p>
<p>临近年底，也开始在思考 2017 年的计划了，我本人是更倾向于去做人工智能相关应用的，毕竟做工程的东西虽然可以磨练技艺，但是说白了还就是几门编程语言几个框架罢了，真正能够对社会产生巨大价值的是激发智能本身。当然在这个过程中还要继续为计算机基础教育添砖加瓦，让更多的同学能找到最适合自己的学习道路。</p>
<p>我一直觉得，把自己想做的事情说出来，告诉身边的人，让更多的人知道，除了可以更好的鞭策自己外，还可能吸引到更多志同道合的人，也许别人的小小建议和不同角度的思考，就能发挥巨大的作用。随着项目的逐渐深入，也慢慢能够体会到高效团队的重要性，或者说，和脑子转得快的人共事其实是很幸福的事情，毕竟以肉眼可见的速度在提高完成度，是颇有成就感的。</p>
<p>很多东西，越是在乎，越会放大快乐和痛苦，这真是一个令人烦恼的问题呀，希望能慢慢找到解答。</p>
<p>江山笑，烟雨遥，涛浪淘尽红尘俗世几多娇。苍生笑，不再寂寥，豪情仍在痴痴笑笑。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;沧海一声笑，滔滔两岸潮，浮沉随浪只记今朝。苍天笑，纷纷世上潮，谁负谁胜出天知晓。&lt;/p&gt;
    
    </summary>
    
      <category term="Gossip" scheme="http://wdxtub.com/categories/Gossip/"/>
    
    
      <category term="周记" scheme="http://wdxtub.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>wdxtub.com Beta 发布说明</title>
    <link href="http://wdxtub.com/2016/11/30/wdxtub-beta-release-notes/"/>
    <id>http://wdxtub.com/2016/11/30/wdxtub-beta-release-notes/</id>
    <published>2016-11-29T23:47:31.000Z</published>
    <updated>2016-12-01T13:00:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>经过一年多时间的筹备，终于能在今天发布 wdxtub.com 的首个 Beta 版本，包含三项我非常满意的重大改动！</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.30: wdxtub.com Beta 版正式发布</li>
</ul>
<h2 id="统计数字"><a href="#统计数字" class="headerlink" title="统计数字"></a>统计数字</h2><p>至北京时间 2016 年 11 月 30 日 21 时 15 分，关于我的博客 wdxtub.com 的统计数字大约有这些：</p>
<ul>
<li>博客数据<ul>
<li>书籍数量: 366</li>
<li>日志数量: 746</li>
<li>评论数量: 887</li>
<li>打赏数量: 78</li>
</ul>
</li>
<li>访问数据<ul>
<li>访问量: 241126</li>
<li>访问人次: 92714</li>
<li>访客来自中国 33 个省级行政区域（唯一少了一个省）</li>
<li>7 个访客中只有 1 个是使用移动设备的</li>
<li>2 个访客中就有 1 个是使用 Mac OS 的</li>
<li>访客中 Chrome 浏览器的使用率高达 68%</li>
<li>有 35% 的访客使用 1080p 的显示器</li>
<li>新老访客比例大约为 1 比 1</li>
<li>有将近一半的访客年龄在 18 到 24 岁之间</li>
<li>访问来源前五名为：广东省、美国、北京市、上海市、浙江省</li>
</ul>
</li>
<li>代码数据<ul>
<li>Github Repo Star 数量: 79</li>
<li>提交数量: 151 commits / 6,459,262 ++ / 3,439,925 –</li>
<li>最常提交时间: 周三/周四晚十点</li>
</ul>
</li>
</ul>
<h2 id="发布摘要"><a href="#发布摘要" class="headerlink" title="发布摘要"></a>发布摘要</h2><p>从半年多写书的过程中我真切理解了写作的几个关键技巧，即『分层』、『聚类』和『主题先行』。从这样的产品思路出发，博客被划分为三大板块，分别代表不同的主题。把目光放在导航栏，就可以看到『不周山』、『通天塔』和『好望角』三个全新的板块。</p>
<p><img src="/images/14805022102586.jpg" alt=""></p>
<p>名字的含义及主题分别是：</p>
<ol>
<li><strong>不周山</strong>：偏理论，用具体的例子来深入理解概念。学习知识就像不周山，永远不会有『周全』的一天，是为活到老，学到老。</li>
<li><strong>通天塔</strong>：偏实战，用具体的实践来打造完整产品。工程实践就像通天塔，需要不断添砖加瓦才能越盖越高。</li>
<li><strong>好望角</strong>：生活、思考、兴趣、观察、回忆、创作。好望角是寻找通往『黄金乐土』的海上通道，终年大风大浪，所谓生活，就是要乘风破浪冲向新大陆。</li>
</ol>
<p>另外两个值得提及的重大更新是：</p>
<ol>
<li>博客进入版本化时代，无论文章还是板块更新，均会有更新记录，方便大家查阅最新信息</li>
<li>正式采用 <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="external">署名(BY)-非商业性(NC)-禁止演绎(ND)</a> 协议</li>
</ol>
<p>当然细小的更新还有很多，具体可以参见下面的版本注释。感谢大家一直以来的支持，我会努力越做越好。</p>
<h2 id="浏览指南"><a href="#浏览指南" class="headerlink" title="浏览指南"></a>浏览指南</h2><p>使用页面左上方的导航栏可以在首页以及其他板块之间切换。</p>
<p><img src="/images/14805022102586.jpg" alt=""></p>
<p>进入每个板块后，页面右边会出现目录，可以方便地进行导航：</p>
<p><img src="/images/14805100643249.jpg" alt=""></p>
<p>如果一篇文章属于某个系列，在文章的开头会有该系列的目录，以及更新时间：</p>
<p><img src="/images/14805102469086.jpg" alt=""></p>
<p>在书影音页面能看到改版后的展示效果：</p>
<p><img src="/images/14805104198419.jpg" alt=""></p>
<p>在关于页面中新增博客更新文档，后续的版本更新会在这里展现：</p>
<p><img src="/images/14805109563356.jpg" alt=""></p>
<p>还有很多比较小的改动，就留待大家慢慢挖掘啦！</p>
<h2 id="版本注释"><a href="#版本注释" class="headerlink" title="版本注释"></a>版本注释</h2><h3 id="Beta-2016-11-30"><a href="#Beta-2016-11-30" class="headerlink" title="Beta 2016.11.30"></a>Beta 2016.11.30</h3><ul>
<li>导航栏更新，用『不周山』『通天塔』和『好望角』取代了原来的『作品』『技术』和『生活』板块</li>
<li>导航栏更新，原来的『关于我』改为『关于』，增加了关于博客本身内容的信息</li>
<li>导航栏更新，原来的『书单』改为『书影音』，增加了关于电影、音乐和游戏部分的内容</li>
<li>每篇文章除了创建时间外，增加了『更新时间』，方便大家了解文章更新状况</li>
<li>为了配合全新的板块设计，对大部分文章内容进行了调整，之后所有的文章都会隶属于某一板块的某一系列，更加清晰</li>
<li>书单部分由原来的表格切换成了无序列表，并根据内容进行了更加详细的分类</li>
<li>感谢网友 keli 的建议，导航栏图标进行了更换（虽然没有找到山和塔的图标）</li>
</ul>
<p>Beta2 预计更新内容</p>
<ul>
<li>全新的原创网站图标</li>
<li>新系列逐步上线</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经过一年多时间的筹备，终于能在今天发布 wdxtub.com 的首个 Beta 版本，包含三项我非常满意的重大改动！&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="wdxtub" scheme="http://wdxtub.com/tags/wdxtub/"/>
    
      <category term="beta" scheme="http://wdxtub.com/tags/beta/"/>
    
      <category term="release" scheme="http://wdxtub.com/tags/release/"/>
    
  </entry>
  
  <entry>
    <title>【不周山之数据挖掘】壹 概率与统计基础知识</title>
    <link href="http://wdxtub.com/2016/11/27/bzs-dm-basis/"/>
    <id>http://wdxtub.com/2016/11/27/bzs-dm-basis/</id>
    <published>2016-11-26T23:42:13.000Z</published>
    <updated>2016-11-27T13:31:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎走进数据挖掘的世界！『不周山之数据挖掘』系列会结合原理与实践，在弄懂数据挖掘理论的前提下，用实例和分析应用数据挖掘。这一讲是系列正文的开端，主要介绍开始学习数据挖掘的预备知识和相关学习资料。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.27: 完成初稿</li>
</ul>
<h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><p>数据挖掘入门指南，理论为主，配合<a href="http://wdxtub.com/2016/09/11/work-page/#通天塔之-W-I-S-E">『通天塔之 W.I.S.E』</a>有更好的理解</p>
<ul>
<li><a href="http://wdxtub.com/2016/11/27/bzs-dm-basis/">壹 概率与统计基础知识</a></li>
<li><a href="http://wdxtub.com/2016/11/27/bzs-dm-internet/">贰 互联网数据挖掘导论</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>复习概率和统计的基本知识</li>
<li>熟悉并掌握各种分布</li>
<li>意识到统计分析的数字是具有欺骗性</li>
<li>阅读推荐书籍</li>
</ol>
<h2 id="两类专家"><a href="#两类专家" class="headerlink" title="两类专家"></a>两类专家</h2><p>开始复习之前，我们先来看看两类专家的对比（出自《信号与噪声》）：</p>
<table>
<thead>
<tr>
<th style="text-align:center">狐狸型专家的想法</th>
<th style="text-align:center">刺猬型专家的想法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">千伎百俩：汇聚不同学科的思想，忽略最初的政治派别</td>
<td style="text-align:center">一技之长：把大部分精力投入到一两个重大问题上，以怀疑的眼光看待『局外人』的观点</td>
</tr>
<tr>
<td style="text-align:center">适应力强：最初的方法失效后，试图找到新的方法，或同时寻求多种方法</td>
<td style="text-align:center">坚持力强：坚持『总揽一切』的方法，新数据只能用来改善原始模式</td>
</tr>
<tr>
<td style="text-align:center">严于律己：有时会愿意（或是欣于）承认预测中的错误并接受谴责</td>
<td style="text-align:center">固执己见：错误归咎到坏运气或特殊情况上——好模式没有赶上好时机</td>
</tr>
<tr>
<td style="text-align:center">承认复杂性：承认宇宙的复杂性，认为许多基本问题不可解决或本身就是不可预测的</td>
<td style="text-align:center">寻找秩序：一旦从噪声中找到信号，便期望世界遵循某种相对简单的支配关系</td>
</tr>
<tr>
<td style="text-align:center">谨慎：用概率术语表达预测结论，并且证明自己的观点是正确的</td>
<td style="text-align:center">自信：很少对自己的预测进行正面回复，并且不愿改变自己的预测</td>
</tr>
<tr>
<td style="text-align:center">经验主义：更多地依赖观察而非理论</td>
<td style="text-align:center">意识形态：期待日常的问题正是宏伟理论或斗争的体现</td>
</tr>
<tr>
<td style="text-align:center">较好的预测家</td>
<td style="text-align:center">较差的预测家</td>
</tr>
</tbody>
</table>
<p>我们学习数据挖掘，自然是希望自己能够成为『好』的预测家！而数据挖掘中很重要的一个意识就是：统计分析中陷阱重重。</p>
<h2 id="统计分析中的陷阱"><a href="#统计分析中的陷阱" class="headerlink" title="统计分析中的陷阱"></a>统计分析中的陷阱</h2><p>概率和统计是一种观察世界的角度，统计思维从某种程度上来说和读写能力一样重要。回顾自己常犯的错误，很多时候不是因为那些我不知道的事情，而是我知道的是错误答案的事情，这种错误的『正确感』最终导致了错误发生。而概率和统计其实是利用数据对我们已有的偏见进行纠正，这个过程中会有很多看似『反常』的结论出现。至于是相信感觉还是相信数字，啊这真是一个令人头疼的问题。下面是摘录自《统计陷阱》中的只言片语，大家可以先感受一下：</p>
<ul>
<li>有三种谎言：谎言，糟糕透顶的谎言和统计资料</li>
<li>整数总是不完善的</li>
<li>单凭某一数据很难反应实情</li>
<li>一条河永远不可能高于它的源头，同理，对样本研究后得到的结论不会好于样本本身</li>
<li>一个以抽样为基础的报告如果要有价值，就必须使用具有代表性的样本，这种样本排除了各种误差</li>
<li>无形的误差与有形的误差一样容易破坏样本的可信度</li>
<li>普查工作者一般都具有足够的统计知识、技术以及调查费用以确保抽样的精确度。他们并非居心叵测之徒。但并不是所有能见到的数据都产生于这样良好的环境，也并不是所有的数据都会有附有类似的精确度说明</li>
<li>如果某条信息提供了显著性程度，你将对它有更深的了解。显著程度通常用概率表示</li>
<li>将『正常的』与『期望的』混为一谈导致事情变得更糟</li>
<li>这些没有透露的数据其欺骗性在于人们经常忽略了它们的不存在，这当然也是使用这些数据的人获取成功的奥秘</li>
<li>当一个平均数、一张图表或者某种趋势遗漏了这些重要的数据，请对它们保留一些怀疑</li>
<li>你的样本以多大的精度代表总体是可以用数据来衡量的，那就是：可能误差和标准误差</li>
<li>只有当差别有意义时才能称之为差别</li>
<li>注意比例尺和起始标尺，这可能会产生极大的误导性</li>
<li>利用一维图形的信息不对称，可以营造出非常夸张的视觉效果</li>
<li>如果你想证明某事，却发现没有能力办到，那么试着解释其他事情并假装它们是同一回事</li>
<li>相关并不等于因果，一定要注意这里的区别</li>
<li>扭曲统计数据的最巧妙方法是利用地图</li>
<li>百分数也给误解提供了肥沃的土壤。和小数一样，它也能为不确切的食物蒙上精确的面纱</li>
<li>将一些看似能直接相加却不能这样操作的事情加在一起会产生大量的欺骗和隐瞒</li>
<li>对统计资料提出的五个问题<ol>
<li>谁说的？有意识的偏差和无意识的偏差</li>
<li>他是如何知道的？注意样本的有偏，数值是否足够大</li>
<li>遗漏了什么？</li>
<li>是否有人偷换了概念？</li>
<li>这个资料有意义吗？</li>
</ol>
</li>
<li>我们以为自己可以控制很多风险，但结果并非如此，也许这才是更大的威胁</li>
<li>贪婪和恐惧是两个非常不稳定的因素，只有两者保持平衡，经济才能顺利发展。若贪婪在经济体系中占上风，就会产生经济泡沫；若恐惧因素压过贪婪，经济又会陷入恐慌</li>
<li>狐狸型预测方法<ul>
<li>用概率的方法思考问题</li>
<li>今天的预测是你以后人生的第一个预测</li>
<li>寻求共识 </li>
</ul>
</li>
<li>信息是决定预测成败的关键，并不是信息越多，预测就越成功</li>
<li>经济是一个动态系统，不是一个方程式</li>
<li>运气和技能通常被视为两个极端，但两者之间的关系其实更复杂一些</li>
<li>若想做出更准确的预测，就必须承认我们的判断是不可靠的</li>
</ul>
<p>是不是有点感觉了？那么现在复习课开始！</p>
<h2 id="概率与统计基础知识"><a href="#概率与统计基础知识" class="headerlink" title="概率与统计基础知识"></a>概率与统计基础知识</h2><p>概率论主要研究随机事件，既然是研究，我们就不能用『可能/很可能/不太可能』这样的字眼了，要学会如何去量化这种可能性。</p>
<p>统计学主要是根据样本去推测总体情况，大部分的统计分集都是基于概率的，所以概率与统计经常会被放在一起。</p>
<p>那么概率与统计要如何复习呢？最简单粗暴的方法就是把一份我见过最完整的 Cheat Sheet 过一次（在<a href="https://datastories.quora.com/The-Only-Probability-Cheatsheet-Youll-Ever-Need" target="_blank" rel="external">这里</a>），当然，这一讲也不会这么水，还是会把重要概念串讲一次的（部分内容来自《统计思维：程序员数学之概率统计》）。</p>
<p>假设我们要统计之前美国大选各个州各个城市的投票倾向，使用专业的统计学手段应该有如下几个步骤，才更可能绕过各种陷阱，得到更可能正确的结论（注意这里我一直在用可能，因为统计分析中，很少有东西是确定的）：</p>
<ol>
<li>收集数据 - 使用可靠来源的数据</li>
<li>描述性统计 - 计算能总结数据的统计量，并评测各种数据可视化的方法</li>
<li>探索性数据分析 - 寻找模式、差异和其他能解答我们问题的特征。同时，我们会检查不一致性，并确认其局限性</li>
<li>假设检验 - 在发现明显的影响时，我们需要评判这种影响是否真实，也就是说是否是因为随机因素造成的</li>
<li>估计 - 我们会用样本数据推断全部人口的特征</li>
</ol>
<h3 id="描述性统计量"><a href="#描述性统计量" class="headerlink" title="描述性统计量"></a>描述性统计量</h3><p>当我们拿到一组数据，在别人问起『这组数据怎么样』这样的问题的时候，我们肯定不能把这组数据一个一个读出来，而是需要用一些数值来简单描述这组数据的特征，是为『描述性统计量』。</p>
<p>那么描述性统计量都有啥呢？常用的有：均值、平均值、方差、分布、直方图、概率质量函数、条件概率…之所以有这么多各种各样的描述性统计量，就是因为数据本身可以描述的维度太多了：</p>
<ul>
<li>平均值没办法了解数据之间的差异，于是有了方差</li>
<li>方差只能描述数据之间的差异，但无法描述数据的分布，于是有了分布和直方图</li>
<li>直方图没有办法套入数学公式体系进行快速推导，于是有了概率质量函数</li>
<li>概率质量函数没办法处理附加条件的情况，于是有了条件概率</li>
</ul>
<p>这么一说，是不是各种统计量就有了意义了？</p>
<h3 id="连续分布"><a href="#连续分布" class="headerlink" title="连续分布"></a>连续分布</h3><p>比较常用的连续分布是指数分布、帕累托分布、正态分布和对数正态分布，之所以称为『连续』，是因为它们的  CDF 是一个连续函数，而很多实际现象都近似于连续分布。</p>
<blockquote>
<p>跟所有模型一样，连续分布也是一种抽象。换言之，就是会舍弃一些无关紧要的细节。例如，真实观察到的分布中可能会有测量误差或是对样本来说很奇怪的数据，而连续模型会消除这些无关紧要的细节。 连续模型也是一种数据压缩。如果模型能很好地拟合数据集，那么少量参数就可以描述大量数据。 有时候，我们会惊讶地发现某种自然现象服从某个连续分布，观察这些现象可以让我们深入理解真实的系统。</p>
</blockquote>
<ul>
<li>指数分布：事件在每个时间点发生的概率相同，间隔时间的分布就是指数分布</li>
<li>帕累托分布：最初用来描述财富分布状况，后来广泛用于描述自然界和社会科学中的各种现象，包括城镇大小、砂粒和陨石、森林火灾和地震等</li>
<li>正态分布：也称为高斯分布，因其可以近似描述很多现象而成为最常用的分布</li>
</ul>
<h3 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h3><p>概率可以认为是一个零到一之间的值，用于定量描述一件事情发生的可能性大小。如果 E 表示一个事件，那么 P(E) 就表示该事件发生的概率。检测 E 发生情况的过程就叫做试验(trial)。</p>
<p>虽然说关于概率本身的哲学意义还有一些争议，不过我们不妨暂时不去想它，来看看可能是历史上最富争议的概率问题 —— 蒙提霍尔问题(The Monty Hall problem)。</p>
<blockquote>
<p>蒙提·霍尔原本是美国电视游戏节目 Let’s Make a Deal 的主持人，蒙提霍尔问题就是源自该节目中的一个游戏。如果你是参赛者，以下是节目现场的情况。 你会看到三扇关闭的门，蒙提会告诉你每扇门后的奖励：其中有一扇门后面是一辆车，而另外两扇门后面则是诸如花生酱或假指甲之类不太值钱的东西。奖品的摆放是随机的。 你的目标就是要猜出哪扇门后是汽车。如果猜对，汽车就归你了。 我们把你选择的门称为 A 门，其他两扇门分别是 B 门和 C 门。 在打开你所选择的 A 门之前，蒙提往往会打开 B 门或 C 门扰乱你的选择。（如果汽车确实是在 A 门后面，那蒙提随机打开 B 门或 C 门都没有问题。） 接下来，蒙提会给你一个机会：你是坚持原来的选择，还是选择另一扇未打开的门。 问题是，坚持原来的选择或选择另一扇门，会有什么不同吗？</p>
</blockquote>
<p>这个看起来很简单的问题因为与人们的直觉完全不同而变得复杂了起来，一个靠谱的解释是：</p>
<blockquote>
<p>大部分人凭直觉觉得这没有区别。因为，还剩下两扇门，所以汽车在A门后面的概率是50%。但这就错了。实际上，坚持选择A门，获胜的机会就只有 1/3；而如果选择另一扇门，获胜的机会就是 2/3。<br>其中的关键在于要明白，这里有三种可能的情况：汽车可能会在 A 门后，也可能在 B 门或 C 门后面。因为奖品是随机摆放的，所以每种情况的概率都是 1/3。 如果坚持选择 A 门，那么就只有在一开始汽车就在 A 门后面的情况下才能获胜，获胜的概率是 1/3。 但如果选择另一扇没打开的门，那么在 B 或 C 后面有车这两种情况下都会获胜，总体的获胜概率就是 2/3。</p>
</blockquote>
<p>最后提一下聚类错觉(clustering illusion)，指看上去好像有某种特点的聚类实际上是随机的。如何去验证呢？可以看看在随机情况下是否会产生类似聚类的概率，如果是的话，则认为聚类结果无意义，这个过程叫做蒙特卡罗模拟(Monte Carlo simulation)。</p>
<h3 id="进阶知识"><a href="#进阶知识" class="headerlink" title="进阶知识"></a>进阶知识</h3><p>接下来的部分涉及到更加多的数学知识，因为篇幅所限，这里不再一一介绍，但是感兴趣的同学可以根据这些关键词按图索骥，一定会有不一样的收获。</p>
<ul>
<li>分布的运算：偏度、随机变量、概率密度函数、卷积、正态分布性质、中心极限定理</li>
<li>假设检验：交叉验证、卡方验证</li>
<li>估计：方差估计、误差、置信区间、贝叶斯估计</li>
<li>相关性：协方差、最小二乘拟合、因果关系</li>
</ul>
<h2 id="术语表"><a href="#术语表" class="headerlink" title="术语表"></a>术语表</h2><p>术语表是很好的自查列表（内容来自《统计思维：程序员数学之概率统计》），如果有不明白的，可以去维基百科看看，或者一劳永逸找一本教材过一遍即可：</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li>经验之谈(anecdotal evidence): 个人随意收集的证据，而不是通过精心设计并经过研究得到的</li>
<li>直观效应(apparent effect): 表示发生了某种有意思的事情的度量或汇总统计量</li>
<li>人为(artifact): 由于偏差、测量错误或其他错误导致的直观效应</li>
<li>队列(cohort): 一组被调查者</li>
<li>横断面研究(cross-sectional study): 收集群体在特定时间点的数据的研究</li>
<li>字段(field): 数据库中组成记录的变量名称</li>
<li>纵贯研究(longitudinal study): 跟踪群体，随着时间推移对同一组人反复采集数据的研究</li>
<li>过采样(oversampling): 为了避免样本量过少，而增加某个子群体代表的数量</li>
<li>总体(population): 要研究的一组事物，通常是一群人，但这个术语也可用于动物、蔬菜和矿产</li>
<li>原始数据(raw data): 未经或只经过很少的检查、计算或解读而采集和重编码的值</li>
<li>重编码(recode): 通过对原始数据进行计算或是其他逻辑处理得到的值</li>
<li>记录(record): 数据库中关于一个人或其他对象的信息的集合</li>
<li>代表性(representative): 如果人群中的每个成员都有同等的机会进入样本，那么这个样本就具有代表性</li>
<li>被调查者(respondent): 参与调查的人</li>
<li>样本(sample): 总体的一个子集，用于收集数据。 </li>
<li>统计显著(statistically significant): 若一个直观效应不太可能是由随机因素引起的，就是统计显著的</li>
<li>汇总统计量(summary statistic): 通过计算将一个数据集归结到一个数字（或者是少量的几个数字），而这个数字能表示数据的某些特点</li>
<li>表(table): 数据库中若干记录的集合</li>
</ul>
<h3 id="描述性统计量-1"><a href="#描述性统计量-1" class="headerlink" title="描述性统计量"></a>描述性统计量</h3><ul>
<li>区间(bin): 将相近数值进行分组的范围</li>
<li>集中趋势(central tendency): 样本或总体的一种特征，直观来说就是最能代表平均水平的值</li>
<li>临床上有重要意义(clinically significant): 分组间差异等跟实践操作有关的结果</li>
<li>条件概率(conditional probability): 某些条件成立的情况下计算出的概率</li>
<li>分布(distribution): 对样本中的各个值及其频数或概率的总结</li>
<li>频数(frequency): 样本中某个值的出现次数</li>
<li>直方图(histogram): 从值到频数的映射，或者表示这种映射关系的图形</li>
<li>众数(mode): 样本中频数最高的值</li>
<li>归一化(normalization): 将频数除以样本大小得到概率的过程</li>
<li>异常值(outlier): 远离集中趋势的值</li>
<li>概率(probability): 频数除以样本大小即得到概率</li>
<li>概率质量函数(Probability Mass Function，PMF): 以函数的形式表示分布，该函数将值映射到概率</li>
<li>相对风险(relative risk): 两个概率的比值，通常用于衡量两个分布的差异</li>
<li>分散(spread): 样本或总体的特征，直观来说就是数据的变动有多大</li>
<li>标准差(standard deviation): 方差的平方根，也是分散的一种度量</li>
<li>修剪(trim): 删除数据集中的异常值</li>
<li>方差(variance): 用于量化分散程度的汇总统计量</li>
</ul>
<h3 id="累积分布函数"><a href="#累积分布函数" class="headerlink" title="累积分布函数"></a>累积分布函数</h3><ul>
<li>条件分布(conditional distribution): 在满足一定前提条件下计算出的分布</li>
<li>累积分布函数(Cumulative Distribution Function，CDF): 将值映射到其百分等级的函数</li>
<li>四分差(interquartile range): 表示总体分散情况的值，等于75和25百分等级之间的差</li>
<li>百分位(percentile): 与百分等级相关联的数值</li>
<li>百分等级(percentile rank): 分布中小于或等于给定值的值在全部值中所占的百分比</li>
<li>放回(replacement): 在抽样过程中，“有放回”表示对于每次抽样，总体都是不变的。“无放回”表示每个元素只能选择一次</li>
<li>再抽样(resampling): 根据由样本计算得到的分布重新生成新的随机样本的过程</li>
</ul>
<h3 id="连续分布-1"><a href="#连续分布-1" class="headerlink" title="连续分布"></a>连续分布</h3><ul>
<li>连续分布(continuous distribution): 由连续函数描述的分布</li>
<li>语料库(corpus): 特定语言中用做样本的正文文本</li>
<li>经验分布(empirical distribution): 样本中值的分布</li>
<li>误差函数(error function): 一种特殊的数学函数，因源自误差度量研究而得名</li>
<li>一次频词(hapaxlegomenon): 表示语料库中只出现一次的词。这个单词在本书中迄今出现了两次</li>
<li>间隔时间(interarrival time): 两个事件的时间间隔</li>
<li>模型(model): 一种有效的简化。对于很多复杂的经验分布，连续分布是不错的模型</li>
<li>正态概率图(normal probability plot): 一种统计图形，用于表示样本中排序后的值与其服从正态分布时的期望值之间的关系</li>
<li>秩变换(rankit): 元素的期望值，该元素位于服从正态分布的已排序列表中。</li>
</ul>
<h3 id="概率-1"><a href="#概率-1" class="headerlink" title="概率"></a>概率</h3><ul>
<li>贝叶斯认识论(Bayesianism): 一种对概率更泛化的解释，用概率表示可信的程度</li>
<li>变异系数(coefficient of variation): 度量数据分散程度的统计量，按集中趋势归一化，用于比较不同均值的分布</li>
<li>事件(event): 按一定概率发生的事情</li>
<li>失败(failure): 事件没有发生的试验</li>
<li>频率论(frequentism): 对概率的一种严格解读，认为概率只能用于一系列完全相同的试验</li>
<li>独立(independent): 若两个事件之间相互没有影响，就称这两个事件是独立的</li>
<li>证据的似然值(likelihood of the evidence): 贝叶斯定理中的一个概念，表示假设成立的情况下看到该证据的概率</li>
<li>蒙特卡罗模拟(Monte Carlo simulation): 通过模拟随机过程计算概率的方法</li>
<li>归一化常量(normalizing constant): 贝叶斯定理中的分母，用于将计算结果归一化为概率</li>
<li>后验(posterior): 贝叶斯更新后计算出的概率</li>
<li>先验(prior): 贝叶斯更新前计算出的概率</li>
<li>成功(success): 事件发生了的试验</li>
<li>试验(trial): 对一系列事件是否可能发生的尝试</li>
<li>更新(update): 用数据修改概率的过程。</li>
</ul>
<h3 id="分布的运算"><a href="#分布的运算" class="headerlink" title="分布的运算"></a>分布的运算</h3><ul>
<li>中心极限定理(Central Limit Theorem): 早期的统计学家弗朗西斯·高尔顿爵士认为中心极限定理是“The supreme law of Unreason”</li>
<li>卷积(convolution): 一种运算，用于计算两个随机变量的和的分布</li>
<li>虚幻的优越性(illusory superiority): 心理学概念，是指人们普遍存在的将自己高估的一种心理</li>
<li>概率密度函数(probability density function): 连续型累积分布函数的导数</li>
<li>随机变量(random variable): 一个能代表一种随机过程的客体</li>
<li>随机数(random variate): 随机变量的实现</li>
<li>鲁棒性(robust): 如果一个统计量不容易受到异常值的影响，我们说它是鲁棒的</li>
<li>偏度(skewness): 分布函数的一种特征，它度量的是分布函数的不对称程度</li>
</ul>
<h3 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h3><ul>
<li>单元格(cell): 在卡方检验中，将观测按一定的标准分到各个单元格里，每个单元格代表一种分类</li>
<li>卡方检验(chi-square test): 用卡方统计量做统计量的统计检验</li>
<li>交叉验证(cross-validation): 交叉验证使用一个数据集进行探索性数据分析，然后用另一个数据集进行测试</li>
<li>假阴性(false negative): 在效应真实存在的情况下，我们认为这个效应是由偶然因素引起的</li>
<li>假阳性(false positive): 在原假设为真的情况下，我们拒绝了原假设的结论</li>
<li>假设检验(hypothesis testing): 判定出现的效应是否具有统计显著性的过程</li>
<li>似然比(likelihood ratio): 一种概率的比值， P(E|A)/P(E|B)，这里A和B是两种假设。似然比不依赖于先验概率，可以用来报道贝叶斯统计推断的结果</li>
<li>原假设(null hypothesis): 一种基于以下假设的模型系统：我们观测到的效应只是由偶然因素引起的</li>
<li>单边检验(one-sided test): 一种检验类型，关注的是出现比观测到的效应更大（或小）的效应的概率</li>
<li>p值(p-value): 在原假设成立的情况下，出现我们观测到的效应的概率</li>
<li>功效(power): 在原假设为假的情况下，检验推翻原假设的概率</li>
<li>显著性(significant): 我们说某个效应具有统计显著性指的是这种情况不大可能是由偶然因素引起的</li>
<li>检验统计量(test statistic): 衡量观测到的效应与原假设下期望的结果之间偏差的统计量</li>
<li>测试集(testing set): 用做测试的数据集</li>
<li>训练集(training set): 用做训练的数据集</li>
<li>双边检验(two-sided test): 一种检验类型，关注的是出现比观测到的效应更大的效应的概率，不考虑正负</li>
</ul>
<h3 id="估计"><a href="#估计" class="headerlink" title="估计"></a>估计</h3><ul>
<li>有偏性(bias): 在平均多次试验的结果后，一个估计量倾向于高估或者低估真实的参数值</li>
<li>删失数据(censored data): 一种数据集，数据来源于某种采集方式，但是这种采集方式会系统性地排除某些数据</li>
<li>置信区间(confidence interval): 一种参数的区间估计，以一定的概率包含待估计的参数</li>
<li>可信区间(credible interval): 贝叶斯统计理论中的置信区间</li>
<li>估计(estimation): 用样本信息估计分布中未知参数的过程</li>
<li>估计量(estimator): 用于估计参数的统计量</li>
<li>极大似然估计量(maximum likelihood estimator): 使得似然函数最大化的估计</li>
<li>均方误差(mean squared error): 一种衡量估计误差的值</li>
<li>点估计(point estimate): 用单一的值估计某个参数</li>
</ul>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><ul>
<li>确定系数(coefficient of determination): 衡量模型拟合结果好坏的指标</li>
<li>对照组(control group): 对照试验中没有接受处理的组，或受到已知效应处理的组</li>
<li>相关性(correlation): 对两个变量关系的一种描述</li>
<li>协方差(covariance): 衡量两个变量变化方向是否一致的统计量</li>
<li>因变量(dependent variable): 我们想要解释或者预测的变量</li>
<li>自变量(independent variable): 用于预测因变量的变量，也称解释变量</li>
<li>最小二乘拟合(least squares fit): 最小化残差平方和的数据拟合方法</li>
<li>自然试验(natural experiment): 一种试验设计的方法，就是利用自然形成的界限将受试者分成几个分组，并且大体上使得分组结果接近随机分组</li>
<li>归一化(normalize): 将一组数据进行转换，使其均值为 0，方差为 1</li>
<li>随机对照试验(randomized controlled trial): 一种试验设计的方法，将受试者随机分成几个分组，并对不同的分组实施不同的处理</li>
<li>秩(rank): 将一个序列按大小排序后，序列中的某个元素所处的位置</li>
<li>残差(residual): 衡量模型预测结果与真实值离差的值</li>
<li>标准分数(standard score): 归一化后的值</li>
<li>处理(treatment): 对照试验中对一个分组所做的干预或改变</li>
</ul>
<h2 id="推荐书籍"><a href="#推荐书籍" class="headerlink" title="推荐书籍"></a>推荐书籍</h2><ul>
<li>Statistics for business and economics</li>
<li>Data Mining: concepts and technologies</li>
<li>数据挖掘导论</li>
<li>Python for Data Analysis</li>
<li>Web Analysis</li>
<li>深入浅出数据分析</li>
<li>增长黑客</li>
<li>网站分析实战</li>
<li>精益数据分析</li>
<li>统计陷阱</li>
<li>统计思维：程序员数学之概率统计</li>
</ul>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>找个朋友问一下蒙提霍尔问题，看看最后你能解释清楚没有</li>
<li>试着用概率统计的思维去观察一下，有没有什么新发现？</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这一讲我们简单了解了数据挖掘的基础知识，下一讲会简要介绍数据挖掘的基本流程和各个组件的功能。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;欢迎走进数据挖掘的世界！『不周山之数据挖掘』系列会结合原理与实践，在弄懂数据挖掘理论的前提下，用实例和分析应用数据挖掘。这一讲是系列正文的开端，主要介绍开始学习数据挖掘的预备知识和相关学习资料。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="数据挖掘" scheme="http://wdxtub.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>【不周山之数据挖掘】贰 互联网数据挖掘导论</title>
    <link href="http://wdxtub.com/2016/11/27/bzs-dm-internet/"/>
    <id>http://wdxtub.com/2016/11/27/bzs-dm-internet/</id>
    <published>2016-11-26T23:42:13.000Z</published>
    <updated>2016-11-27T07:08:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>上一讲中我们简单复习的概率和统计相关知识，现在我们就可以正式开始接触数据挖掘了。本文是接下来内容的大纲，让大家对互联网搜索与挖掘有一个宏观的了解，即知道要做什么和怎么做。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.27: 完成初稿</li>
</ul>
<p>注：本文的框架来源于北京大学<a href="http://www.icst.pku.edu.cn/lcwm/wanxj/" target="_blank" rel="external">万小军</a>开设的<a href="http://www.icst.pku.edu.cn/lcwm/course/WebDataMining2016/?page_id=560" target="_blank" rel="external">互联网数据挖掘 Web Data Mining</a> 课程，我对内容进行了筛选和编排，用来作为『不周山之数据挖掘』系列的导论部分。</p>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><p>数据挖掘入门指南，理论为主，配合<a href="http://wdxtub.com/2016/09/11/work-page/#通天塔之-W-I-S-E">『通天塔之 W.I.S.E』</a>有更好的理解</p>
<ul>
<li><a href="http://wdxtub.com/2016/11/27/bzs-dm-basis/">壹 概率与统计基础知识</a></li>
<li><a href="http://wdxtub.com/2016/11/27/bzs-dm-internet/">贰 互联网数据挖掘导论</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>了解搜索和自然语言处理的基本知识</li>
<li>熟悉数据挖掘的流程与各个步骤所用的技术</li>
<li>对数据挖掘的应用场景有基本的认识</li>
</ol>
<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>随着互联网的日益蓬勃发展，如何从广袤的信息海洋中提取出有价值的信息、模式和关系，逐渐成为了一门新的领域 —— 数据挖掘。作为一门交叉学科，数据挖掘融合了信息检索、互联网、数据库、机器学习、自然语言处理等不同的学科，用多样技术完成具体的数据挖掘应用。常见的应用有：垂直搜索、推荐系统、智能问答、机器翻译、舆情监测、情报收集等等，可谓是深入到了我们日常生活的方方面面</p>
<p>接下来我们会从基础技术说起，从以下三个方面来了解数据挖掘：</p>
<ul>
<li>搜索技术</li>
<li>数据挖掘技术</li>
<li>具体应用</li>
</ul>
<h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>搜索其实是一个很大的主题，但是核心问题其实并不复杂，一是如何去表示文档，二是在这样的基础上如何去检索文档。具体的评价标准是『效果』和『效率』。效果指的是如何准确匹配查询与文档，一般来说会基于检索模型进行。效率值得是如何快速返回检索结果，一般来说是基于索引进行的。</p>
<h3 id="文档表示"><a href="#文档表示" class="headerlink" title="文档表示"></a>文档表示</h3><p>文档表示一般有两种方法：手动或自动。</p>
<p>手动方法主要依靠人工标注，结果比较可靠，而且标注的词汇是预先设定好的关键词，检索起来效率比较高。但是人工标注无论是时间成本还是人力成本都较高，一般来说难以大批量使用。这类人工标注的信息一般称为文档的元描述(Meta-descriptions)，除了包含域信息(author, title, date)外，还回包含关键词和分类。</p>
<p>自动方法最有代表性的是词袋(Bag of Words)技术，即使用文档中出现的词的集合来表示一篇文档。但是这种方法也有很多不足之处，因为是词语的无序集合，句法信息首先已经丢失了，另外针对不同的语言会有不同的难点。</p>
<p>对于中文来说，如何进行分词（即把句子分成词）就是一个很大的难点，尤其是层出不穷的网络热梗，如何保证准确和实时就是非常大的挑战。对于英文来说，虽然没有分词的问题，但是大小写、单复数、时态、词根等等同样让人头疼。这也导致了大部分搜索引擎都不会考虑词根问题，一是因为文档太多，进行二次处理得不偿失，二是因为对于搜索结果来说影响没有那么大，自然就没有太大的动力去做。</p>
<p>但是无论是中文还是英文，有一个操作是一定会做的，就是去掉停用词(Stop Words)，也就是去掉那些不具有内容信息的词，比如对于中文来说『的地得』，对于英文来说的『a, an, the』。但需要注意的是这样一个简单的操作虽然可以大幅减少索引的大小，缩短检索时间，但实际上不能提高检索效果，具体挺用词表的确定也需要根据不同的文档集合和应用具体对待，没有一个一概而论的方案。</p>
<h3 id="文档索引"><a href="#文档索引" class="headerlink" title="文档索引"></a>文档索引</h3><p>表示了文档之后，我们需要对其进行索引，不然每次检索如果需要用户等太久，体验就很糟糕了。而具体到用什么进行检索，最终人们选择了用词而不是短语来作为索引，这里一个比较有代表性的工具就是 Lucene，现在互联网上广为应用的 Elasticsearch 和 Solr 都是基于 Lucene 的。</p>
<p>Lucene 最重要的技术就是倒排索引(inverted index)，可看做链表数组，每个链表的表头包含关键词，其后序单元则包括所有包括这个关键词的文档标号，以及一些其他信息，如该词的频率和位置等。这里关键词查询一般采用 B-Tree 或哈希表，文档列表组织一般采用二叉搜索树。</p>
<h3 id="文档检索"><a href="#文档检索" class="headerlink" title="文档检索"></a>文档检索</h3><p>文档检索的思路也很简单：如果一篇文档与一个查询相似，那么该文档与查询相关。相似性一般根据字符串匹配来判定，比方说相似的词汇或相同的语义。</p>
<p>最初人们常用的是基于布尔代数的匹配，虽然比较简单，但是对查询的要求很高；并且匹配标准过于严格，容易导致过少或过多的检索结果。尽管布尔模型不再用作主流文档检索模型，但其思想常用于实现高级(综合)检索功能。</p>
<p>现在最常用的是向量空间模型(Vector Space Model)，其思路是文档与查询都是高维空间中的一个向量，用户自由输入文本也是一个向量，利用向量空间的相似性进行查询。具体的相似性同样可以用两种方法来确定：内积或者夹角。因为是空间，所以度量距离的时候会采用不同的描述距离的方式，有 Minkowski metric, Euclidian distance, Jacquard measure 和 Dice’s coefficient 等等。</p>
<p>同一篇文档中不同词语其实也会有不同的权重，这里我们比较常用的是 TF-IDF 算法，其中 TF 表示词语出现的频率，而 IDF 则能区别不同词语的重要性。</p>
<h3 id="文档收集"><a href="#文档收集" class="headerlink" title="文档收集"></a>文档收集</h3><p>前面介绍了文档检索的各种概念，但是现在问题来了，文档从哪里来呢？这就要提到我们最常听见的爬虫(Web Crawler)了，它能够快速有效地收集尽可能多的有用 Web 页面，包括页面之间的链接结构。</p>
<p>随着 Web 2.0 的兴起，脚本语言生成的动态内容和各类多媒体内容给爬虫增加了许多难度，但基本的页面爬取策略没有太大的改变，一般以以广度优先为主，深度优先为辅，需要具体的特性主要有：</p>
<ul>
<li>健壮 Robustness, 避免进入死循环</li>
<li>友好 Politeness, 遵守服务器的采集协议</li>
<li>分布式 Distributed, 多台机器分布式采集</li>
<li>可扩展 Scalable, 爬虫架构方便扩展</li>
<li>性能与效率，有效利用系统资源</li>
<li>质量 Quality, 倾向于采集有用的页面</li>
<li>新颖 Freshness, 获取网页的最新版本</li>
<li>可扩充 Extensible, 能够处理新数据类型、新的采集协议等</li>
</ul>
<h3 id="链接分析"><a href="#链接分析" class="headerlink" title="链接分析"></a>链接分析</h3><p>除了页面的内容本身，超链接其实也能提供非常多有价值的信息。一条从页面 A 指向页面 B 的链接表明 A 与 B 相关且 A 推荐/引用/投票/赞成 B。Google 当年最重要的 PageRank 算法，其实就是这个问题的最初且最成功的解决方案。</p>
<p>这里有一个很有趣的现象叫做排序沉入(Rank Sink)，页面 A 引用了页面 B，页面 B 也引用了页面 A，就形成了一个闭环，不再向外传播分数了。这是我们在实际运用中需要避免的情况。</p>
<h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p>数据挖掘根据应用的不同，分为不同的子领域，这些子领域又和机器学习、概率统计、模式识别等有着千丝万缕的关系。接下来先介绍基本概念，然后聊聊一些常见的应用。</p>
<h3 id="主要任务"><a href="#主要任务" class="headerlink" title="主要任务"></a>主要任务</h3><p>数据挖掘的任务主要包括两类，一类是基于一些变量预测其他变量的未知值或未来值，称为预测型任务，常用的技术是分类(Classification)，回归(Regression)和偏差分析(Deviation Detection)。另一类是发现描述数据的人们可解释的模式，称为描述型任务，常用的技术是聚类(Clustering)，关联规则挖掘(Association Rule Discovery)和摘要(Summarization)。</p>
<p>为了完成上述任务，整个数据挖掘的流程为：获取数据 -&gt; 选择数据 -&gt; 预处理数据 -&gt; 数据规整 -&gt; 数据挖掘 -&gt; 模式识别。不同阶段会使用不同的技术，但一定要把整个流程走通，数据挖掘才有意义。</p>
<p>随着数据量的增大，如何让数据挖掘更加容易拓展效率更高，如何去挖掘有上下文关系的数据，如何从复杂、异构、网络化数据中挖掘复杂知识，如何挖掘低质量数据，如何保证安全性和隐私，都是未来数据挖掘需要努力的方向。</p>
<h3 id="常用工具"><a href="#常用工具" class="headerlink" title="常用工具"></a>常用工具</h3><p>开源的工具有：</p>
<ul>
<li>Weka</li>
<li>GATE</li>
<li>Carrot2</li>
<li>NLTK</li>
<li>Orange</li>
<li>RapidMiner</li>
<li>KNIME</li>
</ul>
<p>商用的应用主要有：</p>
<ul>
<li>IBM InfoSphere Warehouse</li>
<li>Microsoft Analysis Services</li>
<li>SAS Enterprise Miner</li>
<li>STATISTICA Data Miner</li>
<li>Oracle Data Mining</li>
</ul>
<h3 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h3><p>自然语言处理是人工智能和语言学领域的分支学科指的是利用计算机对人类特有的书面形式和口头形式的自然语言进行各种类型处理和加工的技术。其中最关键的任务有：自动分词、命名实体识别、词性标注、句法分析、语义分析和篇章分析。主要应用在：机器翻译、<strong>文本分类</strong>、情感分析、信息检索与过滤、自动问答、<strong>信息抽取</strong>、<strong>自动文摘</strong>和人机对话等领域。</p>
<p>推荐教材</p>
<ul>
<li>Foundations of Statistical Natrual Language Processing</li>
<li>Speech and Language Processing</li>
<li>统计自然语言处理</li>
</ul>
<p>这里主要以汉语为例子说说分词。一般认为词是最小的、能够独立运用的、有意义的语言单位。但是汉语分词有许多挑战，比如</p>
<ul>
<li>词和词组的边界模糊</li>
<li>新词(未登陆词)</li>
<li>切分歧义<ul>
<li>汉字串 AJB 被称作<strong>交集型切分歧义</strong>，如果满足 AJ, JB 同时为词，此时汉字串 J 被称作交集串</li>
<li>汉字串 AB 被称作<strong>组合型切分歧义</strong>，如果满足条件 A, B, AB 同时为词</li>
<li>真歧义：存在两种或两种以上的真实存在的切分形式</li>
</ul>
</li>
</ul>
<p>具体的分词方法目前主要有以下几种，前两天也有一个利用深度学习的解决方案开源了，可以关注一下</p>
<ul>
<li>简单的模式匹配<ul>
<li>正向最大匹配(FMM)、逆向最大匹配(BMM, 比正向更有效)、双向匹配(BM, 比较两种方法的结果，大颗粒词越多越好，非词典词和单子词越少越好，可以识别出交叉歧义)</li>
</ul>
</li>
<li>基于规则的方法<ul>
<li>最少分词算法</li>
</ul>
</li>
<li>基于统计的方法<ul>
<li>统计语言模型分词、串频统计和词形匹配相结合的汉语自动分词、无词典分词</li>
<li>第一步是候选网格构造：利用词典匹配，列举输入句子所有可能的切分词语，并以词网格形式保存</li>
<li>第二步计算词网格中的每一条路径的权值，权值通过计算图中的每一个节点(每一个词)的一元统计概率和节点之间的二元统计概率的相关信息</li>
<li>最后根据图搜索算法在图中找到一条权值最大的路径，作为最后的分词结果</li>
<li>优缺点：可利用不同的统计语言模型计算最优路径，具有比较高的分词正确率；但算法时间、空间复杂度较高</li>
</ul>
</li>
</ul>
<h2 id="常见应用"><a href="#常见应用" class="headerlink" title="常见应用"></a>常见应用</h2><p>接下来介绍数据挖掘的积累常见应用</p>
<h3 id="智能问答技术"><a href="#智能问答技术" class="headerlink" title="智能问答技术"></a>智能问答技术</h3><p>智能问答技术起源于信息检索社区，简单来说就是根据用户的提问给出简短的答案或提供答案的证据。根据不同的划分标准，我们可以总结出如下的几类问题类型</p>
<ul>
<li>根据答案类型划分<ul>
<li>事实型问题(Factual questions)</li>
<li>观点型问题(Opinions)</li>
<li>摘要型问题(Summaries)</li>
</ul>
</li>
<li>根据问题言语行为(question speech act)划分<ul>
<li>是否型问题(Yes/NO questions)</li>
<li>WH 问题(WH questions)</li>
<li>间接请求(Indirect Requests)</li>
<li>命令(Commands)</li>
</ul>
</li>
<li>复杂/困难问题<ul>
<li>为什么/怎么样(Why, How questions)</li>
<li>什么(What questions)</li>
</ul>
</li>
</ul>
<p>遗憾的是，目前大部分理解问题的技术都是基于正则表达式的，毕竟在自然语言理解这块，暂时还没有突破性进展。</p>
<p>传统自动问答技术主要是基于语料库的自动问答或基于知识库的自动问答，基本包括三个步骤：</p>
<ol>
<li>问题分析(分类、模板匹配、语义分析)</li>
<li>段落检测(段落抽取、排序)</li>
<li>答案抽取(实体识别、模板匹配、排序)</li>
</ol>
<p>社区问答主要是应用与诸如知乎和 Quora 这类网站，目前主要的方向是问题分类、问题推荐、信誉评估和知识抽取等等。</p>
<h3 id="情感分析与观点挖掘"><a href="#情感分析与观点挖掘" class="headerlink" title="情感分析与观点挖掘"></a>情感分析与观点挖掘</h3><p>情感分析与观点挖掘主要应用于产品比较与推荐、个人与机构声誉分析、电视节目满意度分析、互联网舆情分析和反恐与维稳。目前很多互联网平台（如淘宝、大众点评）都已经利用这种技术帮助提取用户评价中的关键词以提供更好的用户体验。</p>
<p>基本的框架如下所示</p>
<ul>
<li>应用层：情感检索，情感摘要，情感问答</li>
<li>核心层：情感要素抽取，情感倾向性分析，主客观分析/观点文本识别</li>
<li>基础层：NLP 基本模块，情感资源收集与标注</li>
<li>来源：产品评论，电影评论，新闻评论，博客，微博</li>
</ul>
<p>而具体应用中，会将文本按照所表达的总体情感进行分类，可能的分类主要有如下三种，一般会从词、句子、文档三中粒度来进行分析：</p>
<ul>
<li>主客观分析/观点文本识别<ul>
<li>客观：反映关于世界的事实信息</li>
<li>主观：反映个人情感、信念等</li>
</ul>
</li>
<li>倾向性分析(可看作主客观分析的细粒度处理)<ul>
<li>对包含观点的文本进行倾向性判断</li>
<li>一般分为三类：褒义、贬义、中性(在一些问题不考虑中性)</li>
</ul>
</li>
<li>情绪分析<ul>
<li>愤怒、高兴、喜好、悲哀、吃惊等等</li>
</ul>
</li>
</ul>
<p>而对于观点挖掘来说，一个观点表示为一个五元组：目标对象, 目标对象特征, 观点的情感值, 观点持有者, 观点表达时间。实际上，观点抽取任务是很困难的，我们<strong>重点关注两个子任务</strong></p>
<ul>
<li>特征抽取与聚类(aspect extraction and grouping)<ul>
<li>抽取对象的所有特征表达，并将同义特征表达聚类。每个特征类表示了关于该对象的独一无二的某个特征</li>
</ul>
</li>
<li>特征情感分类(aspect sentiment classification)<ul>
<li>确定观点针对每个特征的情感倾向：正面、负面、中性</li>
</ul>
</li>
</ul>
<h3 id="信息摘要"><a href="#信息摘要" class="headerlink" title="信息摘要"></a>信息摘要</h3><p>信息摘要指的是对海量数据内容进行<strong>提炼与总结</strong>，以<strong>简洁、直观</strong>的摘要来概括用户所关注的主要内容，方便用户快速了解与浏览海量内容。遗憾的是，研究 50 多年，有一定进展，但仍不能令人满意。一般来说实现思路有两种</p>
<ol>
<li><strong>抽取式</strong>：从文档中抽取已有句子形成摘要。这种方法实现简单，能保证句子的可读性</li>
<li><strong>生成式/混合式</strong>：生成新的句子，或者对已有句子进行压缩、重构与融合。这种方法难度更大，但更接近摘要的本质</li>
</ol>
<p>抽取式文档摘要的典型工作流程是：文档集 -&gt; 文档理解 -&gt; <strong>句子重要性计算与排名(利用词语句子的各类特征，基于机器学习)</strong> -&gt; 句子选择 -&gt; 摘要句子排序 -&gt; 摘要</p>
<p>目前摘要总体性能不高，需要方法上的突破。</p>
<h3 id="社交网络分析"><a href="#社交网络分析" class="headerlink" title="社交网络分析"></a>社交网络分析</h3><p>社交网络作为 Web 2.0 的典型代表，用户生成的内容相当多，可以看作是某种程度上的群体智慧和在强交互性基础上构造的异构网络。</p>
<p>社交网络分析主要是基于社交关系、结构进行挖掘，比如社区检测、连接预测、影响力分析。而社交内容挖掘则是基于文本等内容数据进行挖掘，比如摘要、关键词、情感分析。因为每个人在社交网络上可以抽象为一个元素，于是他们之间的关系可以用矩阵表示。另一种表示的方式是使用图，其中节点 = 成员，边 = 关系。</p>
<p>比较常见的任务有：</p>
<ul>
<li>社交网络抽取(Social Network Extraction)：从数据源中抽取、构建社交网络</li>
<li><strong>网络中心性分析(Network Centrality Analysis)</strong>：识别社交网络上最重要的节点(重要性的定义由目的、环境所定)<ul>
<li>输入为一个社交网络，输出为最重要的节点列表，一般方法是为节点计算分数或排序，反映节点的重要性/专业性/影响力</li>
<li>对于点重要性的评估可以采用<strong>网络中心性测度(Centrality measures)</strong>方法，具体中心性的定义可能是度数中心性（朋友最多）、中介中心性（处在信息流动关键节点）或亲近中心性（离所有节点平均距离最短）</li>
</ul>
</li>
<li>用户画像：根据用户特点给用户群体分类</li>
<li>链接预测(Link Prediction)：给定一个社交网络，预测哪些节点相互连接。例如: facebook 中的好友推荐</li>
<li>病毒式营销(Viral Marketing)：找出若干用户，为其提供优惠或折扣，从而影响网络上的其他用户，使得收益最大化</li>
</ul>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>尝试在网络寻找应用了数据挖掘的产品，并思考不同公司是如何使用的</li>
<li>对于大数据时代的个人隐私问题，你怎么看？</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这一讲我们简单了解了数据挖掘及应用的方方面面，接下来我们就会具体深入介绍其中一些非常有意思的部分。当然，如果有很多不明白的概念，建议简单看看维基百科了解一下，不过实在不明白也没关系，随着之后的实践，应该会有恍然大悟的一天。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一讲中我们简单复习的概率和统计相关知识，现在我们就可以正式开始接触数据挖掘了。本文是接下来内容的大纲，让大家对互联网搜索与挖掘有一个宏观的了解，即知道要做什么和怎么做。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="数据挖掘" scheme="http://wdxtub.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>第二十四周 - 缘分一道桥</title>
    <link href="http://wdxtub.com/2016/11/25/bridge-of-fate/"/>
    <id>http://wdxtub.com/2016/11/25/bridge-of-fate/</id>
    <published>2016-11-25T15:00:00.000Z</published>
    <updated>2016-11-25T16:40:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>秦时明月汉时关，万里长征人未还，但使龙城飞将在，不教胡马度阴山。</p>
<a id="more"></a>
<hr>
<p>一场大雨之后，深圳也入冬了，虽然温度依然在两位数徘徊，但的确是最冷的两天了。周末在广州看了一场天鹅湖改编的灯光舞台秀，得到的结论是果然我是艺术木头疙瘩，基本处于难以欣赏的状态，唯一能够确定的是中间混杂的农业重金属部分我是非常不喜欢的。</p>
<p>天气变冷之后还在外面跑步就有些艰难了，所以开始了在室内的冬季锻炼计划，这两天是浑身酸疼，不过坚持过去就好了。从朋友圈里得知匹兹堡也迎来了今年冬天的第一场雪，虽然我已经离开，但是却仿佛依然在，这种神奇的感觉，估计明年就不会剩下多少了。</p>
<p>周日(11.20)参加了执信九十五周年校庆，执信之于我像是一个魔法盒子，开启了新世界的大门。走在校园里，看着似曾相识的建筑和教室，踏在操场上的瞬间，感觉大家都回来了。看着优秀校友们做出的成绩，也在心底默默鞭策自己要好好努力。</p>
<p>这周开始了新系列的写作，从最初零散的技术文章，到整理成系列，最后再到作品集，也看到了自己的成长。尤其是冥思苦想出来的『通天塔』和『不周山』系列，还是有中西药结合疗效好的感觉的。随着思考方式的升级，也开始逐渐谋划自己的职业生涯，如何打造自己的核心竞争力和技术门槛，就成为接下来的工作重点了。</p>
<p>雨天上班，到车站还是有一段路的，走在路上意识到：因为有伞，除非是大暴雨，不然其实很难把鞋子淋湿，相比之下，对鞋子更大的威胁是积水，走在积水的路面，除非非常小心，不然湿鞋在所难免。但积水背后是有很多问题的，比方说排水管道，比方说地形地貌。更重要的是，如果能把雨水利用好，不但能解决积水问题，更是把资源利用了起来，但这里的门道，就需要好好想一想了。</p>
<p>最近愈发觉得自己心理素质要加强，很多时候是患得患失让我们最终没有迈开步子，最后既没有得，也没有失，但其实这就是最大的失去吧。不由得想起大鱼海棠里的一段话：</p>
<blockquote>
<p>我们这一生很短，我们终将会失去它，所以不妨大胆一点。爱一个人，攀一座山，追一次梦。有很多事都没有答案，但我相信，上天给我们生命，一定是让我们创造奇迹的。</p>
</blockquote>
<p>谈爱恨，不能潦草。用信任，立下誓言我来熬。这缘分，像一道桥。走天涯，你我卸下战袍，梦回长城谣。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;秦时明月汉时关，万里长征人未还，但使龙城飞将在，不教胡马度阴山。&lt;/p&gt;
    
    </summary>
    
      <category term="Gossip" scheme="http://wdxtub.com/categories/Gossip/"/>
    
    
      <category term="周记" scheme="http://wdxtub.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【不周山之机器学习】壹 基础知识与环境搭建</title>
    <link href="http://wdxtub.com/2016/11/25/bzs-ml-basis/"/>
    <id>http://wdxtub.com/2016/11/25/bzs-ml-basis/</id>
    <published>2016-11-25T01:09:09.000Z</published>
    <updated>2016-11-25T23:21:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎走进机器学习的世界！『不周山之机器学习』系列会结合原理与实践，在弄懂机器学习理论的前提下，把抽象的机器学习理论给用起来。这一讲是系列正文的开端，主要介绍开始学习机器学习的预备知识和相关学习资料，以及搭建好我们用于实践算法的环境。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.25: 完成初稿</li>
</ul>
<h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><ul>
<li><a href="http://wdxtub.com/2016/11/25/bzs-ml-basis/">壹 基础知识与环境搭建</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>复习机器学习所需的数学基础，即线性代数、概率和统计</li>
<li>了解机器学习中的基础概念</li>
<li>完成实验环境的搭建</li>
<li>熟悉 SQL 的基本语法，能够处理比较复杂的查询（可选）</li>
<li>阅读推荐的书籍和文章（可选）</li>
</ol>
<p>注：很多概念对于刚入门的同学来说可能难以理解，不要担心，能明白多少是多少，随着课程的深入，回过头来看看就可以明白了。</p>
<h2 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h2><p>机器学习是一门理论结合实践的学科，甚至可以认为是更加偏重于理论的学科，因为没有对理论的深入了解，可能连现成的机器学习开源工具都学不好。这里列出一些必备基础知识与对应的书籍、课程推荐，方便大家查漏补缺（不会涉及特别艰深的数学）</p>
<p>熟悉机器学习的朋友会发现，线性代数和统计学实际上是机器学习中两种攀登路线的代表。</p>
<ul>
<li>代数方法研究函数和变换，如降维、特征提取、核工程</li>
<li>统计方法研究模型和分布，如图模型、熵模型</li>
</ul>
<p>作为非数学专业科班出身的程序员（比如我），虽然很多时候对于底层的数学理论只有基本的概念，但是了解不同方法背后的核心思想仍然是非常重要的。我建议是先从高层的理论和应用做起，需要深入的时候，再从最底层进行数学的补全，会稍微轻松一些（这句话的意思是涉及到数学的话无论怎么样都不会太轻松）</p>
<h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><p>推荐书籍</p>
<ul>
<li>Linear Algebra and Its Applications (3rd Ed.) by David C. Lay</li>
<li>Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang<ul>
<li><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm" target="_blank" rel="external">课程视频</a></li>
</ul>
</li>
</ul>
<p>线性代数在机器学习中最重要的不是矩阵运算和解方程的方法，而是理解矩阵及其背后的基础概念，这些概念会在之后的学习中不断出现，它们是：</p>
<ul>
<li>子空间 Subsapce</li>
<li>正交 Orthogonality</li>
<li>特征值和特征向量 Eigenvalues and Eigenvectors</li>
<li>线性变换 Linear transform</li>
</ul>
<p>详细内容会在『习题课』子系列中进行介绍。</p>
<h3 id="概率和统计"><a href="#概率和统计" class="headerlink" title="概率和统计"></a>概率和统计</h3><ul>
<li>《普林斯顿微积分读本》</li>
<li>《统计学习理论》李航</li>
<li>A First Course in Probability (9th Ed.) by Sheldon M. Ross</li>
<li>Probability and Statistics (5th Ed.) by Jay L. Devore</li>
</ul>
<p>概率和统计最重要的同样是对基本概念的理解，部分内容需要高等数学基础，《普林斯顿微积分读本》是挺好的复习，其他重要的概念有：</p>
<ul>
<li>概率论公理与随机变量</li>
<li>离散与连续</li>
<li>条件概率与联合分布</li>
<li>期望与极限定理</li>
<li>抽样、估计与模拟</li>
<li>回归与数据分析</li>
</ul>
<p>详细内容会在『习题课』子系列中进行介绍。</p>
<h2 id="机器学习基本概念"><a href="#机器学习基本概念" class="headerlink" title="机器学习基本概念"></a>机器学习基本概念</h2><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><ul>
<li>代表算法：KNN, NB, SVM, DT, BP, RF, GBRT（全称在文末列出）</li>
</ul>
<p>需要标注好的数据，必须知道目标变量的分类信息。对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。</p>
<p>举个例子，现在我有两百张猫和狗的图片，每张图片都已经标记好了是猫还是狗，这样机器可以从这两百张图片中『学习』到猫和狗的特征，这样再来一张猫或者狗的图片，机器能够根据之前学习的特征来判断这是猫还是狗。这里我们首先得知道有两种分类（猫、狗），且要分类的图片也必须在这两类之中。</p>
<h3 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h3><ul>
<li>代表算法：KMEANS, DL（全称在文末列出）</li>
</ul>
<p>数据没有类别信息，也没有目标变量，直接对未标记的样本进行训练学习，来发现我们之前不知道的结构知识。将数据集合分成由类似的对象组成的多个类的过程被称为<strong>聚类</strong>。</p>
<p>举个例子，我们收集一百篇新闻，把里面的词汇拆开，在同一篇文章里的词汇我们认为关联程度更高，这样把所有的词汇都统计一次，就会发现不同的词汇会根据大家习惯的用法而汇聚成不同的类别，比方说褒义词/贬义词。但这里我们没有事先进行标记（即指定哪个词是褒义），而是根据输入的数据自动聚集而成的类别。</p>
<h3 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h3><ul>
<li>代表算法：Graph Inference, Laplacian SVM</li>
</ul>
<p>简单来说就是一小部分数据有类别信息，一大部分数据没有，我们先利用一小部分有类别信息的数据为一大部分没有类别信息的数据进行分析和预测，然后用所有的数据来进行类别预测。一般用于数据量庞大且没办法完全标注的情况（图像识别）。</p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>代表算法：Q-Learning, Temporal difference learning</li>
</ul>
<p>主要用于机器人控制和系统控制领域，输入数据直接会模型的反馈，而不仅仅只是一个判断对错的标准。反馈之后，模型会对此进行调整。简单来说，和我们驯化动物的方式类似，做得好就给奖励，做得不好就给惩罚，这样模型就会根据这样的反馈进行调整。</p>
<h3 id="离散数据"><a href="#离散数据" class="headerlink" title="离散数据"></a>离散数据</h3><p>离散数据（标称型）的目标变量结果只在有限目标集中取值，一般用于<strong>分类</strong>。举个例子，袋子里有红、白、黑三种颜色的小球，每次摸出来一个，记下每次摸出来的颜色，那么这个数据就是一个离散数据，因为每次只可能是红、白、黑三种之一，不可能是其他的（即有限的目标集）。</p>
<h3 id="连续数据"><a href="#连续数据" class="headerlink" title="连续数据"></a>连续数据</h3><p>连续数据（数值型）目标变量主要用于<strong>回归</strong>分析，通过给定数据点的最优拟合曲线。举个例子，我们收集每天的最高温度数据，这个数据是一个连续数据，因为每天的最高温度的值可能是任何的数值（变化范围取决于精度），我们没有办法找到一个可能的值的集合。</p>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>一种概率论在统计学中的应用，主要用于参数评估。理论介绍可以参考<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="external">维基-最大似然估计</a>，这里我用一个简单的例子来进行说明。</p>
<p>假如我们拿到了一个骰子，但掷出每一面的概率不一定是相同的，在没有办法得到真实数值的情况下，我们如何进行估计呢，其中一个方法就是多掷几次看看。比方说我们掷了 600 次，其中：</p>
<ul>
<li>点数 1：100 次</li>
<li>点数 2：150 次</li>
<li>点数 3：150 次</li>
<li>点数 4：50 次</li>
<li>点数 5：50 次</li>
<li>点数 6：100 次</li>
</ul>
<p>从这样的分布来看，每一面的概率是 2:3:3:1:1:2，或者说，如果这个骰子每一面的概率是 2:3:3:1:1:2，那么更可能会掷出我们刚才的结果，所以就干脆把这个概率当做我们已知的最可能的概率，即最大似然估计。按照这个思路，如果我们要预测下一次可能掷出的点数，那么就更可能是点数 2 或 3。</p>
<h3 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h3><p>先验概率是利用过去历史资料计算得到的，一般来说比较准确。而后验概率是在先验概率的基础上增加了信息之后得到的概率，具体的理论介绍可以参考 <a href="https://en.wikipedia.org/wiki/Posterior_probability" target="_blank" rel="external">Wiki - Posterior probability</a>，这里同样使用一个简单的例子进行说明。</p>
<p>假设操场上有两个班各 50 人上体育课，每个班都有 30 个女生和 20 个男生，所有的男生今天都穿了短裤，而一个班的女生穿了短裤，另一个班的女生穿了长裤。忽然来了一个记者在远处抓拍了一张穿短裤但看不清男女的照片，问照片里的是女生的概率是多少。</p>
<p>那么这个概率，就是我们所说的后验概率了，是在先验概率的基础上（即女生占 60%）增加了穿短裤这个信息后需要计算的概率。那么现在来具体计算一下：</p>
<ul>
<li>设事件 A 是看到女生，那么 $P(A)=60\%$，看到男生的概率是 $P(A’)=40\%$</li>
<li>$P(B\;| \;A)$ 是女生穿短裤的概率，这里是 50%</li>
<li>$P(B\;| \;A’)$ 是男生穿短裤的概率，这里是 100%</li>
<li>设事件 B 是看到穿短裤的同学，那么 $P(B)=70\%$，看到穿长裤的同学的概率 $P(B’)=30\%$，这个 70% 是这么算出来的：$P(B)=P(B\;|\;A)P(A)+P(B\;|\;A’)P(A’)$，即『女生穿短裤的概率乘以女生的概率』+『男生穿短裤的概率乘以男生的概率』</li>
</ul>
<p>根据贝叶斯定理，我们可以计算出照片里是女生的概率（即穿短裤的这个人是女生的概率）为</p>
<p>$$P(A\;|\;B)= \frac{P(B\;|\;A)P(A)}{P(B)}= \frac{0.5 \times 0.6}{0.7}=0.4286$$</p>
<p>即照片里是女生的概率是 42.86%</p>
<h3 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h3><ul>
<li>代表算法：KNN, DT, SVM, NN, CRF, LDA(线性判别分析), LR, Boosting</li>
</ul>
<p>由数据直接学习决策函数 $Y=f(X)$ 或条件概率分布 $P(Y\;|\; X)$ 作为预测的模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。</p>
<ul>
<li>优点<ul>
<li>分类边界更灵活，比使用纯概率方法或生产模型得到的更高级</li>
<li>能清晰的分辨出多类或某一类与其他类之间的差异特征</li>
<li>适用于较多类别的识别</li>
<li>可以对数据进行各种程度上的抽象、定义特征并使用特征，简化学习问题</li>
</ul>
</li>
<li>缺点<ul>
<li>不能反映训练数据本身的特性</li>
<li>类似于黑盒，变量之间的关系不明确</li>
<li>无法处理包含隐变量的情况</li>
</ul>
</li>
</ul>
<h3 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h3><ul>
<li>代表算法：NB, GDA, HMM, BN, GMM, LDA(潜在狄利克雷分配)</li>
</ul>
<p>先来复习一下条件概率的公式：</p>
<p>$$P(Y\;|\; X)= \frac{P(X, Y)}{P(X)}$$</p>
<p>所谓生成模型，意思是我们不是直接得到公式左边的部分，而是通过求出公式右边分子分母的两项来得到后验概率 $P(Y\;|\; X)$ 并用它来进行分类。</p>
<ul>
<li>优点<ul>
<li>实际上带的信息要比判别模型丰富，有更强的解释力</li>
<li>研究单类问题比判别模型灵活性强</li>
<li>模型可以通过增量学习得到</li>
<li>能用于数据不完整的情况</li>
<li>可以处理存在隐变量的情况</li>
</ul>
</li>
<li>缺点：<ul>
<li>学习和计算过程比较复杂</li>
</ul>
</li>
</ul>
<p><strong>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</strong></p>
<h3 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h3><ul>
<li>常见线性分类器：LR, 贝叶斯分类，单层感知机，线性回归，SVM(线性核)</li>
</ul>
<p>模型是参数的线性函数，并且存在线性分类面。SVM 是比较特殊的一类，会根据核函数的不同而有不同。线性分类器速度快、编程方便，但是可能拟合效果不会很好。</p>
<ul>
<li>如果特征比数据量还大，那么选择线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</li>
<li>对于维度很高的特征，选择线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</li>
</ul>
<h3 id="非线性分类器"><a href="#非线性分类器" class="headerlink" title="非线性分类器"></a>非线性分类器</h3><ul>
<li>常见非线性分类器：DT, RF, GBDT, 多层感知机，SVM(高斯核)</li>
</ul>
<p>不属于线性分类器的就是非线性分类器，非线性分类器编程复杂，但是效果拟合能力强。</p>
<ul>
<li>对于维度极低的特征，选择非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分</li>
</ul>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。</p>
<p>产生原因</p>
<ul>
<li>因为参数太多，导致模型复杂度上升，容易过拟合</li>
<li>权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征</li>
<li>解决方法：交叉验证法、减少特征、正则化、权值衰减、验证数据</li>
</ul>
<p><strong>泛化能力是指模型对未知数据的预测能力</strong></p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>实际上是通过为要优化的函数增加一个模型复杂度的项目来权衡训练出来模型本身的结构化经验风险，可以防止训练出来的模型过度复杂，降低过拟合的风险。</p>
<p>比方说现在训练出两个模型，一个模型的准确率是 75%，一个是 85%，那么按照原来的选择标准，肯定选 85% 的那个。但是这个 85% 是针对训练数据得出来的模型，可能这个模型只对这些数据有用（也就是过拟合，而这些数据可能不具有很高的代表性），那么在实际使用的过程中，可以表现得比 75% 的那个还要糟糕。但是加入正则化之后，可能原先 75% 的模型会更加优秀，在一定程度上可以降低过拟合的概率。</p>
<p>正所谓奥卡姆剃刀原理所说的那样：<strong>能够很好的解释已知数据并且十分简单才是最好的模型</strong>。</p>
<p>作用是：</p>
<ol>
<li>数值上更容易求解；</li>
<li>特征数目太大时更稳定；</li>
<li>控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。</li>
<li>减小参数空间；参数空间越小，复杂度越低。</li>
<li>系数越小，模型越简单，而模型越简单则泛化能力越强。</li>
<li>可以看成是权值的高斯先验。</li>
</ol>
<p>一般常用的是 L1 和 L2 正则，用来降低模型复杂度：</p>
<ul>
<li>L1 是在损失函数后面加上模型参数的 1 范数（也就是  $|x_i|$ ）</li>
<li>L2 是在损失函数后面加上模型参数的 2 范数（也就是  $\Sigma(x_i^2)$ ），注意L2范数的定义是  $\sqrt{( \Sigma(x_i^2)}$ )，在正则项上没有添加根号是为了更加容易优化</li>
<li>L1 会产生稀疏的特征</li>
<li>L2 会产生更多地特征但是都会接近于 0</li>
</ul>
<p>L1 会趋向于产生少量的特征，而其他的特征都是 0，而 L2 会选择更多的特征，这些特征都会接近于 0。L1 在特征选择时候非常有用，而 L2 就只是一种规则化而已。</p>
<h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3><p>很多机器学习的问题到最后都会变成一类数学问题，而且是一类特定的数学问题，就是凸函数的优化问题。为什么呢？因为对于凸函数，我们已经拥有有效求解出全局最优值的方法，也就是我们常说的凸优化。不过在介绍凸优化之前，我们先了解一些前置基础概念：</p>
<p><strong>凸集</strong></p>
<p>一个集合 C 是凸集，当前仅当 $\forall x,y \in C$ 且 $0 \le \theta \le 1$ 时，都有 $\theta x+(1-\theta)y \in C$</p>
<p>在二维空间举例的话就是，C 是这样一个集合，集合中任意两点所组成的线段，也在集合 C 中，那么 C 是一个凸集（大家可以自己画画图感受一下）。</p>
<p><strong>凸函数</strong></p>
<p>有了凸集的定义，我们可以继续了解凸函数了。一个凸函数 f 其定义域 $D(f)$是凸集，并且对任意 $x,y \in D(f)$ 且 $0 \le \theta \le 1$ 都有<br>$f(\theta x+(1-\theta)y)&lt;=\theta f(x)+(1-\theta)f(y)$（称为 jensen 不等式）</p>
<p>还是用二维空间举例，就是函数曲线上任意两点的割线都在曲线的上方。那么为什么这个性质这么重要呢？我们画一个 U 形，是有一个最低点的，并且任意找两点组成的割线都在曲线的上方，随着我们缩小两点间的距离，我们是能够找到这个最低点的，也就是我们要的全局最优解！</p>
<p>常见的凸函数有</p>
<ul>
<li>指数函数 $f(x)=a^x \; when \;a&gt;1$</li>
<li>负对数函数  $-log_a x \; when \; a>1,x>0$ </li>
<li>开口向上的二次函数</li>
</ul>
<p>凸函数的判定方法为</p>
<ol>
<li>如果 f 是一阶可导的，且 $f’(x)$ 是递增函数，那么是凸函数</li>
<li>如果 f 是一阶可导的，对于任意数据域内的 x,y 满足 $f(y)\ge f(x)+f’(x)(y-x)$，那么是凸函数</li>
<li>如果 f 是二阶可导的，且在区间上非负，那么是凸函数</li>
</ol>
<p>凸函数的进阶——凸优化部分会专门介绍，这里暂时略过</p>
<h2 id="实验环境搭建"><a href="#实验环境搭建" class="headerlink" title="实验环境搭建"></a>实验环境搭建</h2><h3 id="Octave"><a href="#Octave" class="headerlink" title="Octave"></a><a href="https://www.gnu.org/software/octave/" target="_blank" rel="external">Octave</a></h3><p>Octave 可以看作是开源版的 Matlab，具体的安装教程可以参考 <a href="http://wiki.octave.org/Octave_for_MacOS_X" target="_blank" rel="external">Octave Wiki</a>，这里简单说一下使用 homebrew 进行安装的教程（比较简单的方式）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">brew tap homebrew/science</div><div class="line">brew update &amp;&amp; brew upgrade</div><div class="line">brew cask install xquartz</div><div class="line">brew install octave</div></pre></td></tr></table></figure>
<h3 id="Scikit-Learn"><a href="#Scikit-Learn" class="headerlink" title="Scikit-Learn"></a><a href="http://scikit-learn.org/stable/" target="_blank" rel="external">Scikit-Learn</a></h3><p>简单粗暴的方式是直接安装 <a href="https://www.enthought.com/products/canopy" target="_blank" rel="external">Canopy</a> 或 <a href="https://www.continuum.io/downloads" target="_blank" rel="external">Anaconda</a>，如果不想一次安装一堆可能一辈子都用不到的包，可以安装 <a href="http://conda.pydata.org/miniconda.html" target="_blank" rel="external">Miniconda</a>，因为我之前用过 Anaconda 觉得太臃肿，所以这里主要介绍 Miniconda 的安装和使用：</p>
<ul>
<li>下载安装文件，是 <code>.sh</code> 文件</li>
<li>执行 <code>bash Miniconda3-latest-MacOSX-x86_64.sh</code>，跟着步骤走即可</li>
<li>如果使用的是 zsh，那么把 <code>/Users/dawang/miniconda3/bin</code> 添加到 <code>~/.zshrc</code> 中的 PATH 变量中</li>
<li>输入 <code>conda list</code> 可以查看目前已经安装的包</li>
<li>安装 numpy <code>conda install numpy</code></li>
<li>安装 scipy <code>conda install scipy</code></li>
<li>安装 scikit-learn <code>conda install scikit-learn</code></li>
<li>如果想要了解更多，可以参考这里的 <a href="http://conda.pydata.org/docs/test-drive.html" target="_blank" rel="external">30 分钟入门教程</a></li>
<li>如果要更新，可以使用 <code>conda update conda</code></li>
<li>如果想要卸载，先删除整个文件夹 <code>rm -rf ~/miniconda</code>，然后更新 <code>.bash_profile</code>（或其他命令行）的 <code>PATH</code> 变量，最后删除临时文件 <code>rm -rf ~/.condarc ~/.conda ~/.continuum</code></li>
</ul>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>把 Octave 和 Scikit-Learn 官方的快速入门指南过一遍，有一个基础的认知</li>
<li>记下不太了解的名词，留着以后遇到了重点看</li>
<li>回忆一下大数定理和中心极限定理，那些年的高数还记得吗</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这节课中我们简单介绍了学习机器学习所需要的数学基础以及机器学习中常见的基本概念，最后搭建好了实验环境。接下来，我们就可以正式进入机器学习的世界了。</p>
<h2 id="附：缩写对照表"><a href="#附：缩写对照表" class="headerlink" title="附：缩写对照表"></a>附：缩写对照表</h2><ul>
<li>BP - Back Propagation 误差反向传播算法</li>
<li>BN - Bayes Network 贝叶斯网络</li>
<li>CRF - Conditional Random Field 条件随机场</li>
<li>DL - Deep Learning 深度学习</li>
<li>DT - Decision Tree 决策树</li>
<li>GBDT/GBRT/MART - Gradient Boosting Decision Tree </li>
<li>GDA - Gaussian Discriminative Analysis 高斯判别分析</li>
<li>GMM - Gaussian Mixture Model 高斯混合模型</li>
<li>KMEANS - K 均值</li>
<li>KNN - K Nearest Neighbor K 近邻</li>
<li>LDA 潜在狄利克雷分配 or 线性判别分析</li>
<li>LR 线性回归 or Logistic Regression</li>
<li>NB - Naive Bayes 朴素贝叶斯</li>
<li>NN - Neural Network 神经网络</li>
<li>RF - Random Forest 随机森林</li>
<li>SVM - Support Vector Machine 支持向量机</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://kubicode.me/2015/08/16/Machine%20Learning/Common-Interview/" target="_blank" rel="external">机器学习常见面试题整理</a></li>
<li><a href="http://www.cnblogs.com/TenosDoIt/p/3721074.html" target="_blank" rel="external">机器学习：判别模型与生成模型</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;欢迎走进机器学习的世界！『不周山之机器学习』系列会结合原理与实践，在弄懂机器学习理论的前提下，把抽象的机器学习理论给用起来。这一讲是系列正文的开端，主要介绍开始学习机器学习的预备知识和相关学习资料，以及搭建好我们用于实践算法的环境。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="基础" scheme="http://wdxtub.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="环境" scheme="http://wdxtub.com/tags/%E7%8E%AF%E5%A2%83/"/>
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="机器学习" scheme="http://wdxtub.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>『不周山』研究作品集介绍</title>
    <link href="http://wdxtub.com/2016/11/24/buzhoushan-series-intro/"/>
    <id>http://wdxtub.com/2016/11/24/buzhoushan-series-intro/</id>
    <published>2016-11-24T01:09:09.000Z</published>
    <updated>2016-11-24T13:33:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>『不周山』作品集源于『通天塔』系列第一部作品完成度过半而萌生的想法，如果说『通天塔』作品集强调实践，那么『不周山』作品集则是强调理论。本文聊聊我做这事儿的『初心』。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.24: 初稿完成</li>
</ul>
<h2 id="为什么要写？"><a href="#为什么要写？" class="headerlink" title="为什么要写？"></a>为什么要写？</h2><p>不周山源于《山海经·大荒西经》：</p>
<blockquote>
<p>西北海之外，大荒之隅，有山而不合，名曰不周。</p>
</blockquote>
<p>相传不周山是人界唯一能够到达天界的路径，但终年寒冷，长年飘雪，非凡夫俗子所能徒步到达。而在《淮南子·天文训》中，是这样介绍共工怒撞不周山的：</p>
<blockquote>
<p>昔者共工与颛顼争为帝，怒而触不周之山，天柱折、地维绝，天倾西北，故日月星辰移焉；地不满东南，故水潦尘埃归焉。</p>
</blockquote>
<p>斗转星移，诗人毛泽东也曾提及不周山，因为历史背景的，他对共工的感情和传统史书中不太一样：</p>
<blockquote>
<p>唤起工农千百万，同心干，不周山下红旗乱。</p>
</blockquote>
<p>之所以为作品集取名『不周山』，除了表达在追求知识的道路上永远是『不周』之外，更多的是希望借用其中的隐喻，虽不能从人界到天界，但能『更上一层楼』，也算是为『欲穷千里目』做了一些努力。</p>
<p>兴趣是最好的老师，『不周山』作品集想成为的是对计算机学科感兴趣的同学的『助教』，至少让有志于此的朋友们在追求知识的道路上不太孤单。</p>
<p>我不喜欢培训班的短平快，也痛心于高校与业界的脱节，想找到一种方式，能够把原理和基础说明白的同时，通过实际可操作的案例来让大家更加深入理解数学之美与计算机科学之美。</p>
<p>于是便有了『不周山』这个系列。</p>
<h2 id="能给读者带来什么？"><a href="#能给读者带来什么？" class="headerlink" title="能给读者带来什么？"></a>能给读者带来什么？</h2><p>以下几点是我非常想要借助『不周山』系列带给读者的：</p>
<ul>
<li>学以致用的思维与能力</li>
<li>好奇心与打破沙锅问到底</li>
<li>数学基础知识与应用</li>
<li>概率论基础知识与应用</li>
<li>业界流行风潮背后的理论基础</li>
<li>发现问题解决问题的能力</li>
<li>安排计划，学会学习的能力</li>
</ul>
<p>当然，博客的形式还是有较多局限的，对读者的要求也比较高，不会有人催促，甚至也不会有及时的答疑，一切靠自己。另外，理论和原理很多时候比较枯燥，公式的记忆和推导也需要花费大量的时间。不过这是对自己的长远投资，想要提升自己的天花板，一定得熬过这个阶段。</p>
<h2 id="主要写什么？"><a href="#主要写什么？" class="headerlink" title="主要写什么？"></a>主要写什么？</h2><p>基于自己的工作和实践，目前的已经列入计划的有：</p>
<ul>
<li>机器学习：从公式推导开始，步步为营介绍常用的机器学习算法原理</li>
<li>推荐系统：从实际场景出发，介绍常见的推荐系统背后的算法原理</li>
<li>云计算：着重介绍虚拟化技术，也就是云计算的理论基础</li>
<li>算法与数据结构：计算机科学基础，从更高层次理解基础元素</li>
<li>读薄/读厚 CSAPP：老系列的翻新之作，结合全新的中文版进行增补</li>
</ul>
<p>如果大家有任何意见或者建议，欢迎以各种方式跟我聊聊，联系方式可以在<a href="http://wdxtub.com/thanks/">这里</a>找到</p>
<h2 id="写作格式"><a href="#写作格式" class="headerlink" title="写作格式"></a>写作格式</h2><p>每一篇都会包含：</p>
<ul>
<li>系列目录：方便查阅</li>
<li>任务目标：带着目的学习</li>
<li>试一试：实践部分</li>
<li>总结：回顾学过的内容</li>
</ul>
<p>这四个固定模块，完成每篇文章的任务后，都会有一个可交付可展示的东西，像打怪升级一样，以此鼓励大家。</p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>『不周山』作品集会是一个长期的项目，希望对此感兴趣的同学和朋友能够以远程合作的形式参与进来，众人拾柴火焰高嘛。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;『不周山』作品集源于『通天塔』系列第一部作品完成度过半而萌生的想法，如果说『通天塔』作品集强调实践，那么『不周山』作品集则是强调理论。本文聊聊我做这事儿的『初心』。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="原理" scheme="http://wdxtub.com/tags/%E5%8E%9F%E7%90%86/"/>
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
  </entry>
  
  <entry>
    <title>『通天塔』技术作品集介绍</title>
    <link href="http://wdxtub.com/2016/11/19/babel-series-intro/"/>
    <id>http://wdxtub.com/2016/11/19/babel-series-intro/</id>
    <published>2016-11-19T04:12:12.000Z</published>
    <updated>2016-11-24T13:33:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>『通天塔』作品集源于我和华章两位老师的畅谈后萌生的想法，是我在计算机学科教育上一系列尝试的第一步。本文聊聊我做这事儿的『初心』。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.19: 初稿完成</li>
</ul>
<h2 id="为什么要写？"><a href="#为什么要写？" class="headerlink" title="为什么要写？"></a>为什么要写？</h2><p>通天塔也叫<a href="https://zh.wikipedia.org/wiki/%E5%B7%B4%E5%88%A5%E5%A1%94" target="_blank" rel="external">巴别塔</a> ，传说中的故事是这样的：</p>
<blockquote>
<p>当时地上的人们都说同一种语言，当人们离开东方之后，他们来到了示拿之地。在那里，人们想方设法烧砖好让他们能够造出一座城和一座高耸入云的塔来传播自己的名声，以免他们分散到世界各地。上帝来到人间后看到了这座城和这座塔，说一群只说一种语言的人以后便没有他们做不成的事了；于是上帝将他们的语言打乱，这样他们就不能听懂对方说什么了，还把他们分散到了世界各地，这座城市也停止了修建。这座城市就被称为“巴别城”。（来自维基百科）</p>
</blockquote>
<p>之所以为作品集取名『通天塔』，当然是希望能借用其中的隐喻，即使不能『通天』，能站得高一点，也许能看到更好的风景。</p>
<p>兴趣是最好的老师，『通天塔』作品集想成为的是对计算机学科感兴趣的同学的『助教』，至少让有志于此的朋友们在追求知识的道路上不太孤单。</p>
<p>我不喜欢培训班的短平快，也痛心于高校与业界的脱节，想找到一种方式，能够把原理和基础说明白的同时，通过实际可操作的案例来让大家意识到自己学习的东西是有用的，凭借自己的努力可以打造出不一样的东西。</p>
<p>于是便有了『通天塔』这个系列。</p>
<h2 id="能给读者带来什么？"><a href="#能给读者带来什么？" class="headerlink" title="能给读者带来什么？"></a>能给读者带来什么？</h2><p>以下几点是我非常想要借助『通天塔』系列带给读者的：</p>
<ul>
<li>学以致用的思维与能力</li>
<li>好奇心与打破沙锅问到底</li>
<li>计算机系统基础知识的理解</li>
<li>业界常见解决方案的使用</li>
<li>架构和系统设计的思路</li>
<li>发现问题解决问题的能力</li>
<li>安排计划，学会学习的能力</li>
</ul>
<p>当然，博客的形式还是有较多局限的，对读者的要求也比较高，不会有人催促，甚至也不会有及时的答疑，一切靠自己。另外，系统和解决方案的构建大多是从单机开始再拓展到集群，在流程和规范上肯定不如大公司来的专业，不过只要知道了原理，其实工作一段时间自然就会掌握。</p>
<h2 id="主要写什么？"><a href="#主要写什么？" class="headerlink" title="主要写什么？"></a>主要写什么？</h2><p>基于自己的工作和实践，目前的已经列入计划的有：</p>
<ul>
<li>日志分析平台：基于 ElasticStack</li>
<li>数据平台：存储采用 Elasticsearch，后端用 Go，前端用 jQuery</li>
<li>静态博客：打造自己的品牌</li>
<li>W.I.S.E：详情可见<a href="http://wdxtub.com/2016/10/17/wise-plan/">W.I.S.E 计划</a></li>
</ul>
<p>如果大家有任何意见或者建议，欢迎以各种方式跟我聊聊，联系方式可以在<a href="http://wdxtub.com/thanks/">这里</a>找到</p>
<h2 id="写作格式"><a href="#写作格式" class="headerlink" title="写作格式"></a>写作格式</h2><p>每一篇都会包含：</p>
<ul>
<li>系列目录：方便查阅</li>
<li>任务目标：带着目的学习</li>
<li>试一试：实践部分</li>
<li>总结：回顾学过的内容</li>
</ul>
<p>这四个固定模块，完成每篇文章的任务后，都会有一个可交付可展示的东西，像打怪升级一样，以此鼓励大家。</p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>『通天塔』作品集会是一个长期的项目，希望对此感兴趣的同学和朋友能够以远程合作的形式参与进来，众人拾柴火焰高嘛。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;『通天塔』作品集源于我和华章两位老师的畅谈后萌生的想法，是我在计算机学科教育上一系列尝试的第一步。本文聊聊我做这事儿的『初心』。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="原理" scheme="http://wdxtub.com/tags/%E5%8E%9F%E7%90%86/"/>
    
      <category term="实践" scheme="http://wdxtub.com/tags/%E5%AE%9E%E8%B7%B5/"/>
    
      <category term="尝试" scheme="http://wdxtub.com/tags/%E5%B0%9D%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】零 系列简介与环境配置</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/</id>
    <published>2016-11-19T03:11:11.000Z</published>
    <updated>2016-11-21T14:27:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>作为开篇，本文会介绍『日志分析平台』系列的内容梗概并完成基本的环境配置。作为『通天塔』这一技术主题合集的首个系列，我会尝试和『读薄/读厚 CSAPP』系列不一样的风格，但是目的是一致的，就是让感兴趣的朋友少走点弯路。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.19: 初稿完成</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>理解日志分析平台出现的背景</li>
<li>掌握日志从收集、传输到最终统一处理的基本流程中的重要概念</li>
<li>了解 ElasticStack 的各个组成部分及对应的角色</li>
<li>配置好 Linux 基本环境，为之后的工作打好基础</li>
</ol>
<h2 id="什么是日志分析平台"><a href="#什么是日志分析平台" class="headerlink" title="什么是日志分析平台"></a>什么是日志分析平台</h2><p>要回答这个问题，先得弄清楚什么是日志。于是让我们把记忆拉回刚学编程那会儿，想想当时我们是如何写程序运行程序的。具体很多细节我已经记不太清楚，但是把需要检测的变量用 <code>printf</code> 输出到命令行这个简单粗暴的方法，到现在我还时不时会用到。这其实就可以看做是一个『记日志』的行为，虽然非常不靠谱，但是仍提供给我们一些有用的信息。</p>
<p>代码多了之后，想要弄清楚程度到底在干嘛，干到哪一步了，最好的方法就是在每一步的时候输出一些信息，这样出了问题至少能够知道最后运行正常的部分。除了排错之外，日志本身也能给我们提供非常有价值的信息，比方说服务器提供了 100 个对外接口（假设这些接口是并行的，即关闭哪个都无所谓），忽然老板说我们不能提供这么多，只能保留 50 个。那怎么确定要关闭哪五十个呢？其中一个方法就是把访问次数最少的那些给干掉。这时候我们就可以把过去一个星期的日志找出来，统计一下各个接口的使用情况（假设每次接口被访问都会生成一条日志），然后就能排个序，确定需要去掉的接口了。</p>
<p>回顾一下这整个过程：</p>
<ol>
<li>我们提供一些服务，这些服务每被访问一次都会生成一条日志</li>
<li>一般来说我们会把程序产生的日志按日切割，也就是每天会生成一个新的日志文件</li>
<li>有的时候我们需要对大量日志进行统计以得到某些数据</li>
</ol>
<p>当我们的服务只部署在一台服务器上的时候，所有的日志都在同一个地方，基本的统计可以通过 shell 命令配合管道完成。比方说我们想知道接口每天被访问的次数，直接 <code>wc -l date.log</code> 即可，完全不需要费心去折腾什么日志分析平台。但是，随着服务量的增长，原来一行可以搞定的事情变得非常麻烦。</p>
<p>当我们的服务部署在十台服务器上的时候，日志分散在十个地方，基本的统计首先需要在每台服务器上进行，然后再汇总起来。用前面的例子，统计次数的过程就是把原来的命令在十个地方敲十次。这其实还不是最糟的，如果需要跨机器排个序什么的，就…</p>
<p>所以这个时候，日志分析平台应运而生，一般来说套路分三步：</p>
<ol>
<li>把分散在各个机器的日志汇总到一个地方(Shipper, Broker, Indexer)</li>
<li>把这些日志用某种方式保存并索引起来(Search &amp; Storage)</li>
<li>需要的时候直接在汇总的日志中查询(Web Interface)</li>
</ol>
<p><img src="/images/14795970842446.gif" alt=""></p>
<p>听起来没有很麻烦，因为原理大约总是简单的，但具体到做工程，就有各种问题各种坑了。我个人是不提倡自己重新造轮子的（确实没必要），除了现在很多现成的日志分析平台服务之外，我们也可以选择利用开源的力量自己搭建一个日志分析平台。</p>
<p>这也是正是这个系列想要教给大家的。我会从单机系统说起，最后扩展到集群和更复杂的解决方案。</p>
<h2 id="为什么选择-ElasticStack"><a href="#为什么选择-ElasticStack" class="headerlink" title="为什么选择 ElasticStack"></a>为什么选择 ElasticStack</h2><p>（开个玩笑）原因很简单：因为我在用。</p>
<p>（言归正传）ElasticStack 经过这几年的快速发展，版本号一路从 1.0 狂飙到 5.0（这个真的不是在黑），基本上形成了和 <a href="http://flume.apache.org/" target="_blank" rel="external">Flume</a>分庭抗礼的局面。至于为什么，可能是因为大家都喜欢简单粗暴颜高活好不粘人的解决方案吧。</p>
<p>ElasticStack 最初的核心是 ELK(Elasticsearch, Logstash, Kibana) 三兄弟。其中 Logstash 收集数据，Elasticsearch 索引数据，Kibana 展示数据。</p>
<ul>
<li>Elasticsearch 背靠 Lucene 这一老牌劲旅做到了准实时全文索引</li>
<li>Logstash 的配置直接是 Ruby DSL，非常灵活简单</li>
<li>Kibana 则自带各种查询聚合以及生成报表功能。</li>
</ul>
<p>再加上查询简单、扩展容易之类的特点，大受欢迎其实也在情理之中。官方也在不断吸收社区精华的同时开发了安全、报警、监控、报告等一系列功能，再加上能够轻松和 Hadoop 这类分布式计算框架配合，怎么看都是非常不错的选择。</p>
<h2 id="系列内容"><a href="#系列内容" class="headerlink" title="系列内容"></a>系列内容</h2><p>『通天塔之日志分析平台』这个系列的主要是内容是和大家一起一步一步搭建起来一个完整的日志分析平台，具体的内容通过前面的目录应该能够略知一二，会包含业界通用的解决方案，在介绍原理的同时，每一章都会有一定的产出，这样在学习的时候比较不容易懈怠。</p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>关于 ElasticStack 的更多详细介绍会在接下来的文章中继续，现在我们先把系统准备好吧。考虑到现在大部分服务器都在跑 Linux，所以本文会以 Ubuntu 64bit 14.04 这个长期支持版本来作为我们的操作系统。我目前在用的是 MacBook Pro(Retina, 13’, Late 2013)，8GB 内存 2.4 GHz 的 i5，在虚拟机里跑 Ubuntu。</p>
<p>ElasticStack 对系统和软件的配置要求并不高，我们只需要安装 JDK 即可。可以用如下的命令或者是我已经写好的脚本<a href="https://github.com/wdxtub/wdxtools/blob/master/linux-script/ubuntu-java-install.sh" target="_blank" rel="external"><code>ubuntu-java-install.sh</code></a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 添加源 </div><div class="line">sudo add-apt-repository -y ppa:webupd8team/java</div><div class="line"># 更新地址 </div><div class="line">sudo apt-get update</div><div class="line"># 安装 </div><div class="line">sudo apt-get -y install oracle-java8-installer</div></pre></td></tr></table></figure>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><p>因为是序章，所以实践的任务比较简单：</p>
<ul>
<li>在命令行中输入 <code>java -version</code>，看看输出是什么</li>
<li>访问 elastic 的<a href="https://www.elastic.co/" target="_blank" rel="external">官方网站</a>，并简单浏览各个产品的信息</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>相信大家现在已经对我们接下来要做的『日志分析平台』有基本的概念了，如果还有不明白的地方也不要担心，带着未知往前走，其实也是非常有意思的过程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为开篇，本文会介绍『日志分析平台』系列的内容梗概并完成基本的环境配置。作为『通天塔』这一技术主题合集的首个系列，我会尝试和『读薄/读厚 CSAPP』系列不一样的风格，但是目的是一致的，就是让感兴趣的朋友少走点弯路。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="环境" scheme="http://wdxtub.com/tags/%E7%8E%AF%E5%A2%83/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】壹 ELK 环境搭建</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/</id>
    <published>2016-11-19T03:11:10.000Z</published>
    <updated>2016-11-22T12:48:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>前一讲我们对 ElasticStack 进行了简要介绍并完成了基本的系统环境配置，这一次我们要把 Elasticsearch/Logstash/Kibana 安装配置好，并把 Linux 的系统日志导入进来。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.21: 完成初稿</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>掌握并完成 Elasticsearch, Logstash 和 Kibana 的安装配置</li>
<li>了解 Linux 系统日志的内容及保存位置，并利用 Logstash 导入到 Elasticsearch 中，最终由 Kibana 展示</li>
<li>掌握 Linux 的进程控制机制，学会如何启动和关闭前台/后台应用</li>
</ol>
<h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>无论是 Elasticsearch, Logstash 还是 Kibana，我们都推荐手动安装的方式，毕竟不涉及太多配置操作，用 <code>apt-get</code> 反而有些用牛刀杀鸡了。这里我们直接上命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 进入用户文件夹</span></div><div class="line"><span class="built_in">cd</span> ~</div><div class="line"><span class="comment"># 下载 Elasticsearch</span></div><div class="line">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.0.1.tar.gz</div><div class="line"><span class="comment"># 下载 Logstash</span></div><div class="line">wget https://artifacts.elastic.co/downloads/logstash/logstash-5.0.1.tar.gz</div><div class="line"><span class="comment"># 下载 Kibana</span></div><div class="line">wget https://artifacts.elastic.co/downloads/kibana/kibana-5.0.1-linux-x86_64.tar.gz</div><div class="line"></div><div class="line"><span class="comment"># 解压 </span></div><div class="line">tar -xvf elasticsearch-5.0.1.tar.gz</div><div class="line">tar -xvf logstash-5.0.1.tar.gz</div><div class="line">tar -xvf kibana-5.0.1-linux-x86_64.tar.gz</div><div class="line"></div><div class="line"><span class="comment"># 把安装包保存到固定文件夹中，这里叫 software</span></div><div class="line">mv elasticsearch-5.0.1.tar.gz software/mv kibana-5.0.1-linux-x86_64.tar.gz software/mv logstash-5.0.1.tar.gz software/</div></pre></td></tr></table></figure>
<p>解压完成之后，ElasticStack 运行前的准备就基本完成了。Logstash 可以在需要时再启用，这里我们先把 Elasticsearch 和 Kibana 给启动起来（这里我们继续用 tmux，关于 tmux 的使用可以参考我写的<a href="http://wdxtub.com/2016/03/30/tmux-guide/">tmux 指南</a>）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 新建 tmux session</span></div><div class="line">tmux</div><div class="line"><span class="comment"># 启动 Elasticsearch</span></div><div class="line"><span class="comment"># 这里注意，最好虚拟机有 4G 内存，不然很容易卡死</span></div><div class="line"><span class="built_in">cd</span> elasticsearch-5.0.1/bin; ./elasticsearch</div><div class="line"><span class="comment"># 启动 Kibana</span></div><div class="line"><span class="built_in">cd</span> kibana-5.0.1-linux-x86_64/; ./bin/kibana</div></pre></td></tr></table></figure>
<p>打开浏览器，访问 <code>localhost:5601</code>，如果看到如下所示的页面，Elasticsearch 和 Kibana 基本就没问题了。</p>
<p><img src="/images/14797108735042.jpg" alt=""></p>
<p>然后我们体验一下 Logstash，输入下列命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 进入文件夹</span></div><div class="line"><span class="built_in">cd</span> logstash-5.0.1/</div><div class="line"><span class="comment"># 启动 logstash，输入和输出均为命令行</span></div><div class="line">bin/logstash <span class="_">-e</span> <span class="string">'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;'</span></div></pre></td></tr></table></figure>
<p>然后我们随意输入一些内容，显示为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">parallels@ubuntu:~/logstash-5.0.1$ bin/logstash <span class="_">-e</span> <span class="string">'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;'</span>wdxtub.com updatedSending Logstash<span class="string">'s logs to /home/parallels/logstash-5.0.1/logs which is now configured via log4j2.propertiesThe stdin plugin is now waiting for input:[2016-11-20T22:54:20,014][INFO ][logstash.pipeline        ] Starting pipeline &#123;"id"=&gt;"main", "pipeline.workers"=&gt;2, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;5, "pipeline.max_inflight"=&gt;250&#125;[2016-11-20T22:54:20,038][INFO ][logstash.pipeline        ] Pipeline main started[2016-11-20T22:54:20,088][INFO ][logstash.agent           ] Successfully started Logstash API endpoint &#123;:port=&gt;9600&#125;2016-11-21T06:54:20.036Z ubuntu wdxtub.com updatedwdxtub.com is a personal blog2016-11-21T06:54:41.187Z ubuntu wdxtub.com is a personal blogwdxtub.com is created in 20152016-11-21T06:54:55.190Z ubuntu wdxtub.com is created in 2015</span></div></pre></td></tr></table></figure>
<p>至此，ElasticStack 三大组件都已经运行了一次，我们可以用一个实际的任务来上手了。不过开始之前，我们来简单了解一下 ElasticStack 的发展历程。</p>
<h2 id="ElasticStack-5-0-的变化"><a href="#ElasticStack-5-0-的变化" class="headerlink" title="ElasticStack 5.0 的变化"></a>ElasticStack 5.0 的变化</h2><p>ElasticStack 之所以版本一开始就是 5.0，主要原因是把各个产品进行版本统一。5.0 之前，Elasticsearch 的版本是 2.4，Logstash 的版本也是 2.4，但是 Kibana 是 4.5。这样一来开发者其实很难把各个组件对应起来，于是 Elastic 公司干脆直接统一到 5.0，皆大欢喜。</p>
<p>考虑到不是所有的朋友都有接触过 2.4 及之前版本的 ElasticStack，所以这部分会简明扼要介绍一下 5.0 版本的重大改变：</p>
<ul>
<li>Elasticsearch 的底层引擎是 Lucene，5.0 版本中集成了 Lucene6， 新增的多维浮点字段特性极大提高了对 <code>date</code>, <code>numeric</code>, <code>ip</code> 等类型字段的操作的性能。更直观一点说：磁盘空间少一半；索引时间少一半；查询性能提升25%（底层采用 k-d 树编码，更多信息可以在 <a href="http://lucene.apache.org/" target="_blank" rel="external">Lucene 官网</a>中查阅）</li>
<li>Instant Aggregations 特性在 Shard 层级提供了聚合结果的缓存，如果数据没有变化，Elasticsearch 可以直接返回上次的结果</li>
<li>Scliced Scroll 操作允许并发进行数据遍历，大大提升索引重建和遍历的速度</li>
<li>Profile API 可以帮助进行查询的优化，通过确定每个组件的性能消耗来进行优化（设置 <code>profile:true</code> 即可）</li>
<li>Shrink API 可以对分片(Shard)数量进行收缩（从前是不能更改的），利用这个特性，我们可以在写入压力非常大的收集阶段，设置足够多的索引，充分利用shard的并行写能力，索引写完之后收缩成更少的shard，提高查询性能（利用系统的 Hardlink 来进行链接，速度很快）</li>
<li>Rollover API 可以帮助我们按日切割日志，只需要简单的配置即可更加方便灵活分割日志，不用原来 <code>[YYYY-MM-DD]</code> 这样的模板了</li>
<li>Wait for Refresh 提供了文档级别的刷新</li>
<li>Ingest Node 可以直接在建立索引的时候对数据进行加工，这个功能还是很强大的</li>
<li>Task Manager 任务调度管理，来做离线任务</li>
</ul>
<p>总而言之，5.0 是目前最好的一个 ElasticStack 版本，增加了很多新功能，非常值得试一试，更加详细的变动可以查看如下链接，这里不再赘述</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes.html" target="_blank" rel="external">Elasticsearch 5.0 Breaking Changes</a></li>
<li><a href="https://www.elastic.co/guide/en/kibana/current/breaking-changes.html" target="_blank" rel="external">Kibana 5.0 Breaking Changes</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/breaking-changes.html" target="_blank" rel="external">Logstash 5.0 Breaking Changes</a></li>
</ul>
<p>接下来我们先简单了解一下 Elasticsearch 的基本概念，然后就可以上手来完成一个小小的实例了。</p>
<h2 id="Elasticsearch-快速入门"><a href="#Elasticsearch-快速入门" class="headerlink" title="Elasticsearch 快速入门"></a>Elasticsearch 快速入门</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>和 Mongodb 的思路类似，Elasticsearch 中保存的是整个文档(document)，并且还会根据文档的内容进行索引，于是我们得以进行搜索、排序和过滤等操作。在 Elasticsearch 中，利用 JSON 来表示文档。举个例子，下面的 JSON 文档就表示一个用户对象：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"email"</span>: <span class="string">"dawang@wdxtub.com"</span>,</div><div class="line">    <span class="attr">"name"</span>: <span class="string">"Da Wang"</span>,</div><div class="line">    <span class="attr">"info"</span>: &#123;</div><div class="line">        <span class="attr">"bio"</span>: <span class="string">"Sharp Blade, Shape Mind"</span>,</div><div class="line">        <span class="attr">"age"</span>: <span class="string">"25"</span>,</div><div class="line">        <span class="attr">"interests"</span>: [<span class="string">"games"</span>, <span class="string">"music"</span>]</div><div class="line">    &#125;,</div><div class="line">    <span class="attr">"birthday"</span>: <span class="string">"1990/09/11"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 Elasticsearch 中存储数据的行为就叫做索引(indexing)，而前面提到的文档，属于一种类型(type)，这里类型会存在索引(index)中，如果列一个表来和传统数据库比较，大概是这样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">关系型数据</th>
<th style="text-align:center">Elasticsearch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Databases</td>
<td style="text-align:center">Indices</td>
</tr>
<tr>
<td style="text-align:center">Tables</td>
<td style="text-align:center">Types</td>
</tr>
<tr>
<td style="text-align:center">Rows</td>
<td style="text-align:center">Documents</td>
</tr>
<tr>
<td style="text-align:center">Columns</td>
<td style="text-align:center">Fields</td>
</tr>
</tbody>
</table>
<p>一个 Elasticsearch 集群可以包含多个索引(indices，对应于『数据库』)，每个索引可以包含多个类型(types，对应于『表』)，每个类型可以包含多个文档(document，对应于『行』)，每个文档可以包含多个字段(fields，对应于『列』)</p>
<p>这里有一点需要强调一下，前面出现了两种『索引』，第一种，索引(indexing，动词，对应于关系型数据库的插入 insert)指的是把一个文档存储到索引(index，名词) 中；第二种的索引(index，名词）对应于关系型数据库的数据库，这里一定要根据上下文来进行理解。一般来说，我们会对数据库增加索引（这里是第三种意思，就是传统的索引的定义）来提高检索效率，Elasticsearch 和 Lucene 使用『倒排索引』的数据结构来完成这个工作（传统数据库一般用红黑树或者 B 树来完成）。默认情况下，文档中的每个字段都会拥有其对应的倒排索引，Elasticsearch 也是通过这个来进行检索的。</p>
<h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h3><p>我们用一个简单的例子来感受一下 Elasticsearch 的威力吧。设定一个场景，有一天我开了一家名为 “ohmywdx” 的公司，我需要为每个公司里的员工创建记录，我需要做的是：</p>
<ul>
<li>为每个员工的文档(document)建立索引，每个文档包含一个员工的各类信息，类型为 <code>wdxtuber</code></li>
<li><code>wdxtuber</code> 类型属于索引 <code>ohmywdx</code>（这里的索引对应于数据库）</li>
<li><code>ohmywdx</code> 索引存储在 Elasticsearch 集群中</li>
</ul>
<p>我们先来插入几条员工记录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">PUT /ohmywdx/wdxtuber/1</div><div class="line">&#123;</div><div class="line">    <span class="string">"name"</span>: <span class="string">"Da Wang"</span>,</div><div class="line">    <span class="string">"age"</span>: 25,</div><div class="line">    <span class="string">"about"</span>: <span class="string">"First one who is stupid enough to join this company"</span>,</div><div class="line">    <span class="string">"interests"</span>: [<span class="string">"game"</span>, <span class="string">"music"</span>]</div><div class="line">&#125;</div><div class="line"></div><div class="line">PUT /ohmywdx/wdxtuber/2</div><div class="line">&#123;</div><div class="line">    <span class="string">"name"</span>: <span class="string">"Tracy Bryant"</span>,</div><div class="line">    <span class="string">"age"</span>: 20,</div><div class="line">    <span class="string">"about"</span>: <span class="string">"First basketball robot for our company"</span>,</div><div class="line">    <span class="string">"interests"</span>: [<span class="string">"guard"</span>, <span class="string">"forward"</span>]</div><div class="line">&#125;</div><div class="line"></div><div class="line">PUT /ohmywdx/wdxtuber/3</div><div class="line">&#123;</div><div class="line">    <span class="string">"name"</span>: <span class="string">"Shadow Mouse"</span>,</div><div class="line">    <span class="string">"age"</span>: 50,</div><div class="line">    <span class="string">"about"</span>: <span class="string">"Secret agent for our company"</span>,</div><div class="line">    <span class="string">"interests"</span>: [<span class="string">"guitar"</span>, <span class="string">"sugar"</span>]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>具体怎么插入呢，我们可以使用 Kibana 5.0 自带的 Dev Tools 来折腾，把上面的命令粘贴到左边的输入框，然后点击绿色的运行按钮，就可以在右边看到结果了：</p>
<p><img src="/images/14797196056865.jpg" alt=""></p>
<p>按照这个套路，继续把其他两个人的资料插入 Elasticsearch。</p>
<p>有了数据之后，我们来看看如何搜索，简单来说，按照存储的方式来检索即可，不过这里我们使用 GET 方法，如下图所示：</p>
<p><img src="/images/14797197975730.jpg" alt=""></p>
<p>我们可以看到，原始文档内容包含在 <code>_source</code> 字段中。如果说这个搜索太明确了，啥都指定了没意思，我们可以来试试看下面几条搜索</p>
<ul>
<li><code>GET /ohmywdx/wdxtuber/_search</code></li>
<li><code>GET /ohmywdx/wdxtuber/_search?q=name:Da</code></li>
</ul>
<p>这里我们来看看第二个搜索的结果</p>
<figure class="highlight json"><table><tr><td class="code"><pre><div class="line">&#123;  <span class="attr">"took"</span>: <span class="number">11</span>,  <span class="attr">"timed_out"</span>: <span class="literal">false</span>,  <span class="attr">"_shards"</span>: &#123;    <span class="attr">"total"</span>: <span class="number">5</span>,    <span class="attr">"successful"</span>: <span class="number">5</span>,    <span class="attr">"failed"</span>: <span class="number">0</span>  &#125;,  <span class="attr">"hits"</span>: &#123;    <span class="attr">"total"</span>: <span class="number">1</span>,    <span class="attr">"max_score"</span>: <span class="number">0.25811607</span>,    <span class="attr">"hits"</span>: [      &#123;        <span class="attr">"_index"</span>: <span class="string">"ohmywdx"</span>,        <span class="attr">"_type"</span>: <span class="string">"wdxtuber"</span>,        <span class="attr">"_id"</span>: <span class="string">"1"</span>,        <span class="attr">"_score"</span>: <span class="number">0.25811607</span>,        <span class="attr">"_source"</span>: &#123;          <span class="attr">"name"</span>: <span class="string">"Da Wang"</span>,          <span class="attr">"age"</span>: <span class="number">25</span>,          <span class="attr">"about"</span>: <span class="string">"First one who is stupid enough to join this company"</span>,          <span class="attr">"interests"</span>: [            <span class="string">"game"</span>,            <span class="string">"music"</span>          ]        &#125;      &#125;    ]  &#125;&#125;</div></pre></td></tr></table></figure>
<p>除了 <code>_source</code> 的信息之外，我们可以看到有一个 <code>_score</code>，敏感的同学大概会意识到，Elasticsearch 是根据相关性来对结果进行排序的，这个得分就是相关性分数。</p>
<p>除了前面说明的搜索，我们还可以使用 DSL 语句来组合更加复杂的搜索，什么过滤、组合条件、全文、短语搜索，都不在话下，我们甚至可以高亮搜索结果。另外我们还可以利用『聚合』来实现关系型数据库中『Group By』类似的操作。其他诸如推荐、定位、渗透、模糊及部分匹配同样也支持。不过这一篇仅仅是一个简要介绍，就不继续深入了。</p>
<h2 id="实例：收集-Linux-系统日志"><a href="#实例：收集-Linux-系统日志" class="headerlink" title="实例：收集 Linux 系统日志"></a>实例：收集 Linux 系统日志</h2><p>万事俱备，只欠东风，我们现在就把  ElasticStack 用起来！第一个任务很简单，就是把本机的日志给监控起来，这样我们在查询系统发生的事件时，就不用再去 <code>/var/log/</code> 文件夹里『翻箱倒柜』了。</p>
<h3 id="系统日志介绍"><a href="#系统日志介绍" class="headerlink" title="系统日志介绍"></a>系统日志介绍</h3><p>这里简要介绍一下比较通用的系统日志及对应的内容，之后导入日志的时候我会挑选：</p>
<ul>
<li><code>/var/log/apport.log</code> 应用程序崩溃记录</li>
<li><code>/var/log/apt/</code> 用 apt-get 安装卸载软件的信息</li>
<li><code>/var/log/auth.log</code>  登录认证的日志</li>
<li><code>/var/log/boot.log</code>  系统启动时的日志。</li>
<li><code>/var/log/dmesg</code> 包含内核缓冲信息(kernel ringbuffer)。在系统启动时，显示屏幕上的与硬件有关的信息</li>
<li><code>/var/log/faillog</code> 包含用户登录失败信息。此外，错误登录命令也会记录在本文件中</li>
<li><code>/var/log/fsck</code> 文件系统日志</li>
<li><code>/var/log/kern.log</code> 包含内核产生的日志，有助于在定制内核时解决问题</li>
<li><code>/var/log/wtmp</code> 包含登录信息。使用 wtmp 可以找出谁正在登陆进入系统，谁使用命令显示这个文件或信息等</li>
</ul>
<h3 id="启动-Logstash"><a href="#启动-Logstash" class="headerlink" title="启动 Logstash"></a>启动 Logstash</h3><p>这一步的任务是利用 Logstash 把系统日志给导入 Elasticsearch 中。暂时没有使用最新的 Beats 组件，而是继续使用传统的 Logstash 来进行操作（比较重型），用法和之前有些不同，我们会把配置写在一个文件中，而不是直接在命令中输入。</p>
<p>Logstash 使用一个名叫 FileWatch 的 Ruby Gem 库来监听文件变化。这个库支持 glob 展开文件路径，而且会记录一个叫 <code>.sincedb</code> 的数据库文件来跟踪被监听的日志文件的当前读取位置。通过记录下来的 <code>inode</code>, <code>major number</code>, <code>minor number</code> 和 <code>pos</code> 就可以保证不漏过每一条日志。</p>
<p>具体配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 我的习惯是把配置文件统一放到名为 confs 的文件夹中</div><div class="line"># 本配置文件名为 syslog.conf</div><div class="line">input &#123;</div><div class="line">  file &#123;</div><div class="line">    # 确定需要检测的文件</div><div class="line">    path =&gt; [ &quot;/var/log/*.log&quot;, &quot;/var/log/messages&quot;, &quot;/var/log/syslog&quot;, &quot;/var/log/apt&quot;, &quot;/var/log/fsck&quot;, &quot;/var/log/faillog&quot;]</div><div class="line">    # 日志类型</div><div class="line">    type =&gt; &quot;syslog&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line">output &#123;</div><div class="line">  # 输出到命令行，一般用于调试</div><div class="line">  stdout &#123; </div><div class="line">    codec =&gt; rubydebug </div><div class="line">  &#125;</div><div class="line">  # 输出到 elasticsearch，这里指定索引名为 system-log</div><div class="line">  elasticsearch &#123; </div><div class="line">    hosts =&gt; &quot;localhost:9200&quot;</div><div class="line">    index =&gt; &quot;system-log&quot; </div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里说一下 File rotation 的情况，为了处理被 rotate 的情况，最好把 rotate 之后的文件名也加到 path 中（如上面所示），这里注意，如果 <code>start_position</code> 被设为 <code>beginning</code>，被 rotate 的文件因为会被认为是新文件，而重新导入。如果用默认值 <code>end</code>，那么在最后一次读之后到被 rotate 结束前生成的日志不会被采集。</p>
<p>有了配置文件，我们就可以把日志导入到 Elasticsearch 了，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># -f 表示从文件中读取配置</span></div><div class="line">bin/logstash <span class="_">-f</span> confs/syslog.conf</div></pre></td></tr></table></figure>
<p>大概的输出是如下：</p>
<p><img src="/images/14797212868418.jpg" alt=""></p>
<p>如果到这里一切正常，我们就可以去 Kibana 中查看导入的日志了。</p>
<h3 id="使用-Kibana"><a href="#使用-Kibana" class="headerlink" title="使用 Kibana"></a>使用 Kibana</h3><p>Kibana 相当于是 Elasticsearch 的一个可视化插件，所以我们需要在 Management 页面中告诉 Kibana 我们刚才创建的 <code>system-log</code> 索引，完成之后可以看到具体的条目及对应的信息（包括是否可被检索，是否能聚合，是否被分词等）</p>
<p><img src="/images/14797126705730.jpg" alt=""></p>
<p>创建完成后我们就可以在 Discover 面板里查看数据了。</p>
<p><img src="/images/14797126971993.jpg" alt=""></p>
<p>分别介绍下每个面板的作用（更加详细的介绍参见 <a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a>）</p>
<ul>
<li>Discover: 探索数据</li>
<li>Visualize: 可视化统计</li>
<li>Dashboard: 仪表盘</li>
<li>Timelion: 时序，这里我们暂时不用</li>
<li>Management: 设置</li>
<li>Dev Tools: 开发工具，可以方便的测试内置接口</li>
</ul>
<p>这里我简单给出两个实例，介绍一下 Visualize 和 Dashboard 的基本用法</p>
<p>如下图配置，我们可以轻松查看日志都是从哪些文件导入的，除了饼图外，柱状图折线图之类的都是支持的，大家可以自行尝试一下</p>
<p><img src="/images/14797163664177.jpg" alt=""></p>
<p>每个 Visualization 都可以保存，保存之后可以在 Dashboard 面板里集中显示，这样我们只需要看一眼，就对机器运行的状况有一个清晰的了解。</p>
<p><img src="/images/14797164257675.jpg" alt=""></p>
<p>至此，我们就完成了收集系统日志并展示的任务，本章内容到此基本结束。</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>搭建完成之后，看看自己的系统中最多出现的 log 是什么？</li>
<li>在 Kibana 中 Dev Tools 的使用 Elasticsearch 的 HTTP 接口来进行简单的查询</li>
<li>尝试 logstash 自己感兴趣的插件，看看能不能为系统日志添加更多字段</li>
<li>创建几个不同的 Visualization 并添加到 Dashboard 中，让内容更丰富一些</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本节中我们完成了 ElasticStack 核心的 ELK 安装，并用一个简单的实例熟悉了相关操作。刚开始接触，一定是会有很多陌生的概念，建议大家去浏览一下官方的快速入门文档，写得还是比较清晰的。下一讲我们会介绍整个日志处理流程中很重要的『缓冲区』- Kafka，有了它，我们的日志分析平台就有了基本的雏形了。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://www.elastic.co/guide/index.html" target="_blank" rel="external">Elastic Stack and Product Documentation</a></li>
<li><a href="http://www.infoq.com/cn/news/2016/08/Elasticsearch-5-0-Elastic" target="_blank" rel="external">大数据杂谈微课堂|Elasticsearch 5.0新版本的特性与改进</a></li>
<li><a href="http://www.infoq.com/cn/news/2016/11/Elasticsearch-5-0-publish" target="_blank" rel="external">开源搜索引擎Elasticsearch 5.0版本正式发布</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一讲我们对 ElasticStack 进行了简要介绍并完成了基本的系统环境配置，这一次我们要把 Elasticsearch/Logstash/Kibana 安装配置好，并把 Linux 的系统日志导入进来。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="环境" scheme="http://wdxtub.com/tags/%E7%8E%AF%E5%A2%83/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】贰 Kafka 缓冲区</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/</id>
    <published>2016-11-19T03:11:09.000Z</published>
    <updated>2016-11-22T12:48:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>前一讲我们已经搭建好了 ElasticStack 的核心组件，但是在日常使用中，一般会在 Logstash 和 Elasticsearch 之间加一层 Kafka 用来缓存和控制，这次我们就来看看如何实现这样的功能。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.21: 完成初稿</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>安装并配置好 Kafka</li>
<li>理解 Kafka 的工作机制</li>
<li>掌握 Kafka 的基本操作，学会基本的错误处理</li>
<li>完成 Logstash-Kafka-Elasticsearch 链路的构建，理解这种架构的优劣</li>
<li>通过实际操作，增加对 ElasticStack 的理解</li>
</ol>
<h2 id="Kafka-简介"><a href="#Kafka-简介" class="headerlink" title="Kafka 简介"></a>Kafka 简介</h2><p>作为云计算大数据的套件，Kafka 是一个分布式的、可分区的、可复制的消息系统。该有的功能基本都有，而且有自己的特色：</p>
<ul>
<li>以 topic 为单位进行消息归纳</li>
<li>向 topic 发布消息的是 producer</li>
<li>从 topic 获取消息的是 consumer</li>
<li>集群方式运行，每个服务叫 broker</li>
<li>客户端和服务器通过 TCP 进行通信</li>
</ul>
<p>在Kafka集群中，没有“中心主节点”的概念，集群中所有的服务器都是对等的，因此，可以在不做任何配置的更改的情况下实现服务器的的添加与删除，同样的消息的生产者和消费者也能够做到随意重启和机器的上下线。</p>
<p>对每个 topic 来说，Kafka 会对其进行分区，每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做 offset,用来在分区中唯一的标识这个消息。</p>
<p>发布消息通常有两种模式：队列模式(queuing)和发布-订阅模式(publish-subscribe)。队列模式中，consumers 可以同时从服务端读取消息，每个消息只被其中一个 consumer 读到；发布-订阅模式中消息被广播到所有的 consumer 中。更常见的是，每个 topic 都有若干数量的 consumer 组，每个组都是一个逻辑上的『订阅者』，为了容错和更好的稳定性，每个组由若干 consumer 组成。这其实就是一个发布-订阅模式，只不过订阅者是个组而不是单个 consumer。</p>
<p>通过分区的概念，Kafka 可以在多个 consumer 组并发的情况下提供较好的有序性和负载均衡。将每个分区分只分发给一个 consumer 组，这样一个分区就只被这个组的一个 consumer 消费，就可以顺序的消费这个分区的消息。因为有多个分区，依然可以在多个 consumer 组之间进行负载均衡。注意 consumer 组的数量不能多于分区的数量，也就是有多少分区就允许多少并发消费。</p>
<p>Kafka 只能保证一个分区之内消息的有序性，在不同的分区之间是不可以的，这已经可以满足大部分应用的需求。如果需要 topic 中所有消息的有序性，那就只能让这个 topic 只有一个分区，当然也就只有一个 consumer 组消费它。</p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>我们可以根据自己的需求来进行简单的配置，具体如下：</p>
<h3 id="1-下载-Kafka"><a href="#1-下载-Kafka" class="headerlink" title="(1) 下载 Kafka"></a>(1) 下载 Kafka</h3><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 美国主机</span></div><div class="line">wget http://www-us.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz</div><div class="line"><span class="comment"># 解压</span></div><div class="line">tar -xzf kafka_2.11-0.10.1.0.tgz</div><div class="line"><span class="comment"># 进入文件夹</span></div><div class="line"><span class="built_in">cd</span> kafka_2.11-0.10.1.0</div></pre></td></tr></table></figure>
<h3 id="2-配置-Zookeeper-及-Kafka"><a href="#2-配置-Zookeeper-及-Kafka" class="headerlink" title="(2) 配置 Zookeeper 及 Kafka"></a>(2) 配置 Zookeeper 及 Kafka</h3><p>Zookeeper 的配置在 <code>config/zookeeper.properties</code> 文件中，Kafka 的配置在 <code>config/server.properties</code> 文件中。</p>
<p>Zookeeper 的配置不需要特别更改，注意默认数据存放的位置是 <code>/zookeeper</code>，这里最好放到挂载磁盘上（如果使用云主机，一般来说系统盘比较小，具体可以用 <code>df -h</code> 查看）。Kafka 的默认数据存放位置是 <code>/tmp/kafka-logs</code>，我们把 zookeeper 和 kafka 的数据存放位置一并进行修改</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 在 zookeeper.properties 中</span></div><div class="line">/data/home/logger/kafka-data/zookeeper</div><div class="line"></div><div class="line"><span class="comment"># 在 server.properties 中</span></div><div class="line">log.dirs=/data/home/logger/kafka-data/kafka-logs</div></pre></td></tr></table></figure>
<p>其他配置这里推荐进行一些修改，具体如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># advertised.listerners 改为对外服务的地址</span></div><div class="line"><span class="comment"># 比如对外的 ip 地址是 xx.xx.xx.xx，端口是 8080，那么</span></div><div class="line">advertised.listeners=PLAINTEXT://xx.xx.xx.xx:8080</div><div class="line"></div><div class="line"><span class="comment"># 允许删除 topic</span></div><div class="line">delete.topic.enable=<span class="literal">true</span></div><div class="line"></div><div class="line"><span class="comment"># 不允许自动创建 topic，方便管理</span></div><div class="line">auto.create.topics.enable=<span class="literal">false</span></div><div class="line"></div><div class="line"><span class="comment"># 设定每个 topic 的分区数量，这里设为 100</span></div><div class="line">num.partitions=100</div><div class="line"></div><div class="line"><span class="comment"># 设定日志保留的时间，这里改为 72 小时</span></div><div class="line">log.retention.hours=72</div></pre></td></tr></table></figure>
<h3 id="3-启动-Zookeeper-及-Kafka"><a href="#3-启动-Zookeeper-及-Kafka" class="headerlink" title="(3) 启动 Zookeeper 及 Kafka"></a>(3) 启动 Zookeeper 及 Kafka</h3><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 可以使用 tmux 或 nohup &amp; 等方式来进行后台运行，这里使用 tmux</span></div><div class="line"><span class="comment"># 启动 Zookeeper</span></div><div class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</div><div class="line"></div><div class="line"><span class="comment"># 启动 Kafka</span></div><div class="line">bin/kafka-server-start.sh config/server.properties</div></pre></td></tr></table></figure>
<p>如果没有出现错误，则启动成功，接下来可以做一个简单的测试</p>
<h3 id="4-内部测试-Kafka"><a href="#4-内部测试-Kafka" class="headerlink" title="(4) 内部测试 Kafka"></a>(4) 内部测试 Kafka</h3><p>先创建一个叫做 wdxtub 的 topic，它只有一个分区和一个副本，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 创建 topic</span></div><div class="line">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wdxtub</div></pre></td></tr></table></figure>
<p>然后我们可以使用 <code>bin/kafka-topics.sh --list --zookeeper localhost:2181</code> 命令来查看目前已有的 topic 列表，这时候应该能看到我们刚才创建的名为 wdxtub 的 topic。如果看到程序返回了 <code>wdxtub</code>，那么表示 topic 创建成功。</p>
<p>接下来我们创建一个简单的 producer，用来从标准输入中读取消息并发送给 Kafka，命令为 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 创建一个向 topic wdxtub 发送消息的 producer</span></div><div class="line"><span class="comment"># 按回车发送，ctrl+c 退出</span></div><div class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wdxtub</div></pre></td></tr></table></figure>
<p>另外新建一个窗口，启动一个 consumer，用来读取消息并输出到标准输出，命令为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 创建一个从 topic wdxtub 读取消息的 consumer</span></div><div class="line">bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic wdxtub --from-beginning</div></pre></td></tr></table></figure>
<p>启动成功后，我们在 producer 中输入的内容，就可以在 consumer 中看到：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># producer 窗口内容</span></div><div class="line">$&gt; ~/kafka_2.11-0.10.1.0$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wdxtub</div><div class="line">abcdefu</div><div class="line">dalkdjflka^H^H^H^H^H^H^H</div><div class="line">wdxtub.com</div><div class="line">wdxtub.com is good</div><div class="line"></div><div class="line"><span class="comment"># consumer 窗口内容</span></div><div class="line">$&gt; ~/kafka_2.11-0.10.1.0$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic wdxtub --from-beginning</div><div class="line">Using the ConsoleConsumer with old consumer is deprecated and will be removed <span class="keyword">in</span> a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].</div><div class="line">abcdefu</div><div class="line">dalkdjflka</div><div class="line">wdxtub.com</div><div class="line">wdxtub.com is good</div></pre></td></tr></table></figure>
<h3 id="5-Nginx-配置"><a href="#5-Nginx-配置" class="headerlink" title="(5) Nginx 配置"></a>(5) Nginx 配置</h3><p>因为 Kafka 集群的通讯是走内网 ip，而外网访问的端口因为安全考虑只开了少数几个（这里是 8080），所以我们用 Nginx 反向代理来连通内外网</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">upstream mq_pool&#123;</div><div class="line">server ip1:9092 weight=1 max_fails=3 fail_timeout=30s;</div><div class="line">server localhost:9092 weight=1 max_fails=3 fail_timeout=30s;</div><div class="line">&#125;</div><div class="line"></div><div class="line">server&#123;</div><div class="line">listen 8080;</div><div class="line">allow all;</div><div class="line">proxy_pass mq_pool;</div><div class="line">proxy_connect_timeout 24h;</div><div class="line">proxy_timeout 24h;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个配置的意思大概是把所有 8080 端口的消息转发到 <code>mq_pool</code> 的两台机器上（负载均衡），其他的就是常规配置。</p>
<h3 id="6-外部测试-Kafka"><a href="#6-外部测试-Kafka" class="headerlink" title="(6) 外部测试 Kafka"></a>(6) 外部测试 Kafka</h3><p>现在我们的 Kafka 已经在运行了，但是刚才的测试程序是在本机，所以我们无法保证外部应用也能向 Kafka 发送消息（很多时候会用 Nginx 来控制），这里我们就来编写一段简单的 python 脚本来测试能否从其他服务器连接 Kafka。</p>
<p>这里我们采用的 python 包名为 <a href="https://github.com/dpkp/kafka-python" target="_blank" rel="external">dpkp/kafka-python</a>，如果已经有 <code>pip</code> 工具的话，直接 <code>pip install kafka-python</code> 即可。然后我们可以简单编写一个 producer 来进行测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 名为 kafka-test.py</span></div><div class="line"><span class="keyword">from</span> kafka <span class="keyword">import</span> KafkaProducer</div><div class="line"><span class="comment"># 设置 Kafka 地址</span></div><div class="line">producer = KafkaProducer(</div><div class="line">    bootstrap_servers=<span class="string">'your.host.name:8080'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 设置需要发送的 topic 及内容</span></div><div class="line">producer.send(<span class="string">'wdxtub'</span>, <span class="string">'Hello World! This is wdxtub.com.'</span>)</div></pre></td></tr></table></figure>
<p>执行一下 <code>python kafka-test.py</code>，如果能在第(4)步中打开的 consumer 中看到 Hello World 这行字儿，说明能够正确连接。</p>
<h2 id="Kafka-常用操作"><a href="#Kafka-常用操作" class="headerlink" title="Kafka 常用操作"></a>Kafka 常用操作</h2><p>所有的工具都可以在 <code>bin/</code> 文件夹下查看，如果不带任何参数，就会给出所有命令的列表说明，这里只简要说明一些常用的命令</p>
<h3 id="管理-topic"><a href="#管理-topic" class="headerlink" title="管理 topic"></a>管理 topic</h3><p>可以手动创建 topic，或在数据进来时自动创建不存在的 topic，如果是自动创建的话，可能需要根据<a href="http://kafka.apache.org/documentation.html#topic-config" target="_blank" rel="external">这里</a>来进行对应调整。</p>
<p><strong>创建 topic</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --create --topic my_topic_name --partitions 20 --replication-factor 3 --config x=y</code></p>
<p>replication-factor 控制复制的份数，建议 2-3 份来兼顾容错和效率。partitions 控制该 topic 将被分区的数目，partitions 的数目最好不要超过服务器的个数（因为分区的意义是增加并行效率，而服务器数量决定了并行的数量，假设只有 2 台服务器，分 4 个区和 2 个区其实差别不大）。另外，topic 的名称不能超过 249 个字符</p>
<p><strong>修改 topic</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name --partitions 40</code></p>
<p>这里需要注意，即使修改了分区的个数，已有的数据也不会进行变动，Kafka 不会做任何自动重分布</p>
<p><strong>增加配置</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name --config x=y</code></p>
<p><strong>移除配置</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name --delete-config x</code></p>
<p><strong>删除 topic</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --delete --topic my_topic_name</code></p>
<p>这个需要 <code>delete.topic.enable=true</code>，目前 Kafka 不支持减少 topic 的分区数目</p>
<h3 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h3><p>Kafka 会自动检测 broker 的状态并根据机器状态选举出新的 leader。但是如果需要进行配置更改停机的时候，我们就需要使用优雅关闭了，好处在于：</p>
<ol>
<li>会把所有的日志同步到磁盘上，避免重启之后的日志恢复，减少重启时间</li>
<li>会在关闭前把以这台机为 leader 的分区数据迁移到其他节点，会减少不可用的时间</li>
</ol>
<p>但是这个需要开启 <code>controlled.shutdown.enable=true</code>。</p>
<p>刚重启之后的节点不是任何分区的 leader，所以这时候需要进行重新分配：</p>
<p><code>bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot</code></p>
<p>这里需要开启 <code>auto.leader.rebalance.enable=true</code></p>
<p>然后可以使用脚本 <code>bin/kafka-server-stop.sh</code></p>
<p>注意，如果配置文件中没有 <code>auto.leader.rebalance.enable=true</code>，就还需要重新平衡。</p>
<h2 id="深入理解"><a href="#深入理解" class="headerlink" title="深入理解"></a>深入理解</h2><p>这里只是一部分摘录，更多内容可查阅参考链接（尤其是美团技术博客的那篇）</p>
<h3 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h3><p>Kafka 大量依赖文件系统去存储和缓存消息。而文件系统最终会放在硬盘上，不过不用担心，很多时候硬盘的快慢完全取决于使用它的方式。设计良好的硬盘架构可以和内存一样快。</p>
<p>所以与传统的将数据缓存在内存中然后刷到硬盘的设计不同，Kafka直接将数据写到了文件系统的日志中，因此也避开了 JVM 的劣势——Java 对象占用空间巨大，数据量增大后垃圾回收有困难。使用文件系统，即使系统重启了，也不需要刷新数据，也简化了维护数据一致性的逻辑。</p>
<p>对于主要用于日志处理的消息系统，数据的持久化可以简单的通过将数据追加到文件中实现，读的时候从文件中读就好了。这样做的好处是读和写都是 O(1) 的，并且读操作不会阻塞写操作和其他操作。这样带来的性能优势是很明显的，因为性能和数据的大小没有关系了。</p>
<p>既然可以使用几乎没有容量限制（相对于内存来说）的硬盘空间建立消息系统，就可以在没有性能损失的情况下提供一些一般消息系统不具备的特性。比如，一般的消息系统都是在消息被消费后立即删除，Kafka却可以将消息保存一段时间（比如一星期），这给consumer提供了很好的机动性和灵活性。</p>
<h3 id="事务定义"><a href="#事务定义" class="headerlink" title="事务定义"></a>事务定义</h3><p>数据传输的事务定义通常有以下三种级别：</p>
<ul>
<li>最多一次: 消息不会被重复发送，最多被传输一次，但也有可能一次不传输。</li>
<li>最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输.</li>
<li>精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而且仅仅被传输一次，这是大家所期望的。</li>
</ul>
<p>Kafka 的机制和 git 有点类似，有一个 commit 的概念，一旦提交且 broker 在工作，那么数据就不会丢失。如果 producer 发布消息时发生了网络错误，但又不确定实在提交之前发生的还是提交之后发生的，这种情况虽然不常见，但是必须考虑进去，现在Kafka版本还没有解决这个问题，将来的版本正在努力尝试解决。</p>
<p>并不是所有的情况都需要“精确的一次”这样高的级别，Kafka 允许 producer 灵活的指定级别。比如 producer 可以指定必须等待消息被提交的通知，或者完全的异步发送消息而不等待任何通知，或者仅仅等待 leader 声明它拿到了消息（followers没有必要）。</p>
<p>现在从 consumer 的方面考虑这个问题，所有的副本都有相同的日志文件和相同的offset，consumer 维护自己消费的消息的 offset。如果 consumer 崩溃了，会有另外一个 consumer 接着消费消息，它需要从一个合适的 offset 继续处理。这种情况下可以有以下选择：</p>
<ul>
<li>consumer 可以先读取消息，然后将 offset 写入日志文件中，然后再处理消息。这存在一种可能就是在存储 offset 后还没处理消息就 crash 了，新的 consumer 继续从这个 offset 处理，那么就会有些消息永远不会被处理，这就是上面说的『最多一次』</li>
<li>consumer 可以先读取消息，处理消息，最后记录o ffset，当然如果在记录 offset 之前就 crash 了，新的 consumer 会重复的消费一些消息，这就是上面说的『最少一次』</li>
<li>『精确一次』可以通过将提交分为两个阶段来解决：保存了 offset 后提交一次，消息处理成功之后再提交一次。但是还有个更简单的做法：将消息的 offset 和消息被处理后的结果保存在一起。比如用 Hadoop ETL 处理消息时，将处理后的结果和 offset 同时保存在 HDFS 中，这样就能保证消息和 offser 同时被处理了</li>
</ul>
<h3 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h3><p>Kafka 在提高效率方面做了很大努力。Kafka 的一个主要使用场景是处理网站活动日志，吞吐量是非常大的，每个页面都会产生好多次写操作。读方面，假设每个消息只被消费一次，读的量的也是很大的，Kafka 也尽量使读的操作更轻量化。</p>
<p>线性读写的情况下影响磁盘性能问题大约有两个方面：太多的琐碎的 I/O 操作和太多的字节拷贝。I/O 问题发生在客户端和服务端之间，也发生在服务端内部的持久化的操作中。</p>
<p><strong>消息集(message set)</strong></p>
<p>为了避免这些问题，Kafka 建立了<strong>消息集(message set)</strong>的概念，将消息组织到一起，作为处理的单位。以消息集为单位处理消息，比以单个的消息为单位处理，会提升不少性能。Producer 把消息集一块发送给服务端，而不是一条条的发送；服务端把消息集一次性的追加到日志文件中，这样减少了琐碎的 I/O 操作。consumer 也可以一次性的请求一个消息集。</p>
<p>另外一个性能优化是在字节拷贝方面。在低负载的情况下这不是问题，但是在高负载的情况下它的影响还是很大的。为了避免这个问题，Kafka 使用了标准的二进制消息格式，这个格式可以在 producer, broker 和 producer 之间共享而无需做任何改动。</p>
<p><strong>zero copy</strong></p>
<p>Broker 维护的消息日志仅仅是一些目录文件，消息集以固定队的格式写入到日志文件中，这个格式 producer 和 consumer 是共享的，这使得 Kafka 可以一个很重要的点进行优化：消息在网络上的传递。现代的 unix 操作系统提供了高性能的将数据从页面缓存发送到 socket 的系统函数，在 linux 中，这个函数是 <code>sendfile</code></p>
<p>为了更好的理解 <code>sendfile</code> 的好处，我们先来看下一般将数据从文件发送到 socket 的数据流向：</p>
<ul>
<li>操作系统把数据从文件拷贝内核中的页缓存中</li>
<li>应用程序从页缓存从把数据拷贝自己的内存缓存中</li>
<li>应用程序将数据写入到内核中 socket 缓存中</li>
<li>操作系统把数据从 socket 缓存中拷贝到网卡接口缓存，从这里发送到网络上。</li>
</ul>
<p>这显然是低效率的，有 4 次拷贝和 2 次系统调用。<code>sendfile</code> 通过直接将数据从页面缓存发送网卡接口缓存，避免了重复拷贝，大大的优化了性能。</p>
<p>在一个多consumers的场景里，数据仅仅被拷贝到页面缓存一次而不是每次消费消息的时候都重复的进行拷贝。这使得消息以近乎网络带宽的速率发送出去。这样在磁盘层面你几乎看不到任何的读操作，因为数据都是从页面缓存中直接发送到网络上去了。</p>
<p><strong>数据压缩</strong></p>
<p>很多时候，性能的瓶颈并非CPU或者硬盘而是网络带宽，对于需要在数据中心之间传送大量数据的应用更是如此。当然用户可以在没有 Kafka 支持的情况下各自压缩自己的消息，但是这将导致较低的压缩率，因为相比于将消息单独压缩，将大量文件压缩在一起才能起到最好的压缩效果。</p>
<p>Kafka 采用了端到端的压缩：因为有『消息集』的概念，客户端的消息可以一起被压缩后送到服务端，并以压缩后的格式写入日志文件，以压缩的格式发送到 consumer，消息从 producer 发出到 consumer 拿到都被是压缩的，只有在 consumer 使用的时候才被解压缩，所以叫做『端到端的压缩』。Kafka支持GZIP和Snappy压缩协议。</p>
<h2 id="实例：把系统日志通过-Kafka-接入-Elasticsearch"><a href="#实例：把系统日志通过-Kafka-接入-Elasticsearch" class="headerlink" title="实例：把系统日志通过 Kafka 接入 Elasticsearch"></a>实例：把系统日志通过 Kafka 接入 Elasticsearch</h2><p>现在我们就把上一讲搭建好的架构中加入 Kakfa 作为缓冲区，具体分两步</p>
<h3 id="1-Logstash-gt-Kafka"><a href="#1-Logstash-gt-Kafka" class="headerlink" title="(1) Logstash -&gt; Kafka"></a>(1) Logstash -&gt; Kafka</h3><p>让我们回想一下之前的架构，Logstash 会直接把日志发送给 Elasticsearch，再由 Kibana 进行展示。因为 Logstash 是同步把日志发送给 Elasticsearch 的，所以等于这俩耦合在了一起，Elasticsearch 一旦挂掉，可能就会丢失数据。</p>
<p>于是，我们考虑利用 Kafka 作为缓冲区，让 Logstash 不受 Elasticsearch 的影响。所以第一步就是让 Logstash 把日志发送到 Kafka，这里 Logstash 相当于 producer。</p>
<p>不过在开始之前，我们先启动 Kafka</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 启动 Zookeeper</span></div><div class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</div><div class="line"><span class="comment"># 启动 Kafka</span></div><div class="line">bin/kafka-server-start.sh config/server.properties</div></pre></td></tr></table></figure>
<p>我们之前的 Logstash 配置文件是把日志直接发送到 Elasticsearch 的，这里我们需要更新为发送到 Kafka</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 我的习惯是把配置文件统一放到名为 confs 的文件夹中</div><div class="line"># 本配置文件名为 log-to-kafka.conf</div><div class="line">input &#123;</div><div class="line">  file &#123;</div><div class="line">    # 确定需要检测的文件</div><div class="line">    path =&gt; [ &quot;/var/log/*.log&quot;, &quot;/var/log/messages&quot;, &quot;/var/log/syslog&quot;, &quot;/var/log/apt&quot;, &quot;/var/log/fsck&quot;, &quot;/var/log/faillog&quot;]</div><div class="line">    # 日志类型</div><div class="line">    type =&gt; &quot;syslog&quot;</div><div class="line">    add_field =&gt; &#123; &quot;service&quot; =&gt; &quot;system-log&quot;&#125;</div><div class="line">    # stat_interval =&gt; 1800</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line">output &#123;</div><div class="line">  # 输出到命令行，一般用于调试</div><div class="line">  stdout &#123; </div><div class="line">    codec =&gt; rubydebug </div><div class="line">  &#125;</div><div class="line">  # 输出到 Kafka，topic 名称为 logs，地址为默认的端口号</div><div class="line">  kafka &#123;</div><div class="line">    topic_id =&gt; &quot;logs&quot;</div><div class="line">    bootstrap_servers =&gt; &quot;localhost:9092&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>file 插件其他一些配置设定原因</p>
<ul>
<li><code>add_field</code> 添加一个 topic 字段，用作之后导入 elasticsearch 的索引标识</li>
<li><code>stat_interval</code> 单位是秒，这里 30 分钟进行一次检测，不过测试的时候需要去掉这个配置</li>
</ul>
<p>kafka 插件其他一些需要注意的配置</p>
<ul>
<li><code>acks</code> 可以选的值为 <code>0</code>, <code>1</code>, <code>all</code>，这里解释一下，0 表示不需要 server 返回就认为请求已完成；1 表示需要 leader 返回才认为请求完成；all 表示需要所有的服务器返回才认为请求完成</li>
<li><code>batch_size</code> 单位是字节，如果是发送到同一分区，会攒够这个大小才发送一次请求</li>
<li><code>block_on_buffer_full</code> 这个设置在缓冲区慢了之后阻塞还是直接报错</li>
<li><code>buffer_memory</code> 发送给服务器之前的缓冲区大小，单位是字节</li>
<li><code>client_id</code> 可以在这里设定有意义的名字，就不一定要用 ip 和 端口来区分</li>
<li><code>compression_type</code> 压缩方式，默认是 <code>none</code>，其他可选的是 <code>gzip</code> 和 <code>snappy</code></li>
</ul>
<h3 id="2-Kafka-gt-Elasticsearch"><a href="#2-Kafka-gt-Elasticsearch" class="headerlink" title="(2) Kafka -&gt; Elasticsearch"></a>(2) Kafka -&gt; Elasticsearch</h3><p>利用 Logstash 从 Kafka 导出数据到 Elasticsearch。这一步就比较简单了，先从 Kafka 中读取，然后写入到 elasticsearch，这里 Logstash 作为 consumer。唯一需要注意的地方是要保证 topic 名称一致</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 文件名 kafka-to-es.conf</div><div class="line">input &#123;</div><div class="line">  kafka &#123;</div><div class="line">    bootstrap_servers =&gt; &quot;localhost:9092&quot;</div><div class="line">    topics =&gt; [&quot;logs&quot;]</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">output &#123;</div><div class="line">  # for debugging</div><div class="line">  stdout &#123;</div><div class="line">     codec =&gt; rubydebug</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  elasticsearch &#123; </div><div class="line">    hosts =&gt; &quot;localhost:9200&quot;</div><div class="line">    index =&gt; &quot;system-log&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>至此，我们完成了从 Logstash 到 Kafka 再到 Elasticsearch 的连接，下一步就可以用 kibana 来展示日志的监控分析结果了。</p>
<p><img src="/images/14797818253620.jpg" alt=""></p>
<p>如上图所示，打开 Kibana，即可见到我们使用 Logstash 通过 Kafka 再发送到 Elasticsearch 的日志。至此，我们就成功把 Kafka 加入到日志分析平台的架构中了。</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>查阅 Logstash 的 Kafka 插件的文档，了解其他的配置选项</li>
<li>Logstash 能够处理 json 格式的日志，试着把系统日志转换成 json，并进行处理</li>
<li>更新 Logstash 配置，看看能不能多记录一些系统事件</li>
<li>随着日志的增多，使用 Kibana 多创建一些图表并添加到 Dashboard 中</li>
</ol>
<p>一个可能的例子如下：</p>
<p><img src="/images/14797829019725.jpg" alt=""></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这一讲我们主要学习了 Kafka 的相关内容，并在了解原理的基础上更新了日志分析平台的架构，这样我们的日志在发送到 Elasticsearch 之前。下一讲我们会在单机的状态下完成监控、安全、报警与通知的功能。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://kafka.apache.org/quickstart" target="_blank" rel="external">Kafka 快速入门</a></li>
<li><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz" target="_blank" rel="external">Kafka 2.11-0.10.1.0 下载</a></li>
<li><a href="http://blog.csdn.net/LOUISLIAOXH/article/details/51567515" target="_blank" rel="external">Kafka学习整理六(server.properties配置实践)</a></li>
<li><a href="http://kafka.apache.org/" target="_blank" rel="external">Apache Kafka</a></li>
<li><a href="http://kafka.apache.org/documentation.html#quickstart" target="_blank" rel="external">Quick Start</a></li>
<li><a href="http://www.aboutyun.com/thread-12882-1-1.html" target="_blank" rel="external">Kafka入门经典教程</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-kafka/" target="_blank" rel="external">Apache kafka 工作原理介绍</a></li>
<li><a href="http://www.coderli.com/setup-kafka-cluster-step-by-step/" target="_blank" rel="external">事无巨细 Apache Kafka 0.9.0.1 集群环境搭建</a></li>
<li><a href="http://blog.csdn.net/dhtx_wzgl/article/details/46892231" target="_blank" rel="external">kafka集群搭建</a></li>
<li><a href="http://tech.meituan.com/kafka-fs-design-theory.html" target="_blank" rel="external">Kafka文件存储机制那些事</a></li>
<li><a href="http://kaimingwan.com/post/kafka/kafkayuan-li-yi-ji-she-ji-shi-xian-si-xiang" target="_blank" rel="external">kafka原理以及设计实现思想</a></li>
<li><a href="http://www.dexcoder.com/dexcoder/article/2194" target="_blank" rel="external">kafka设计原理介绍</a></li>
<li><a href="http://blog.jobbole.com/99195/" target="_blank" rel="external">Kafka集群操作指南</a></li>
<li><a href="https://www.quora.com/What-is-the-actual-role-of-ZooKeeper-in-Kafka" target="_blank" rel="external">What is the actual role of ZooKeeper in Kafka?</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一讲我们已经搭建好了 ElasticStack 的核心组件，但是在日常使用中，一般会在 Logstash 和 Elasticsearch 之间加一层 Kafka 用来缓存和控制，这次我们就来看看如何实现这样的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="Kafka" scheme="http://wdxtub.com/tags/Kafka/"/>
    
  </entry>
  
</feed>
