<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小土刀</title>
  <subtitle>Agony is my triumph</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wdxtub.com/"/>
  <updated>2016-09-10T02:45:52.000Z</updated>
  <id>http://wdxtub.com/</id>
  
  <author>
    <name>wdxtub</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>推荐系统指南</title>
    <link href="http://wdxtub.com/2016/09/10/recsys-guide/"/>
    <id>http://wdxtub.com/2016/09/10/recsys-guide/</id>
    <published>2016-09-10T02:40:43.000Z</published>
    <updated>2016-09-10T02:45:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是我学习推荐系统时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。</p>
<a id="more"></a>
<hr>
<p>推荐系统也是我比较感兴趣并且最近一年一直在研究的方向，搜索可以看做是上一个时代的钥匙，但是在信息爆炸的今天，推荐是现在的钥匙。</p>
<p>推荐算法五花八门，但是其实各有各的局限，比较常用的有：</p>
<ul>
<li>基于内容推荐</li>
<li>协同过滤推荐</li>
<li>基于关联规则推荐</li>
<li>基于效用推荐</li>
<li>基于知识推荐</li>
<li>组合推荐</li>
</ul>
<p>推荐算法本身是一个综合性的问题，他说浅他可以做的很浅，说深也可以把他做到很深。你可以简单地用最基本的Content-based，再复杂点可以Collaborative Filtering，如果你想做的深入一些，基于SVD/LDA等的降维算法，基于SVD++等的评分预测算法，基于Learning To Rank的排序算法，甚至你再转换问题，把推荐问题再转换成分类问题，或者采用以上算法前先用各种聚类算法做数据的预处理，你可以折腾出很多很多的花样。所以做推荐领域的工程师是个很“痛苦”的事儿，因为只要机器学习领域有任何的突破性进展，你都需要去做跟踪，NLP领域出了Word2Vec，出了GloVe，其他领域的算法工程师可以说我对NLP不感兴趣，但是你必须跟踪，因为他可以辅助你去做文本内容类的推荐算法；Deep Learning可以让图像识别领域做更棒的特征工程，你也马上要去跟踪学习，因为在做图片推荐时终于有一种方式也许能解决元信息问题；RecSys2013的best paper通过调整节点顺序从而优化矩阵分块策略，极大改善了矩阵分解算法的效率，你就要去跟踪来更新自己的旧有离线算法；微软亚研搞出了一个Light LDA允许在低网络流量下去做LDA的多机并行，你就要兴冲冲地跑过去读他们啰啰嗦嗦的几十页的paper，因为终于不用忍受LDA低劣的性能了，而这些追踪往往是无穷无尽的。但是如果你一旦停止了更新知识库，就会学术界远远甩在身后，做一个“协同过滤”工程师。</p>
<p>但是算法的一切调整只有寄托于产品才能发挥出其最大的威力，但是如何根据产品去选择和调整算法是我认识大多数的算法工程师所非常薄弱的一点。举个实际的例子，我们都知道在所有的比赛中，多种算法的混合是最重要的环节，经常就会有人问我，说哪种混合策略是最好的，但是其实这是严重依赖于产品本身的。例如Tinder，他们的产品形态是每次只出现一个人，让你点击喜欢还是不喜欢，那么这种情况你必须需要一个算法的分类器来为每个用户选择一个合适的推荐算法，并且根据用户的反馈来实时调整分类器，因为如果用户连续Unlike了几个用户，他可能就流失掉了。但是对于LinkedIn的用户推荐列表中，你很适合用若干算法混合的算法，因为这样可以保证至少让整个列表中至少有X个所感兴趣的用户，而往往同一个推荐算法的Items是趋同的。再对于展示在首页的推荐入口区域，可以优先选择若干推荐算法的交集策略，这样可以用少量的高质量Item最大化的满足用户的心理底线，从而吸引用户点击。所以认清产品的形态和交互形式，依据产品去订制算法是成为优秀算法工程师，而非算法研究员的重要一点。</p>
<p>数据工程师不仅仅是处理数据而是理解数据。我遇到的数据工程师大抵分成两类，一类是数据开发工程师，例如Hadoop工程师，数据仓库工程师；一类是学术化的工程师，深钻模型，这种工程师其实还是更适合研究院；当然，这两种工程师都各有优缺点，但是我更觉得对于大部分企业来说更需要一个理解数据而非处理数据的工程师，核心价值更应该在于深入去理解产品业务，数据处理，数据建模，做数据分析和挖掘，接下来对于产品的发展做数据化的驱动，并且知道何时应该继续对模型进行优化，何时应该适可而止。</p>
<p>沿着上一点继续说，Growth Hacker &amp; Data Scientist。一个优秀的算法/数据工程师应该具备Growth Hacker 和 Data Scientist的能力，其实这两点也恰恰标志着不仅仅是数据，而是一个产品的最重要两点：增长和留存。作为Growth Hacker，你应该为企业找到潜在的机会点，帮助产品增长；另一方面，你也应该作为Data Scientist，发现现有数据的问题，帮助产品优化体验，提升留存，而推荐系统往往是属于这一部分的子集。</p>
<p>数据驱动是什么？数据驱动是从已有的数据中去发现规律对产品进行优化，但是数据做不到的是从未知中挖掘机会点，而这往往是一个优秀产品经理的直觉。我经常和人举的一个例子是，要是数据驱动，也许今天应该也出现不了微信。很多时候优秀的创意就是来源于一个直觉，而不是循规蹈矩的分析推导，因为这样往往会陷入我在上文提到的大数据的窘境。所以不要无视数据，更不要神化数据，该相信直觉的时候还是相信直觉。其实有时候做算法也是一样，你不可能把上千种算法都A-B Test一次，有时候别人问我为什么，我能说的也就是直觉，作为算法工程师的直觉，“用另外一种算法效果不会更好的”。</p>
<h2 id="主要推荐方法的对比"><a href="#主要推荐方法的对比" class="headerlink" title="主要推荐方法的对比"></a>主要推荐方法的对比</h2><table>
<thead>
<tr>
<th>推荐方法</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>基于内容推荐</td>
<td>推荐结果直观，容易解释；不需要领域知识</td>
<td>稀疏问题；新用户问题；复杂属性不好处理；要有足够数据构造分类器</td>
</tr>
<tr>
<td>协同过滤推荐</td>
<td>新异兴趣发现、不需要领域知识；随着时间推移性能提高；推荐个性化、自动化程度高；能处理复杂的非结构化对象</td>
<td>稀疏问题；可扩展性问题；新用户问题；质量取决于历史数据集；系统开始时推荐质量差；</td>
</tr>
<tr>
<td>基于规则推荐</td>
<td>能发现新兴趣点；不要领域知识</td>
<td>规则抽取难、耗时；产品名同义性问题；个性化程度低；</td>
</tr>
<tr>
<td>基于效用推荐</td>
<td>无冷开始和稀疏问题；对用户偏好变化敏感；能考虑非产品特性</td>
<td>用户必须输入效用函数；推荐是静态的，灵活性差；属性重叠问题；</td>
</tr>
<tr>
<td>基于知识推荐</td>
<td>能把用户需求映射到产品上；能考虑非产品属性</td>
<td>知识难获得；推荐是静态的</td>
</tr>
</tbody>
</table>
<h2 id="冷启动问题"><a href="#冷启动问题" class="headerlink" title="冷启动问题"></a>冷启动问题</h2><ul>
<li>用户冷启动<ul>
<li>提供 热门排行榜/热销榜/热搜榜，引导用户</li>
<li>利用客户提交的年龄，性别做粗粒度的个性化</li>
</ul>
</li>
<li>物品冷启动<ul>
<li>精准推荐/模糊推荐  </li>
<li>推荐给喜欢该商品的用户/商品相似度的用户</li>
</ul>
</li>
<li>提供非个性化的推荐<ul>
<li>利用社交网站授权，导入好友关系，给用户推荐好友喜欢的物品</li>
<li>登录时根据用户的反馈，收集用户兴趣，给用户推荐和反馈的相似的物品（ItemCF） </li>
</ul>
</li>
<li>利用用户注册信息<ul>
<li>人口统计学信息（年龄/性别/职业/学历/居住地）</li>
<li>用户兴趣的描述</li>
<li>导入站外行为数据（社交网络的数据）</li>
</ul>
</li>
<li>流程<ul>
<li>获取用户注册信息</li>
<li>将注册的用户分类</li>
<li>给用户推荐所属分类中用户喜欢的物品</li>
</ul>
</li>
<li>选择合适的物品启动用户兴趣<ul>
<li>热门</li>
<li>具有代表性和区分性  电影：卖票并且受欢迎的</li>
<li>启动物品集合需要多样性  Nadav Golbandi</li>
</ul>
</li>
<li>利用物品的内容信息<ul>
<li>关键词向量（文本-分词-实体检测-关键词排名-关键词向量）</li>
</ul>
</li>
<li>发挥专家的作用<ul>
<li>Pandora（个性化电台）组织专家项目组，对歌曲维度进行标注</li>
</ul>
</li>
<li>Jinni（电影基因系统）<ul>
<li>半人工-专家总结电影基因，创建基因库</li>
<li>半自动-文本分析</li>
</ul>
</li>
</ul>
<h2 id="AB-测试"><a href="#AB-测试" class="headerlink" title="AB 测试"></a>AB 测试</h2><p>产品的改变并不总是意味着进步，有时候无法评判多种设计方案中哪一种更优秀的， 这时A/B test就派上用场了，A/B test可以回答两个问题：</p>
<ol>
<li>哪个方案好</li>
<li>结果的可信程度</li>
</ol>
<p>A/B test结果是基于用户得到的结果，用数据说话， 而不是凭空想象去为用户代言，并且通过一定的数学分析给出结果的可信度。</p>
<p>A/B test需要如下几个前提：</p>
<ol>
<li>多个方案并行测试；</li>
<li>每个方案只有一个变量不同（当然，这太严格了）；</li>
<li>能够以某种规则优胜劣汰</li>
</ol>
<p>其中第2点暗示了A/B测试的应用范围：A/B测试必须是单变量，但有的时候，我们并不追求知道某个细节对方案的影响，而只想知道方案的整体效果如何，那么可以适当增加变量，当然测试方案有非常大的差异时一般不太适合做A/B测试，因为它们的 变量太多了，变量之间会有很多的干扰，所以很难通过A/B测试的方法找出各个变量对结果的影响程度。</p>
<p>在满足上述前提时，便可以做A/B test了。</p>
<h3 id="置信区间"><a href="#置信区间" class="headerlink" title="置信区间"></a>置信区间</h3><p>目标转换率变化区间估计：在做A/B test的时候，抽样得到的数据并不能准确反映整体的真实水平，即样本得到的估计是有偏差的，因此需要去评估这个值可能的变化区间。例如通过区间估计得到：</p>
<ul>
<li>A方案转换率为：6.5% ± 1.5%</li>
<li>B方案转换率为：7.5% ± 1.5%</li>
</ul>
<p>方案胜出概率估计：由于最终有意义的是确立胜出的版本，然而并不是所有的实验都能做到样本足够大，区分度足够高的，因此确定版本胜出的概率，很多英文资料里面记为Chance to beat baseline，即在给定转换率下，变体版本的实际转换率高于参展版本（默认是原始版本）的实际转换率的可能性。在实验之前需要设定一个阈值（称为置信度），某版本胜出的可能性高于这个值并且稳定时，便可以宣布该版本胜出。置信度越高，结果的可靠信越高；随着置信度的增加实验时间将会变长。</p>
<p>我们使用统计学理论计算Z检验和区间估计计算出误差范围及胜出概率</p>
<p>统计概率这部分知识暂略</p>
<h2 id="十条经验和教训"><a href="#十条经验和教训" class="headerlink" title="十条经验和教训"></a>十条经验和教训</h2><ol>
<li>确定你真的需要推荐系统。推荐系统只有在用户遇到信息过载时才必要。如果你的网站物品不太多，或者用户兴趣都比较单一，那么也许并不需要推荐系统。所以不要纠结于推荐系统这个词，不要为了做推荐系统而做推荐系统，而是应该从用户的角度出发，设计出能够真正帮助用户发现内容的系统，无论这个系统算法是否复杂，只要能够真正帮助用户，就是一个好的系统。</li>
<li>确定商业目标和用户满意度之间的关系。对用户好的推荐系统不代表商业上有用的推荐系统，因此要首先确定用户满意的推荐系统和商业上需求的差距。一般来说，有些时候用户满意和商业需求并不吻合。但是一般情况下，用户满意度总是符合企业的长期利益，因此这一条的主要观点是要平衡企业的长期利益和短期利益之间的关系。</li>
<li>选择合适的开发人员。一般来说，如果是一家大公司，应该雇用自己的开发人员来专门进行推荐系统的开发。</li>
<li>忘记冷启动的问题。不断地创新，互联网上有任何你想要的数据。只要用户喜欢你的产品，他们就会不断贡献新的数据。</li>
<li>平衡数据和算法之间的关系。使用正确的用户数据对推荐系统至关重要。对用户行为数据的深刻理解是设计好推荐系统的必要条件，因此分析数据是设计系统中最重要的部分。数据分析决定了如何设计模型，而算法只是决定了最终如何优化模型。</li>
<li>找到相关的物品很容易，但是何时以何种方式将它们展现给用户是很困难的。不要为了推荐而推荐。</li>
<li>不要浪费时间计算相似兴趣的用户，可以直接利用社会网络数据。</li>
<li>需要不断地提升算法的扩展性。</li>
<li>选择合适的用户反馈方式。</li>
<li>设计合理的评测系统，时刻关注推荐系统各方面的性能。</li>
</ol>
<h2 id="基于内容推荐"><a href="#基于内容推荐" class="headerlink" title="基于内容推荐"></a>基于内容推荐</h2><p>基于内容的推荐（Content-based Recommendation）是信息过滤技术的延续与发展，它是建立在项目的内容信息上作出推荐的，而不需要依据用户对项目的评价意见，更多地需要用机 器学习的方法从关于内容的特征描述的事例中得到用户的兴趣资料。在基于内容的推荐系统中，项目或对象是通过相关的特征的属性来定义，系统基于用户评价对象 的特征，学习用户的兴趣，考察用户资料与待预测项目的相匹配程度。用户的资料模型取决于所用学习方法，常用的有决策树、神经网络和基于向量的表示方法等。 基于内容的用户资料是需要有用户的历史数据，用户资料模型可能随着用户的偏好改变而发生变化。</p>
<p>基于内容推荐方法的优点是：</p>
<ol>
<li>不需要其它用户的数据，没有冷开始问题和稀疏问题。</li>
<li>能为具有特殊兴趣爱好的用户进行推荐。</li>
<li>能推荐新的或不是很流行的项目，没有新项目问题。</li>
<li>通过列出推荐项目的内容特征，可以解释为什么推荐那些项目。</li>
<li>已有比较好的技术，如关于分类学习方面的技术已相当成熟。</li>
</ol>
<p>缺点是要求内容能容易抽取成有意义的特征，要求特征内容有良好的结构性，并且用户的口味必须能够用内容特征形式来表达，不能显式地得到其它用户的判断情况。</p>
<h2 id="协同过滤推荐"><a href="#协同过滤推荐" class="headerlink" title="协同过滤推荐"></a>协同过滤推荐</h2><p>协同过滤推荐（Collaborative Filtering Recommendation）技术是推荐系统中应用最早和最为成功的技术之一。它一般采用最近邻技术，利用用户的历史喜好信息计算用户之间的距离，然后 利用目标用户的最近邻居用户对商品评价的加权评价值来预测目标用户对特定商品的喜好程度，系统从而根据这一喜好程度来对目标用户进行推荐。协同过滤最大优 点是对推荐对象没有特殊的要求，能处理非结构化的复杂对象，如音乐、电影。</p>
<p>协同过滤是基于这样的假设：为一用户找到他真正感兴趣的内容的好方法是首先找到与此用户有相似兴趣的其他用户，然后将他们感兴趣的内容推荐给此用户。其基本 思想非常易于理解，在日常生活中，我们往往会利用好朋友的推荐来进行一些选择。协同过滤正是把这一思想运用到电子商务推荐系统中来，基于其他用户对某一内 容的评价来向目标用户进行推荐。</p>
<p>基于协同过滤的推荐系统可以说是从用户的角度来进行相应推荐的，而且是自动的，即用户获得的推荐是系统从购买模式或浏览行为等隐式获得的，不需要用户努力地找到适合自己兴趣的推荐信息，如填写一些调查表格等。</p>
<p>和基于内容的过滤方法相比，协同过滤具有如下的优点：</p>
<ol>
<li>能够过滤难以进行机器自动内容分析的信息，如艺术品，音乐等。</li>
<li>共享其他人的经验，避免了内容分析的不完全和不精确，并且能够基于一些复杂的，难以表述的概念（如信息质量、个人品味）进行过滤。</li>
<li>有推荐新信息的能力。可以发现内容上完全不相似的信息，用户对推荐信息的内容事先是预料不到的。这也是协同过滤和基于内容的过滤一个较大的差别，基于内容的过滤推荐很多都是用户本来就熟悉的内容，而协同过滤可以发现用户潜在的但自己尚未发现的兴趣偏好。</li>
<li>能够有效的使用其他相似用户的反馈信息，较少用户的反馈量，加快个性化学习的速度。<br>虽然协同过滤作为一种典型的推荐技术有其相当的应用，但协同过滤仍有许多的问题需要解决。最典型的问题有稀疏问题（Sparsity）和可扩展问题（Scalability）。</li>
</ol>
<p>协同过滤（CF）可以看做是一个分类问题，也可以看做是矩阵分解问题。协同滤波主要是基于每个人自己的喜好都类似这一特征，它不依赖于个人的基本信息。比如刚刚那个电影评分的例子中，预测那些没有被评分的电影的分数只依赖于已经打分的那些分数，并不需要去学习那些电影的特征。</p>
<p>SVD将矩阵分解为三个矩阵的乘积，公式如下所示：</p>
 $$Data_{m\times n}=U_{m\times m}\Sigma_{m\times n}V_{n\times n}^T$$ 
<p>中间的矩阵sigma为对角矩阵，对角元素的值为Data矩阵的奇异值(注意奇异值和特征值是不同的)，且已经从大到小排列好了。即使去掉特征值小的那些特征，依然可以很好的重构出原始矩阵。</p>
<p>如果 m 代表商品的个数，n 代表用户的个数，则 U 矩阵的每一行代表商品的属性，现在通过降维 U 矩阵（取深色部分）后，每一个商品的属性可以用更低的维度表示（假设为 k 维）。这样当新来一个用户的商品推荐向量 X，则可以根据公式  $X'U_1\Sigma_1^{-1}$$X’U_1\Sigma_1^{-1}$ 得到一个 k 维的向量，然后在 V’ 中寻找最相似的那一个用户（相似度测量可用余弦公式等），根据这个用户的评分来推荐（主要是推荐新用户未打分的那些商品）。</p>
<h2 id="基于关联规则推荐"><a href="#基于关联规则推荐" class="headerlink" title="基于关联规则推荐"></a>基于关联规则推荐</h2><p>基于关联规则的推荐（Association Rule-based Recommendation）是以关联规则为基础，把已购商品作为规则头，规则体为推荐对象。关联规则挖掘可以发现不同商品在销售过程中的相关性，在零 售业中已经得到了成功的应用。管理规则就是在一个交易数据库中统计购买了商品集X的交易中有多大比例的交易同时购买了商品集Y，其直观的意义就是用户在购 买某些商品的时候有多大倾向去购买另外一些商品。比如购买牛奶的同时很多人会同时购买面包。</p>
<p>算法的第一步关联规则的发现最为关键且最耗时，是算法的瓶颈，但可以离线进行。其次，商品名称的同义性问题也是关联规则的一个难点。</p>
<h2 id="基于效用推荐"><a href="#基于效用推荐" class="headerlink" title="基于效用推荐"></a>基于效用推荐</h2><p>基于效用的推荐（Utility-based Recommendation）是建立在对用户使用项目的效用情况上计算的，其核心问题是怎么样为每一个用户去创建一个效用函数，因此，用户资料模型很大 程度上是由系统所采用的效用函数决定的。基于效用推荐的好处是它能把非产品的属性，如提供商的可靠性（Vendor Reliability）和产品的可得性（Product Availability）等考虑到效用计算中。</p>
<h2 id="基于知识推荐"><a href="#基于知识推荐" class="headerlink" title="基于知识推荐"></a>基于知识推荐</h2><p>基于知识的推荐（Knowledge-based Recommendation）在某种程度是可以看成是一种推理（Inference）技术，它不是建立在用户需要和偏好基础上推荐的。基于知识的方法因它们所用的功能知识不同而有明显区别。效用知识（Functional Knowledge）是一种关于一个项目如何满足某一特定用户的知识，因此能解释需要和推荐的关系，所以用户资料可以是任何能支持推理的知识结构，它可以是用户已经规范化的查询，也可以是一个更详细的用户需要的表示。</p>
<h2 id="组合推荐"><a href="#组合推荐" class="headerlink" title="组合推荐"></a>组合推荐</h2><p>由于各种推荐方法都有优缺点，所以在实际中，组合推荐（Hybrid Recommendation）经常被采用。研究和应用最多的是内容推荐和协同过滤推荐的组合。最简单的做法就是分别用基于内容的方法和协同过滤推荐方法 去产生一个推荐预测结果，然后用某方法组合其结果。尽管从理论上有很多种推荐组合方法，但在某一具体问题中并不见得都有效，组合推荐一个最重要原则就是通 过组合后要能避免或弥补各自推荐技术的弱点。</p>
<p>在组合方式上，有研究人员提出了七种组合思路：</p>
<ol>
<li>加权（Weight）：加权多种推荐技术结果。</li>
<li>变换（Switch）：根据问题背景和实际情况或要求决定变换采用不同的推荐技术。</li>
<li>混合（Mixed）：同时采用多种推荐技术给出多种推荐结果为用户提供参考。</li>
<li>特征组合（Feature combination）：组合来自不同推荐数据源的特征被另一种推荐算法所采用。</li>
<li>层叠（Cascade）：先用一种推荐技术产生一种粗糙的推荐结果，第二种推荐技术在此推荐结果的基础上进一步作出更精确的推荐。</li>
<li>特征扩充（Feature augmentation）：一种技术产生附加的特征信息嵌入到另一种推荐技术的特征输入中。</li>
<li>元级别（Meta-level）：用一种推荐方法产生的模型作为另一种推荐方法的输入。</li>
</ol>
<h2 id="推荐书籍"><a href="#推荐书籍" class="headerlink" title="推荐书籍"></a>推荐书籍</h2><ul>
<li>《推荐系统实践》</li>
<li>一些发表的综述论文</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是我学习推荐系统时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="笔记" scheme="http://wdxtub.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="推荐系统" scheme="http://wdxtub.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>数据分析指南</title>
    <link href="http://wdxtub.com/2016/09/10/data-analysis-guide/"/>
    <id>http://wdxtub.com/2016/09/10/data-analysis-guide/</id>
    <published>2016-09-10T02:38:12.000Z</published>
    <updated>2016-09-10T02:39:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是我学习数据分析时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。</p>
<a id="more"></a>
<hr>
<p>数据分析是我这一年多来才开始研究的方向，资历尚浅，但是看了不少科普类书籍，有一些小小的感悟，记录在这里。最后是之前上的一套公开课的笔记，感觉对于入门还是很有帮助的。</p>
<h2 id="统计分析中的一些陷阱"><a href="#统计分析中的一些陷阱" class="headerlink" title="统计分析中的一些陷阱"></a>统计分析中的一些陷阱</h2><ul>
<li>有 3 种谎言：谎言，糟糕透顶的谎言和统计资料</li>
<li>对于追求效率的公民而言，统计思维总有一天会和读写能力一样重要</li>
<li>使我们陷入麻烦的通常并非我们不知道的事情，而是那些我们知道却不正确的事情</li>
<li>整数总是不完善的</li>
<li>我需要完成一个很大的课题——统计学，但却感到我的写作功底十分有限，如果不牺牲准确性和完整性，就很难使人理解</li>
<li>单凭某一数据很难反应实情</li>
<li>一条河永远不可能高于它的源头，同理，对样本研究后得到的结论不会好于样本本身</li>
<li>一个以抽样为基础的报告如果要有价值，就必须使用具有代表性的样本，这种样本排除了各种误差</li>
<li>无形的误差与有形的误差一样容易破坏样本的可信度</li>
<li>普查工作者一般都具有足够的统计知识、技术以及调查费用以确保抽样的精确度。他们并非居心叵测之徒。但并不是所有能见到的数据都产生于这样良好的环境，也并不是所有的数据都会有附有类似的精确度说明</li>
<li>如果某条信息提供了显著性程度，你将对它有更深的了解。显著成都通常用概率表示。</li>
<li>将『正常的』与『期望的』混为一谈导致事情变得更糟</li>
<li>这些没有透露的数据其欺骗性在于人们经常忽略了它们的不存在，这当然也是使用这些数据的人获取成功的奥秘</li>
<li>当一个平均数、一张图表或者某种趋势遗漏了这些重要的数据，请对它们保留一些怀疑。</li>
<li>你的样本以多大的精度代表总体是可以用数据来衡量的，那就是：可能误差和标准误差</li>
<li>只有当差别有意义时才能称之为差别</li>
<li>注意比例尺和起始标尺，这可能会产生极大的误导性</li>
<li>利用一维图形的信息不对称，可以营造出非常夸张的视觉效果</li>
<li>如果你想证明某事，却发现没有能力办到，那么试着解释其他事情并假装它们是同一回事。</li>
<li>相关并不等于因果，一定要注意这里的区别</li>
<li>扭曲统计数据的最巧妙方法是利用地图</li>
<li>百分数也给误解提供了肥沃的土壤。和小数一样，它也能为不确切的食物蒙上精确的面纱</li>
<li>将一些看似能直接相加却不能这样操作的事情加在一起会产生大量的欺骗和隐瞒</li>
<li>对统计资料提出的五个问题<ol>
<li>谁说的？有意识的偏差和无意识的偏差</li>
<li>他是如何知道的？注意样本的有偏，数值是否足够大</li>
<li>遗漏了什么？</li>
<li>是否有人偷换了概念？</li>
<li>这个资料有意义吗？</li>
</ol>
</li>
<li>我们以为自己可以控制很多风险，但结果并非如此，也许这才是更大的威胁。</li>
<li>贪婪和恐惧是两个非常不稳定的因素，只有两者保持平衡，经济才能顺利发展。若贪婪在经济体系中占上风，就会产生经济泡沫；若恐惧因素压过贪婪，经济又会陷入恐慌。</li>
<li>狐狸型预测方法<ul>
<li>用概率的方法思考问题</li>
<li>今天的预测是你以后人生的第一个预测</li>
<li>寻求共识 </li>
</ul>
</li>
<li>信息是决定预测成败的关键，并不是信息越多，预测就越成功</li>
<li>经济是一个动态系统，不是一个方程式</li>
<li>运气和技能通常被视为两个极端，但两者之间的关系其实更复杂一些</li>
<li>若想做出更准确的预测，就必须承认我们的判断是不可靠的</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">狐狸型专家的想法</th>
<th style="text-align:center">刺猬型专家的想法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">千伎百俩：汇聚不同学科的思想，忽略最初的政治派别</td>
<td style="text-align:center">一技之长：把大部分精力投入到一两个重大问题上，以怀疑的眼光看待『局外人』的观点</td>
</tr>
<tr>
<td style="text-align:center">适应力强：最初的方法失效后，试图找到新的方法，或同时寻求多种方法</td>
<td style="text-align:center">坚持力强：坚持『总揽一切』的方法，新数据只能用来改善原始模式</td>
</tr>
<tr>
<td style="text-align:center">严于律己：有时会愿意（或是欣于）承认预测中的错误并接受谴责</td>
<td style="text-align:center">固执己见：错误归咎到坏运气或特殊情况上——好模式没有赶上好时机</td>
</tr>
<tr>
<td style="text-align:center">承认复杂性：承认宇宙的复杂性，认为许多基本问题不可解决或本身就是不可预测的</td>
<td style="text-align:center">寻找秩序：一旦从噪声中找到信号，便期望世界遵循某种相对简单的支配关系</td>
</tr>
<tr>
<td style="text-align:center">谨慎：用概率术语表达预测结论，并且证明自己的观点是正确的</td>
<td style="text-align:center">自信：很少对自己的预测进行正面回复，并且不愿改变自己的预测</td>
</tr>
<tr>
<td style="text-align:center">经验主义：更多地依赖观察而非理论</td>
<td style="text-align:center">意识形态：期待日常的问题正是宏伟理论或斗争的体现</td>
</tr>
<tr>
<td style="text-align:center">较好的预测家</td>
<td style="text-align:center">较差的预测家</td>
</tr>
</tbody>
</table>
<h2 id="互联网数据挖掘"><a href="#互联网数据挖掘" class="headerlink" title="互联网数据挖掘"></a>互联网数据挖掘</h2><p>万小军 <a href="http://www.icst.pku.edu.cn/lcwm" target="_blank" rel="external">http://www.icst.pku.edu.cn/lcwm</a></p>
<h2 id="互联网挖掘概述"><a href="#互联网挖掘概述" class="headerlink" title="互联网挖掘概述"></a>互联网挖掘概述</h2><ul>
<li>Web 挖掘<ul>
<li>由 Etzioni(1996) 提出</li>
<li>使用数据挖掘的技术自动从 Web文档/服务发现和提取信息和知识，包括隐含的模式和关系等</li>
</ul>
</li>
<li>相关技术<ul>
<li>信息检索</li>
<li>互联网</li>
<li>数据库</li>
<li>机器学习</li>
<li>自然语言处理</li>
<li>数据挖掘</li>
</ul>
</li>
<li>相关学术会议<ul>
<li>Web 搜索: SIGIR, WWW, CIKM, WSDM</li>
<li>数据挖掘: KDD, ICDM</li>
<li>自然语言处理: ACL, EMNLP</li>
<li>多媒体检索: ACM, MM</li>
</ul>
</li>
<li>应用<ul>
<li>垂直搜索</li>
<li>个性化推荐</li>
<li>智能问答</li>
<li>机器翻译</li>
<li>舆情监测</li>
<li>情报与反恐</li>
<li>预测</li>
</ul>
</li>
</ul>
<h2 id="文本信息检索"><a href="#文本信息检索" class="headerlink" title="文本信息检索"></a>文本信息检索</h2><ul>
<li>两个核心问题<ul>
<li>效果<ul>
<li>如何准确匹配查询与文档</li>
<li>基于检索模型</li>
</ul>
</li>
<li>效率<ul>
<li>如何快速返回检索结果</li>
<li>基于索引</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="文档表示"><a href="#文档表示" class="headerlink" title="文档表示"></a>文档表示</h3><ul>
<li>元描述(Meta-descriptions)<ul>
<li>域信息(author, title, date)</li>
<li>关键词、受控词汇</li>
<li>分类</li>
<li>优点<ul>
<li>主要依赖人工标注，结果比较可靠</li>
<li>基于受控词汇的检索很高效</li>
</ul>
</li>
<li>不足<ul>
<li>人工标注非常耗时</li>
<li>检索受限制</li>
</ul>
</li>
</ul>
</li>
<li>自动文档表示<ul>
<li>词袋(Bag of Words)<ul>
<li>一篇文档由该文档中出现的词的集合所表示</li>
<li>词语的无序集合，句法信息丢失</li>
<li>符号化(tokenization):识别词的边界</li>
<li>英文中大小写</li>
<li>词语形态规范化<ul>
<li>匹配 company 和 companies? sell 与 sold?</li>
<li>删除词语的形态信息: 时态，数量…</li>
</ul>
</li>
<li>词根(Stemming)<ul>
<li>删除后缀</li>
<li>基于规则进行(例如 Porter’s stemmer)</li>
<li>Stemming 的结果可能不是词语</li>
<li>不相关的词可能具有相同的 stem</li>
</ul>
</li>
<li>词形还原(Lemmatization)<ul>
<li>将词语变为其语法原型(syntactic stem)</li>
<li>使用一般规则与例外处理</li>
<li>处理结果仍然为词</li>
<li>处理过程要考虑词性的不同</li>
</ul>
</li>
<li>停用词(Stop Words)<ul>
<li>不具有内容信息的词</li>
<li>停用词表依赖于具体文档集及具体应用</li>
<li>过滤停用词的原因<ul>
<li>停用词并不能提高检索效果</li>
<li>可以大幅减少索引大小</li>
<li>减少检索时间</li>
</ul>
</li>
</ul>
</li>
<li>大部分互联网搜索引擎并不使用 stemming/lemmatizatoin<ul>
<li>文档集很大</li>
<li>不太考虑召回率</li>
<li>Stemming 结果并不完美</li>
</ul>
</li>
<li>大部分互联网搜索引擎使用停用词表</li>
<li>无法从词袋表示恢复原文档</li>
<li>优点: 简单、有效</li>
<li>缺点: 忽略了词之间和句法关系以及篇章结构的信息</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="文档索引"><a href="#文档索引" class="headerlink" title="文档索引"></a>文档索引</h3><ul>
<li>目的在于提高检索效率</li>
<li>索引对象的选择</li>
<li>Phrase 识别比较困难，并不能显著提高检索效果</li>
<li>通常采用 word 构建索引</li>
<li>索引压缩: 减小索引大小</li>
<li>动态索引: 索引库的动态维护、更新</li>
<li>分布式索引: 索引信息分布在不同机器上</li>
<li>具体细节参阅 Lucene 开源检索系统的相关代码</li>
</ul>
<h4 id="倒排索引-inverted-index"><a href="#倒排索引-inverted-index" class="headerlink" title="倒排索引(inverted index)"></a>倒排索引(inverted index)</h4><ul>
<li>以关键词为核心对文档进行索引</li>
<li>帮助快速地找到文档中所包含的关键词</li>
<li>可看做连标数组，每个链表的表头包含关键词，其后序单元则包括所有包括这个关键词的文档标号，以及一些其他信息，如该词的频率和位置等</li>
<li>优势<ul>
<li>关键词个数比文档少，因此检索效率高</li>
<li>特别适合信息检索</li>
</ul>
</li>
<li>数据结构<ul>
<li>关键词查询一般采用 B-Tree 或哈希表</li>
<li>文档列表组织一般采用二叉搜索树</li>
</ul>
</li>
</ul>
<h3 id="文本信息检索模型"><a href="#文本信息检索模型" class="headerlink" title="文本信息检索模型"></a>文本信息检索模型</h3><ul>
<li>基本思想: 如果一篇文档与一个查询相似，那么该文档与查询相关</li>
<li>相似性<ul>
<li>字符串匹配</li>
<li>相似的词汇</li>
<li>相同的语义</li>
</ul>
</li>
<li>检索模型<ul>
<li>对实际检索过程的抽象和建模</li>
<li>终极目标是基于语义进行匹配</li>
</ul>
</li>
</ul>
<h4 id="布尔模型-Boolean-Model"><a href="#布尔模型-Boolean-Model" class="headerlink" title="布尔模型(Boolean Model)"></a>布尔模型(Boolean Model)</h4><ul>
<li>基于布尔代数</li>
<li>优点<ul>
<li>简单</li>
<li>对查询严格掌控</li>
</ul>
</li>
<li>缺点<ul>
<li>一般用户难以构造布尔查询</li>
<li>检索结果文档无法排序</li>
<li>严格匹配，导致过少或过多的检索结果</li>
</ul>
</li>
<li>尽管布尔模型不再用作主流文档检索模型，但其思想常用于实现高级(综合)检索功能</li>
</ul>
<h4 id="向量空间模型-Vector-Space-Model"><a href="#向量空间模型-Vector-Space-Model" class="headerlink" title="向量空间模型(Vector Space Model)"></a>向量空间模型(Vector Space Model)</h4><ul>
<li>现代信息检索系统中比较常用的检索模型</li>
<li>特性<ul>
<li>用户输入自由文本作为查询</li>
<li>对文档进行排序</li>
<li>匹配准则放松</li>
</ul>
</li>
<li>思想: 文档与查询都是高维空间中的一个向量</li>
<li>向量空间表示<ul>
<li>文档是词语组成的向量</li>
<li>词语是文档组成的向量</li>
<li>查询是词语组成的向量</li>
</ul>
</li>
<li>相似性<ul>
<li>用内积衡量<ul>
<li>缺点<ul>
<li>长文档由于更可能包含匹配词语，因而更可能相关</li>
<li>然而，如果两篇文档具有同样的相似值，用户更倾向于短文档，短文档更聚焦在用户信息需求上</li>
</ul>
</li>
<li>相似性计算中应该考虑文档长度(进行规范化)</li>
</ul>
</li>
<li>用夹角来衡量<ul>
<li>向量归一化</li>
<li>余弦度量(Cosine Measure)</li>
<li>余弦相似度(Cosine Similarity)</li>
</ul>
</li>
</ul>
</li>
<li>词语权重<ul>
<li>之前采用二值表示: 非 1 即 0<ul>
<li>没有反映词语频率</li>
<li>假定所有词语均同等重要</li>
</ul>
</li>
<li>tf: 词语出现的频率<ul>
<li>tf~i,j~ = frequency of term i in document j</li>
</ul>
</li>
<li>idf: 区别不同词语的重要性</li>
<li>tfidf</li>
</ul>
</li>
<li>其他相似性度量<ul>
<li>Minkowski metric(dissimilarity)</li>
<li>Euclidian distance(dissimilarity)</li>
<li>Jacquard measure</li>
<li>Dice’s coefficient</li>
</ul>
</li>
</ul>
<h4 id="概率模型-Probabilistic-Model"><a href="#概率模型-Probabilistic-Model" class="headerlink" title="概率模型(Probabilistic Model)"></a>概率模型(Probabilistic Model)</h4><ul>
<li>1977 年由 Stephen Robertson 提出</li>
</ul>
<h3 id="信息检索评价"><a href="#信息检索评价" class="headerlink" title="信息检索评价"></a>信息检索评价</h3><ul>
<li>评鉴检索模型或搜索引擎的性能</li>
<li>搜索质量 vs 搜索效率</li>
<li>对搜索质量的评价<ul>
<li>评测数据集</li>
<li>评测指标</li>
</ul>
</li>
<li>相关评测<ul>
<li>TREC<ul>
<li>trec.nist.gov</li>
<li>最具影响力</li>
<li>多种信息检索任务，侧重于英文</li>
</ul>
</li>
<li>NTCIR<ul>
<li>research.nii.ac.jp/ntcir/</li>
</ul>
</li>
<li>CLEF<ul>
<li>www.clef-campaign.org</li>
</ul>
</li>
</ul>
</li>
<li>评测数据集<ul>
<li>一般人工构建</li>
<li>构成<ul>
<li>较大规模的文档集合 D</li>
<li>查询集 Q 以及每个查询 q 对应的相关文档列表 REL~q~</li>
</ul>
</li>
</ul>
</li>
<li>评价指标<ul>
<li>衡量检索结果与标准答案的一致性</li>
<li>对非排序检索的评价<ul>
<li>对检索结果集合进行整体评价</li>
<li>准确率(precision)与召回率(recall)，通常相互依赖</li>
<li>F 值(F-measure)</li>
</ul>
</li>
<li>对排序结果的评价<ul>
<li>考虑相关文档在检索结果中的排序位置</li>
<li>在不同 recall levels 的 precision 值</li>
<li>沿着检索结果列表从头往后考察</li>
</ul>
</li>
<li>对多级排序检索的评价<ul>
<li>NDCG - Normalized Discounted Cumulative Gain</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Web-检索"><a href="#Web-检索" class="headerlink" title="Web 检索"></a>Web 检索</h2><p>Web 检索 = 文档检索 + 针对 Web 检索的新技术</p>
<h3 id="Web-页面采集"><a href="#Web-页面采集" class="headerlink" title="Web 页面采集"></a>Web 页面采集</h3><ul>
<li>Web Crawler, spider<ul>
<li>快速有效地收集尽可能多的有用 Web 页面，包括页面之间的链接结构</li>
</ul>
</li>
<li>Web 爬虫需要具备的特性<ul>
<li>健壮 robustness, 避免 spider traps</li>
<li>友好 politeness, 遵守 web server 的采集协议</li>
<li>分布式 distributed, 多台机器分布式采集</li>
<li>可扩展 scalable, 爬虫架构方便扩展</li>
<li>性能与效率，有效利用系统资源</li>
<li>质量 quality, 倾向于采集有用的页面</li>
<li>新颖 freshness, 获取网页的最新版本</li>
<li>可扩充 Extensible, 能够处理新数据类型、新的采集协议等</li>
</ul>
</li>
<li>Web 页面爬取策略<ul>
<li>深度优先</li>
<li>广度优先</li>
<li>实际应用中以广度优先为主，深度优先为辅</li>
</ul>
</li>
<li>难点<ul>
<li>暗网的采集，只有向数据库提交查询才形成的 Web 页面</li>
<li>Web 2.0 内容，脚本语言等生成的动态内容</li>
<li>多媒体内容</li>
</ul>
</li>
</ul>
<h3 id="Web-页面排序"><a href="#Web-页面排序" class="headerlink" title="Web 页面排序"></a>Web 页面排序</h3><ul>
<li>与查询相似度最大的网页并不一定是最好的结果</li>
<li>用户对 Web 页面的期望<ul>
<li>易于理解，可靠，作为查询话题的入口，能够回答差群的信息需求</li>
</ul>
</li>
<li>基于相关度的排序<ul>
<li>计算查询与页面内容的相似程度(依据文档检索模型)</li>
</ul>
</li>
<li>基于重要性的排序<ul>
<li>基于链接分析计算</li>
</ul>
</li>
<li>综合排序<ul>
<li>页面排序值 = 页面内容相关度 +/* 页面重要性</li>
</ul>
</li>
</ul>
<h3 id="Web-链接分析"><a href="#Web-链接分析" class="headerlink" title="Web 链接分析"></a>Web 链接分析</h3><ul>
<li>Web 页面之间的超链关系非常重要</li>
<li>一条从页面 A 指向页面 B 的链接表明<ul>
<li>A 与 B 相关</li>
<li>A 推荐/引用/投票/赞成 B</li>
</ul>
</li>
<li>经典算法<ul>
<li>HITS[Kleinberg, 1997] - Hypertext Induced Topic Selection</li>
<li>PageRank[Page &amp; Brin, 1998]</li>
</ul>
</li>
<li>排序沉入(Rank Sink)<ul>
<li>形成 loop, 没有 outlink</li>
<li>页面 A 与 B 相互提高 Rank, 而不向外分发 Rank</li>
</ul>
</li>
</ul>
<h3 id="基于学习的网页排序"><a href="#基于学习的网页排序" class="headerlink" title="基于学习的网页排序"></a>基于学习的网页排序</h3><ul>
<li>Learning to Rank 主流算法<ul>
<li>RankSVM</li>
<li>RankNet</li>
<li>ListNet</li>
</ul>
</li>
<li>Learning to Rank 工具包<ul>
<li>RankLib</li>
<li>people.cs.umass.edu/~vdang/ranklib.html</li>
</ul>
</li>
</ul>
<h3 id="搜索引擎优化-SEO"><a href="#搜索引擎优化-SEO" class="headerlink" title="搜索引擎优化(SEO)"></a>搜索引擎优化(SEO)</h3><ul>
<li>Search engine optimization<ul>
<li>提高网站或网页在搜索引擎中可见度(排名)的过程</li>
</ul>
</li>
<li>基本思想<ul>
<li>基于各类搜索引擎的工作原理(采集、索引、排序等)，对网站进行相关的优化的调整，提高网站在搜索引擎中的排名</li>
<li>白帽：合理优化，不牺牲用户体验</li>
<li>黑帽：不正当优化，牺牲用户体验<ul>
<li>重复、隐藏文字、链接工厂、桥页、跳页</li>
</ul>
</li>
</ul>
</li>
<li>影响排名的因素<ul>
<li>内部因素<ul>
<li>URL 中出现关键词</li>
<li>网页 Title 中出现关键词</li>
<li>常规内容中出现关键词</li>
<li>在页面的最后一段中出现关键词</li>
<li><code>&lt;Head&gt;</code> 标签 比如 h1, h2 中出现关键词</li>
<li>站内的链接中出现关键词</li>
<li>导向相关内容的导出链接</li>
<li>导出链接中出现关键词</li>
<li>图片文件名中出现关键词</li>
<li>Alt 标签中出现关键词</li>
<li>comment 中出现关键词</li>
<li>合理的频率更新内容</li>
<li>内容对搜索引擎的展示位置</li>
<li>网站结构循环 PR, 而非散发 PR</li>
<li>关键词进行适当的修饰(加粗、斜体等)</li>
</ul>
</li>
<li>外部因素<ul>
<li>大量的导入链接</li>
<li>从高 PR 值的网页获得导入链接</li>
<li>从相关内容网站获得导入链接</li>
<li>导入链接指向的网页有具体内容</li>
<li>锚文字中有关键词</li>
<li>锚文字周围有相关词</li>
<li>锚文字存在于文章或句子中</li>
<li>导入链接的时间长度，一般导入链接的存在时间有3-6个月</li>
<li>单向链接的价值高于交换链接</li>
<li>导入链接的页面的导出链接小于 100 个，流出链接越少越好</li>
<li>链接来自不同 IP</li>
<li>合理的导入链接增长频率</li>
</ul>
</li>
</ul>
</li>
<li>SEO 操作<ul>
<li>站外 SEO<ul>
<li>网站外部链接优化、网站的链接见识、网站的外部数据分析等</li>
<li>交换、购买链接，提交网址到分类目录等</li>
</ul>
</li>
<li>站内 SEO<ul>
<li>网站结构的设计、网站代码优化和内部链接优化、网站内容的优化、网站用户体验优化等</li>
<li>丰富网站关键词、主题集中、友好的网页结构、避免重复、有规律的更新等</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="自然语言处理基础"><a href="#自然语言处理基础" class="headerlink" title="自然语言处理基础"></a>自然语言处理基础</h2><p>又称<strong>自然语言理解</strong>，是人工智能和语言学领域的分支学科。利用计算机对人类特有的书面形式和口头形式的自然语言进行各种类型处理和加工的技术。</p>
<p>自然语言生成：利用计算机自动生成可理解的自然语言文本的技术</p>
<h3 id="基本任务"><a href="#基本任务" class="headerlink" title="基本任务"></a>基本任务</h3><ul>
<li>关键任务<ul>
<li>自动分词</li>
<li>命名实体识别</li>
<li>词性标注</li>
<li>句法分析</li>
<li>语义分析</li>
<li>篇章分析</li>
</ul>
</li>
<li>应用型任务<ul>
<li>机器翻译 Translation</li>
<li><strong>文本分类</strong> Classification</li>
<li>情感分析</li>
<li>信息检索与过滤 Mining/Retrieval</li>
<li>自动问答 Question Answering</li>
<li><strong>信息抽取</strong> Extraction</li>
<li><strong>自动文摘</strong> Summarization</li>
<li>人机对话 Dialogue</li>
</ul>
</li>
</ul>
<h3 id="为什么如此之难"><a href="#为什么如此之难" class="headerlink" title="为什么如此之难"></a>为什么如此之难</h3><ul>
<li>自然语言与生俱来的歧义问题</li>
<li>自然语言中存在未知的语言现象<ul>
<li>新的词汇</li>
<li>新的含义</li>
<li>新的用法和语句结构</li>
</ul>
</li>
<li>人类用语言进行社会交流时会省略公有制是，但机器很难获取及更新世界知识</li>
</ul>
<h3 id="推荐教材"><a href="#推荐教材" class="headerlink" title="推荐教材"></a>推荐教材</h3><ul>
<li>Foundations of Statistical Natrual Language Processing</li>
<li>Speech and Language Processing</li>
<li>统计自然语言处理</li>
</ul>
<h3 id="汉语自动分词"><a href="#汉语自动分词" class="headerlink" title="汉语自动分词"></a>汉语自动分词</h3><p>一般认为：词是最小的、能够独立运用的、有意义的语言单位</p>
<p><strong>汉语分词的挑战</strong></p>
<ul>
<li>词和词组的边界模糊</li>
<li>新词(未登陆词)</li>
<li>切分歧义<ul>
<li>汉字串 AJB 被称作<strong>交集型切分歧义</strong>，如果满足 AJ, JB 同时为词，此时汉字串 J 被称作交集串</li>
<li>汉字串 AB 被称作<strong>组合型切分歧义</strong>，如果满足条件 A, B, AB 同时为词</li>
<li>真歧义：存在两种或两种以上的真实存在的切分形式</li>
</ul>
</li>
</ul>
<p><strong>分词方法</strong></p>
<ul>
<li>简单的模式匹配<ul>
<li>正向最大匹配(FMM)、逆向最大匹配(BMM, 比正向更有效)、双向匹配(BM, 比较两种方法的结果，大颗粒词越多越好，非词典词和单子词越少越好，可以识别出交叉歧义)</li>
</ul>
</li>
<li>基于规则的方法<ul>
<li>最少分词算法</li>
</ul>
</li>
<li>基于统计的方法<ul>
<li>统计语言模型分词、串频统计和词形匹配相结合的汉语自动分词、无词典分词</li>
<li>第一步是候选网格构造：利用词典匹配，列举输入句子所有可能的切分词语，并以词网格形式保存</li>
<li>第二步计算词网格中的每一条路径的权值，权值通过计算图中的每一个节点(每一个词)的一元统计概率和节点之间的二元统计概率的相关信息</li>
<li>最后根据图搜索算法在图中找到一条权值最大的路径，作为最后的分词结果</li>
<li>优缺点：可利用不同的统计语言模型计算最优路径，具有比较高的分词正确率；但算法时间、空间复杂度较高</li>
</ul>
</li>
</ul>
<h3 id="词性标注-POS-Tagging"><a href="#词性标注-POS-Tagging" class="headerlink" title="词性标注(POS Tagging)"></a>词性标注(POS Tagging)</h3><ul>
<li>为句子中的每个词语标注词性(part-of-speech marker)</li>
<li>可看做是词法分析的关键任务，也可看做是句法分析的最低层次</li>
<li>对后续句法分析、词义消岐等任务非常有用</li>
</ul>
<h3 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h3><ul>
<li>句法/成分/短语结构分析 vs 依存关系分析</li>
<li>完全分析 vs 局部分析/浅层分析</li>
<li>Context Free Grammars(CFG)</li>
</ul>
<h2 id="数据挖掘概述与关联规则挖掘"><a href="#数据挖掘概述与关联规则挖掘" class="headerlink" title="数据挖掘概述与关联规则挖掘"></a>数据挖掘概述与关联规则挖掘</h2><p>数据挖掘从多学科领域发展而来：Statistics/AI, Machine Learning, Pattern Recognition, Database systems</p>
<h3 id="数据挖掘流程"><a href="#数据挖掘流程" class="headerlink" title="数据挖掘流程"></a>数据挖掘流程</h3><p><strong>Data</strong> -Selection-&gt; <strong>Target data</strong> -Preprocessing-&gt; <strong>Preprocessed data</strong> -Transformation-&gt; <strong>Transformed data</strong> -Data mining-&gt; <strong>Patterns</strong> -Interpretation/evaluatoin-&gt; <strong>Knowledge</strong></p>
<h3 id="数据挖掘任务"><a href="#数据挖掘任务" class="headerlink" title="数据挖掘任务"></a>数据挖掘任务</h3><ul>
<li>预测型(Prediction Methods): 基于一些变量预测其他变量的未知值或未来值<ul>
<li>分类 Classification</li>
<li>回归 Regression</li>
<li>偏差检测 Deviation Detection</li>
</ul>
</li>
<li>描述型(Description Methods): 发现描述数据的人们可解释的模式<ul>
<li>聚类 Clustering</li>
<li>关联规则挖掘 Association Rule Discovery</li>
<li>摘要 Summarization</li>
</ul>
</li>
</ul>
<h3 id="关联规则挖掘：定义"><a href="#关联规则挖掘：定义" class="headerlink" title="关联规则挖掘：定义"></a>关联规则挖掘：定义</h3><p>关联规则反映一个事物与其他事物之间的相互依存性和关联性。如果两个或者多个事物之间存在一定的关联关系，那么其中一个事物就能够通过其他事物预测到。关联规则表示了项之间的关系。</p>
<ul>
<li>给定一个记录集合，每个记录包含若干项<ul>
<li>产生依赖规则，可以基于某些项的出现预测一个项的出现</li>
</ul>
</li>
</ul>
<p>应用：市场营销</p>
<ul>
<li>假如发现一条规则</li>
<li>{面包圈..} -&gt; {薯片}</li>
<li>可以帮助提高薯片的销量</li>
</ul>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>为数据集进行总结，提供一个简洁/紧凑的表示，包括可视化与报表生成</p>
<h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><ul>
<li>基于若干变量的值预测一个给定的具有连续值的变量的值<ul>
<li>假设一个线性或非线性的依赖模型</li>
</ul>
</li>
<li>举例<ul>
<li>基于广告开支预测新产品的销售额</li>
<li>基于温度、湿度、气压等预测风速</li>
<li>预测股票指数</li>
</ul>
</li>
</ul>
<p>为数据预测一个连续值，确定两种或两种以上变量间的相互依赖关系。</p>
<ul>
<li>最简单的情形: 一元线性回归<ul>
<li>只包括一个自变量 x 和一个因变量 y，且二者的关系可用一条直线近似表示</li>
<li>回归分析的目标为找一个线性函数迎合训练数据</li>
<li>可采用最小二乘法</li>
</ul>
</li>
<li>其他回归方法<ul>
<li>Logistic 回归</li>
<li>岭回归</li>
<li>支持向量回归</li>
</ul>
</li>
<li>回归结果评价<ul>
<li>均方误差(Mean Squared Error, MSE)</li>
<li>均方根误差(root mean square error, RMSE)</li>
</ul>
</li>
</ul>
<h3 id="偏差-异常检测"><a href="#偏差-异常检测" class="headerlink" title="偏差/异常检测"></a>偏差/异常检测</h3><ul>
<li>从正常行为中检测显著的偏差/异常</li>
<li>举例<ul>
<li>信用卡欺诈检测</li>
<li>网络入侵检测</li>
</ul>
</li>
</ul>
<h3 id="数据挖掘工具"><a href="#数据挖掘工具" class="headerlink" title="数据挖掘工具"></a>数据挖掘工具</h3><ul>
<li>Free open-source software and application<ul>
<li>Weka</li>
<li>GATE</li>
<li>Carrot2</li>
<li>NLTK</li>
<li>Orange</li>
<li>RapidMiner</li>
<li>KNIME</li>
</ul>
</li>
<li>Commercial software<ul>
<li>IBM InfoSphere Warehouse</li>
<li>Microsoft Analysis Services</li>
<li>SAS Enterprise Miner</li>
<li>STATISTICA Data Miner</li>
<li>Oracle Data Mining</li>
</ul>
</li>
</ul>
<h3 id="数据挖掘的挑战"><a href="#数据挖掘的挑战" class="headerlink" title="数据挖掘的挑战"></a>数据挖掘的挑战</h3><ul>
<li>可拓展性</li>
<li>挖掘高维数据、高速数据流</li>
<li>挖掘序列数据、时间序列数据</li>
<li>从复杂、异构、网络化数据中挖掘复杂知识</li>
<li>挖掘低质量数据</li>
<li>安全性、隐私保护</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>将数据划分到已知类别。分类器的构建基于有监督学习(标注数据:训练集)</p>
<ul>
<li>给定一个样例集合(训练集)<ul>
<li>每个样例包含一个属性集合，其中一个属性是类标记/类号</li>
</ul>
</li>
<li>基于训练集构建一个模型，该模型将类标记属性看作是其它属性值的一个函数</li>
<li>目标：对心的样例尽可能准确地赋予标记<ul>
<li>基于一个测试集来评估模型的准确性</li>
</ul>
</li>
</ul>
<p>Training Set -&gt; Learn Classifier -&gt; Model &lt;- Test set</p>
<ul>
<li>二类分类：正、负</li>
<li>多类分类：多类(&gt;=3)</li>
</ul>
<p>二类分类是分类问题的最基本形式，多类分类问题可通过转化为二类分类问题加以解决。</p>
<ul>
<li>One vs. One<ul>
<li>对于 K 类分类需要 K(K-1)/2 个二类分类器</li>
<li>测试分类时采用投票方式确定类别</li>
<li>E.g. Labels: a, b, c</li>
<li>分类器: a vs b, b vs c, a vs c</li>
</ul>
</li>
<li>One vs. All(Rest):<ul>
<li>对于 K 类分类需要 K 个二类分类器</li>
<li>测试分类时取返回最大值的类别</li>
<li>E.g. Labels: a, b, c</li>
<li>分类器: a vs (b, c), b vs (a, c), c vs (a, b)</li>
</ul>
</li>
</ul>
<p>层次式分类，类别构成层次式结构(树状)。中图法、ODP目录。</p>
<ul>
<li>应用<ul>
<li>新闻分类</li>
<li>广告页面判别</li>
<li>垃圾邮件过滤</li>
<li>垃圾短信过滤</li>
<li>博客风格判断</li>
<li>评论情感分析</li>
</ul>
</li>
</ul>
<h3 id="基于规则的分类"><a href="#基于规则的分类" class="headerlink" title="基于规则的分类"></a>基于规则的分类</h3><p>人工/专家制定分类规则，使用布尔操作符(AND, OR, NOT)，可将规则组织成决策树，节点代表规则，叶节点代表类别</p>
<ul>
<li>优势<ul>
<li>透明，易于理解，易于修改</li>
</ul>
</li>
<li>劣势<ul>
<li>复杂，耗时</li>
<li>主要依赖人工/专家的智能，而非系统/机器智能</li>
<li>不易拓展</li>
<li>绝对的类别划分，无置信度</li>
</ul>
</li>
</ul>
<h3 id="基于统计的分类"><a href="#基于统计的分类" class="headerlink" title="基于统计的分类"></a>基于统计的分类</h3><ul>
<li>优势<ul>
<li>可以输出置信概率、允许阈值控制、可扩展</li>
</ul>
</li>
<li>劣势<ul>
<li>需要已标注好的训练数据</li>
</ul>
</li>
<li>典型方法<ul>
<li>朴素贝叶斯</li>
<li>Rocchio</li>
<li>K 近邻</li>
<li>支持向量机</li>
</ul>
</li>
</ul>
<h3 id="文本统计分类流程"><a href="#文本统计分类流程" class="headerlink" title="文本统计分类流程"></a>文本统计分类流程</h3><ul>
<li>预处理<ul>
<li>Tokenization</li>
<li>Filtering</li>
<li>Stemming</li>
</ul>
</li>
<li>特征计算<ul>
<li>tf</li>
<li>Log(tf+1)</li>
<li>tfidf</li>
</ul>
</li>
<li>特征选择<ul>
<li>MI</li>
<li>Chi-Square</li>
<li>Information Gain</li>
</ul>
</li>
<li>分类学习<ul>
<li>SVM</li>
<li>Rocchio</li>
<li>Naive Bayes</li>
</ul>
</li>
<li>结果评估<ul>
<li>Recall</li>
<li>F-Measure</li>
<li>Precision</li>
</ul>
</li>
</ul>
<h3 id="文本分类-特征向量表示"><a href="#文本分类-特征向量表示" class="headerlink" title="文本分类-特征向量表示"></a>文本分类-特征向量表示</h3><ul>
<li>文本表示为特征向量<ul>
<li>词语(term): 基本单元</li>
<li>权重 TF * IDF<ul>
<li>TF: 词频</li>
<li>IDF: log(N/DF), N 为文档总数量，DF 为包含该词语的文档数量</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="文本分类-特征选择"><a href="#文本分类-特征选择" class="headerlink" title="文本分类-特征选择"></a>文本分类-特征选择</h3><ul>
<li>好处<ul>
<li>减少特征维数，改善效果</li>
<li>防止过拟合，增强模型的泛化能力</li>
<li>提高学习效率</li>
<li>提高模型的可解释性</li>
</ul>
</li>
<li>方法，从原有特征集合中找出一个更加有效的子集<ul>
<li>Document Frequency 文档频率<ul>
<li>词语的 DF 小于某个阈值去掉(太小，没有代表性)</li>
<li>词语的 DF 大于某个阈值也去掉(太多，没有区分度)</li>
</ul>
</li>
<li>Mutual Information 互信息<ul>
<li>MI 越大词语 t 和 c 共现程度越大</li>
</ul>
</li>
<li>Information Gain 信息增益<ul>
<li>词语 t 为整个分类所能提供的信息量(不考虑任何特征的熵和考虑该特征后的熵的差值)</li>
</ul>
</li>
<li>Information Ratio</li>
<li>Chi Squrae</li>
<li>Odd Ratio</li>
</ul>
</li>
</ul>
<h3 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a>朴素贝叶斯分类</h3><ul>
<li>基于概率理论的学习和分类方法</li>
<li>贝叶斯理论充当重要角色</li>
<li>分类是根据给定样本描述的可能的类别基础上产生的后验概率分布<ul>
<li>基于贝叶斯理论来计算后验概率</li>
</ul>
</li>
<li>分类原理：最大后验估计 MAP(maximum posteriori)</li>
</ul>
<h3 id="Rocchio-分类方法"><a href="#Rocchio-分类方法" class="headerlink" title="Rocchio 分类方法"></a>Rocchio 分类方法</h3><ul>
<li>使用中心向量(centroid vector)表示一个类别<ul>
<li>中心向量为某个类别下所有文档向量的平均向量</li>
<li>基于训练集计算</li>
</ul>
</li>
<li>对于新文档，计算该文档与每个类别的中心向量的距离/相似度<ul>
<li>基于余弦测度</li>
</ul>
</li>
<li>确定其类别为距离最小/相似性最大的类别</li>
<li>优势: 训练快速，模型很小，快速分类</li>
<li>劣势: 类别数量增加时准确率降低</li>
</ul>
<h3 id="K-近邻分类方法"><a href="#K-近邻分类方法" class="headerlink" title="K 近邻分类方法"></a>K 近邻分类方法</h3><ul>
<li>与 Rocchio 方法类似</li>
<li>检查新文档的 k 个近邻向量，利用这些近邻向量的类别来确定该文档的类别<ul>
<li>K 通过实验确定</li>
<li>“近邻” 通过相似度/距离测度来定义</li>
</ul>
</li>
<li>1 近邻分类方法<ul>
<li>将新文档划分到与其最相近的文档样例所属的类别</li>
</ul>
</li>
<li>K 近邻分类方法<ul>
<li>基于投票机制，将新文档划分到 k 个近邻文档中多数文档所属的类别</li>
<li>进一步，基于加权投票机制，根据近邻文档的相近程度，赋予每个近邻文档一定的权重</li>
</ul>
</li>
<li>优势<ul>
<li>不需要训练阶段</li>
<li>类别数量增加也具有良好的扩展性</li>
</ul>
</li>
<li>劣势<ul>
<li>训练数据很大时模型很大</li>
<li>需要大量内存</li>
<li>性能较慢</li>
</ul>
</li>
</ul>
<h3 id="支持向量机-Support-Vector-Machine"><a href="#支持向量机-Support-Vector-Machine" class="headerlink" title="支持向量机 Support Vector Machine"></a>支持向量机 Support Vector Machine</h3><ul>
<li>对于二类分类，该方法在向量空间中找出一个决策超平面(分类函数 f(x) = wx + b)，对属于两个类别的文档向量进行有效粉绿<ul>
<li>通常有很多可能的分割超平面</li>
<li>找到最好的一个超平面: 最大间隔准则<ul>
<li>使得属于不同类别的数据点间隔最大</li>
</ul>
</li>
<li>间隔区边缘上的向量称为支撑向量</li>
</ul>
</li>
<li>实际上要求解一个带约束的二次规划(quadratic programming, QP)问题</li>
<li>训练结果<ul>
<li>支撑向量</li>
</ul>
</li>
<li>分类<ul>
<li>计算新文档向量与支撑向量的距离</li>
</ul>
</li>
<li>常用工具<ul>
<li>SVMLight</li>
<li>LIBSVM</li>
</ul>
</li>
<li>优势<ul>
<li>只有支撑向量用来对新文档分类</li>
<li>模型小</li>
<li>一般不会过拟合</li>
</ul>
</li>
<li>劣势<ul>
<li>训练过程非常复杂: 优化问题</li>
</ul>
</li>
</ul>
<h3 id="分类结果评估"><a href="#分类结果评估" class="headerlink" title="分类结果评估"></a>分类结果评估</h3><ul>
<li>对每个分类都可以计算出 precision, recall, F-measure<ul>
<li>一般侧重正例的结果</li>
</ul>
</li>
<li>总体评价: 精度(Accuracy) = 正确分类的样本数量 / 总样本数量</li>
</ul>
<h3 id="如何应用于实际"><a href="#如何应用于实际" class="headerlink" title="如何应用于实际"></a>如何应用于实际</h3><ul>
<li>大多数应用都是直接采用已有的分类算法与工具<ul>
<li>不需要开发新的</li>
</ul>
</li>
<li>关键在于特征工程<ul>
<li>针对特定应用问题寻找合理、有效的特征集</li>
</ul>
</li>
<li>一般需要调节分类算法与工具的多个参数<ul>
<li>比如 KNN 中的 K，SVM 中的 C 参数等</li>
</ul>
</li>
</ul>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><ul>
<li>给定一个数据点集合，每个数据点具有一组属性，数据点之间能进行相似度度量，聚类目标为找到若干类簇：<ul>
<li>同一类簇中的数据点相似</li>
<li>不同类簇中的数据点不相似</li>
</ul>
</li>
<li>无监督学习<ul>
<li>没有标注数据</li>
<li>类簇未知</li>
</ul>
</li>
<li>相似度度量准则<ul>
<li>欧氏距离(L2 norm)</li>
<li>余弦准则</li>
<li>L1 范式(L1 norm)</li>
</ul>
</li>
<li>聚类算法<ul>
<li>K-Means 聚类</li>
<li>层次式聚类(Hierarchical clustering)</li>
<li>增量式单遍聚类</li>
<li>基于图分割的聚类</li>
<li>基于密度峰值的聚类</li>
</ul>
</li>
</ul>
<p>应用：文档聚类</p>
<ul>
<li>目标：发现文档类簇，同一类簇中文档相似(基于重要词语)</li>
<li>方法：识别每篇文档中的重要词语，基于文档相似性进行聚类</li>
</ul>
<h3 id="层次式聚类"><a href="#层次式聚类" class="headerlink" title="层次式聚类"></a>层次式聚类</h3><ul>
<li>自底向上的凝聚式聚类(Agglomerative clustering)<ul>
<li>初始每个文档自成一个类簇</li>
<li>每次合并最相似/距离最近的两个类簇，形成一个新类簇，循环执行，直到满足终止条件<ul>
<li>终止条件为类簇数量或者相似度阈值</li>
</ul>
</li>
<li>结果为树形图</li>
</ul>
</li>
<li>不同的计算类簇间距离/相似性的方法<ul>
<li>最小距离(single link)</li>
<li>最大距离(complete link)</li>
<li>平均距离(group average)</li>
</ul>
</li>
<li>自顶向下的划分式聚类(Divisive clustering)<ul>
<li>初始只有一个类簇，包括所有文档</li>
<li>每次从当前类簇中选择最大(或最不合理)的一个类簇，将其分割成两个(或多个)新的类簇，循环执行，直到满足终止条件<ul>
<li>类簇分割方法可以采用其他聚类方法，比如 K 均值算法等</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="K-均值-K-means-聚类"><a href="#K-均值-K-means-聚类" class="headerlink" title="K 均值(K-means)聚类"></a>K 均值(K-means)聚类</h3><ul>
<li>一种基于划分的聚类算法</li>
<li>算法将文档集划分到 k 个类簇中<ul>
<li>每个类簇有一个类簇中心，称为 centroid</li>
<li>可基于该类簇文档向量的平均向量计算</li>
</ul>
</li>
<li>K 由用户指定</li>
</ul>
<h3 id="Buckshot-算法"><a href="#Buckshot-算法" class="headerlink" title="Buckshot 算法"></a>Buckshot 算法</h3><p>结合 K-means 与凝聚式聚类</p>
<ul>
<li>从原始 n 个数据点中随机取 √n 个数据点</li>
<li>针对这些样本数据点上运行凝聚式聚类</li>
<li>使用凝聚式聚类结果作为初始种子点</li>
<li>在元数据上基于初始种子点运行 K-means</li>
</ul>
<p>改善了种子点的选择</p>
<h3 id="增量式单遍聚类-Single-Pass"><a href="#增量式单遍聚类-Single-Pass" class="headerlink" title="增量式单遍聚类(Single-Pass)"></a>增量式单遍聚类(Single-Pass)</h3><ul>
<li>增量式聚类，适合于高效处理动态文本数据流</li>
<li>将新文档与已有类簇逐一对比，如果与某个类簇的相似度值大于设定的阈值，那么将该文档归并到相应类簇中，否则基于该文档形成一个新的类簇<ul>
<li>数据流中第一个文档形成第一个类簇</li>
</ul>
</li>
</ul>
<h3 id="基于图分割的聚类"><a href="#基于图分割的聚类" class="headerlink" title="基于图分割的聚类"></a>基于图分割的聚类</h3><ul>
<li>计算文档对之间的相似度值，构建相似度图 G=(V,E)<ul>
<li>每个文档为图的一个节点</li>
<li>文档之间有边连接，边的权重 W 为文档相似度</li>
</ul>
</li>
<li>文档聚类问题转换成在图上进行顶点分割的问题</li>
<li>切割标准 min cut(A,B)</li>
<li>问题<ul>
<li>只考虑了类簇之间的连接情况</li>
<li>没有考虑类簇内部的密度</li>
</ul>
</li>
<li>改进方法: Normalized Cut</li>
</ul>
<h3 id="基于密度峰值的聚类-Science"><a href="#基于密度峰值的聚类-Science" class="headerlink" title="基于密度峰值的聚类(Science)"></a>基于密度峰值的聚类(Science)</h3><ul>
<li>简单优美，可自动确定聚类数目，检测孤立点，可识别不同形状的聚类</li>
<li>假设: 类簇中心被一些局部密度较低的点围绕，并且这些点距离其他有高局部密度的点的距离都比较大</li>
<li>聚类过程<ul>
<li>计算出所有数据点的局部密度值和到高局部密度点的距离后，可以得到一张决策图</li>
<li>在决策图上挑选具有较大密度的样本点作为类簇中心</li>
<li>将其他样本点按照局部密度从高到底依次确定所属类簇，其类簇为领域内最近的高于该点局部密度的样本点所处的类簇</li>
</ul>
</li>
</ul>
<h3 id="半监督聚类"><a href="#半监督聚类" class="headerlink" title="半监督聚类"></a>半监督聚类</h3><p>除了带聚类的数据，还提供了少量先验知识</p>
<ul>
<li>部分标注信息<ul>
<li>例如为若干数据点标注了类簇</li>
</ul>
</li>
<li>约束信息<ul>
<li>例如要求某两个数据点必须在同一类簇，或者某两个数据点不能在同一类簇</li>
</ul>
</li>
<li>Seeded K-means<ul>
<li>用户提供了 Seeded points</li>
<li>利用提供的标注信息(seeded points)寻找初始类簇中心，然后运行 K-means</li>
<li>Seeded points 的标签可能会改变</li>
</ul>
</li>
<li>Constrained K-means<ul>
<li>用户提供了 Seeded points</li>
<li>利用提供的标注信息(seeded points)寻找初始类簇中心，然后运行 K-means</li>
<li>Seeded points 的标签不允许被改变</li>
</ul>
</li>
</ul>
<h3 id="聚类结果评估"><a href="#聚类结果评估" class="headerlink" title="聚类结果评估"></a>聚类结果评估</h3><ul>
<li>基于人工标注的类簇进行评价</li>
<li>评价准则包括: F 值、纯度(Purity)、规范化信息(NMI)</li>
</ul>
<h3 id="聚类算法的选择"><a href="#聚类算法的选择" class="headerlink" title="聚类算法的选择"></a>聚类算法的选择</h3><ul>
<li>聚类算法的选择具有挑战性<ul>
<li>每种算法都有各自的优势与不足，聚类效果通常依赖于聚类算法、距离函数、具体的数据与应用</li>
</ul>
</li>
<li>实际做法<ul>
<li>运行使用不同距离函数的多个聚类算法，分析和比较聚类结果</li>
</ul>
</li>
<li>对聚类结果的解释要基于原始数据的意义以及聚类算法的特性</li>
</ul>
<h3 id="半监督聚类-1"><a href="#半监督聚类-1" class="headerlink" title="半监督聚类"></a>半监督聚类</h3><ul>
<li>COP K-means<ul>
<li>用户提供了 must-link 与 cannot-link 约束</li>
<li>初始化: 类簇中心随机选择，但必须保证满足 must-link 约束，也就是 must-link 的两个数据点不能选择做为不同类簇的中心</li>
<li>算法: 一个数据点必须在不违反任何约束的情况下归属到临近的类簇</li>
</ul>
</li>
</ul>
<h3 id="检索结果聚类"><a href="#检索结果聚类" class="headerlink" title="检索结果聚类"></a>检索结果聚类</h3><ul>
<li>两类方法<ul>
<li>先对 snippet 进行聚类，然后为每个类簇选择标签</li>
<li>先分析获取有意义的类簇标签，然后将检索结果划分到不同标签</li>
</ul>
</li>
<li>基于后缀树的检索结果聚类(Suffix Tree Clustering, STC)<ul>
<li>时间复杂度低，高效</li>
<li>容易获得每个类簇的关键词标签</li>
<li>允许一个文档属于多个类簇</li>
</ul>
</li>
<li>基于回归学习的检索结果聚类<ul>
<li>现货的重要的短语代表类簇，然后再将检索结果跟类簇关联</li>
<li>同样允许一文档隶属多个类簇</li>
<li>候选短语(频率大于3的n-gram(n&lt;=3))</li>
<li>短语重要性排序<ul>
<li>看做回归问题</li>
<li>利用 SVM-Light 实现的支撑向量回归等多个回归方法</li>
<li>人工标注短语训练集，也即短语与其重要性分值</li>
</ul>
</li>
<li>短语特征<ul>
<li>Phrase Frequency/Inverted Document Frequency(TFIDF)</li>
<li>Phrase Length</li>
<li>Intra-Cluster Similarity</li>
<li>Cluster Entropy</li>
<li>Phrase Independence</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="智能问答基础技术"><a href="#智能问答基础技术" class="headerlink" title="智能问答基础技术"></a>智能问答基础技术</h2><p>起源于信息检索社区。根据用户的提问给出简短的答案，有时需要提供答案的证据。</p>
<h3 id="问题类型"><a href="#问题类型" class="headerlink" title="问题类型"></a>问题类型</h3><ul>
<li>根据答案类型划分<ul>
<li>事实型问题(Factual questions)</li>
<li>观点型问题(Opinions)</li>
<li>摘要型问题(Summaries)</li>
</ul>
</li>
<li>根据问题言语行为(question speech act)划分<ul>
<li>是否型问题(Yes/NO questions)</li>
<li>WH 问题(WH questions)</li>
<li>间接请求(Indirect Requests)</li>
<li>命令(Commands)</li>
</ul>
</li>
<li>复杂/困难问题<ul>
<li>为什么/怎么样(Why, How questions)</li>
<li>什么(What questions)</li>
</ul>
</li>
</ul>
<h3 id="主要技术"><a href="#主要技术" class="headerlink" title="主要技术"></a>主要技术</h3><ul>
<li>传统自动问答技术<ul>
<li>基于语料库的自动问答<ul>
<li>问题分析(分类、模板匹配、语义分析)</li>
<li>段落检测(段落抽取、排序)</li>
<li>答案抽取(实体识别、模板匹配、排序)</li>
</ul>
</li>
<li>基于知识库的自动问答</li>
</ul>
</li>
<li>社区问答技术<ul>
<li>问题分类、问题推荐</li>
<li>专家发现、信誉评估</li>
<li>知识抽取</li>
</ul>
</li>
</ul>
<h2 id="互联网信息抽取"><a href="#互联网信息抽取" class="headerlink" title="互联网信息抽取"></a>互联网信息抽取</h2><p><strong>非结构化数据</strong>(纯文本，句子，查询字符串) 和<strong>半结构化数据</strong>(HTML文档，查询日志，词典) -IE-&gt; <strong>结构化数据</strong>(语义知识)</p>
<ul>
<li>网页主要文本抽取(BTE)方法<ul>
<li>识别出一个包含最多词语(排除最多标签)的连续区域</li>
</ul>
</li>
<li>基于文本标签比例的抽取方法(CERT, Content Extraction via Tag Ratios)<ul>
<li>绘制文本标签直方图</li>
<li>对文本标签直方图进行变换，并进行聚类，得到网页内容</li>
</ul>
</li>
<li>基于网页分块重要性识别的抽取方法(Learning Block Importance Models for Web Pages)<ul>
<li>首先将网页分为块状区域(VIPS, 基于视觉的网页分块算法)</li>
<li>然后对每块区域判断重要性</li>
</ul>
</li>
<li>基于模板的方法<ul>
<li>基于批量处子统一模板的网页来自动检测模板</li>
<li>利用模板抽取网页内容</li>
</ul>
</li>
</ul>
<h3 id="文本语义信息抽取"><a href="#文本语义信息抽取" class="headerlink" title="文本语义信息抽取"></a>文本语义信息抽取</h3><p><strong>主要任务</strong></p>
<ul>
<li>命名实体抽取(Named entity extraction)<ul>
<li>命名实体识别(Named entity recognitoin, NER)</li>
<li>共指消解(Co-reference resolution)</li>
</ul>
</li>
<li>属性抽取(Attribute extraction)</li>
<li>关系挖掘(Relation mining)<ul>
<li>相关短语和实体(Related terms and entities)</li>
<li>归类(Categorization)</li>
<li>关系检测与分类(Relation detection and classification)</li>
</ul>
</li>
<li>事件挖掘(Event mining)<ul>
<li>事件检测与分类(Event detection and classification)</li>
</ul>
</li>
</ul>
<h3 id="实体识别"><a href="#实体识别" class="headerlink" title="实体识别"></a>实体识别</h3><ul>
<li>识别文本中出现的实体<ul>
<li>MUC(1997): Person, Location, Organization, Date/Time/Currency</li>
<li>ACE(2005): 100 多种更具体的类型</li>
</ul>
</li>
<li>人工规则 vs 机器学习方法</li>
<li>针对不同试题类型与领域考虑不同方法<ul>
<li>封闭类(e.g., geographical locations, disease names, gene &amp; protein names): 人工规则 + 词典</li>
<li>语法相关(e.g., phone numbers, zip codes): 正则表达式</li>
<li>语义相关(e.g., person and compnay names): 综合考虑上下文，句法特征，词典，启发式规则等</li>
<li>对于典型的实体类型抽取效果已经较好</li>
</ul>
</li>
</ul>
<h3 id="语义类挖掘"><a href="#语义类挖掘" class="headerlink" title="语义类挖掘"></a>语义类挖掘</h3><ul>
<li>目标：发现同位短语(coordinate terms), 类似于可比实体发现</li>
<li>主要方法<ul>
<li>一阶共现(First-order co-occurrences): 普通共现 &amp; 模板</li>
<li><strong>二阶共现(Second-order co-occurrences)</strong>: 分布式相似性<ul>
<li>分布式假设：出现在相似上下文(词语、句法)中的词语比较相似</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>基于分布相似性(DS)</strong></p>
<ul>
<li>定义上下文：句法上下文，词语上下文</li>
<li>将每个短语表示为一个特征向量<ul>
<li>特征：短语出现的一个上下文</li>
<li>特征值：上下文针对短语的权重</li>
</ul>
</li>
<li>计算短语相似性<ul>
<li>特征向量之间的相似性</li>
</ul>
</li>
</ul>
<p><strong>语义层级构建</strong></p>
<ul>
<li>主要子任务<ul>
<li>为短语(term)赋予类标签或上位词(label)<ul>
<li>Beijing -&gt; city, cpatial</li>
<li>Apple -&gt; company, fruit</li>
<li>Red -&gt; Color</li>
<li>方法: Pattern matching + counting<ul>
<li>人工设计或自动生成模板</li>
<li>文本模板或 HTML 表格</li>
<li>输出 <term, label,="" pattern,="" source,="" weight=""> 元组</term,></li>
</ul>
</li>
<li>挑战<ul>
<li>边界检测: 短语边界、标签边界</li>
<li>标签选择</li>
</ul>
</li>
<li>合并元组<ul>
<li>对于每个短语 T 与标签 L，计算 w(T, L)</li>
</ul>
</li>
<li>方法:简单计数<ul>
<li>对于每对(T, L)统计<t, l,="" p,="" s,="" w="">元组的数量</t,></li>
<li>或者 TF-IDF</li>
</ul>
</li>
</ul>
</li>
<li>为语义类(semantic class)赋予类标签(label)<ul>
<li>{Beijing, Shanghai, Dalian…} -&gt; cities, Chinese cities…</li>
<li>方法: 投票(Voting)</li>
</ul>
</li>
<li>构建层级</li>
</ul>
</li>
</ul>
<p><strong>通用关系</strong></p>
<p>关系: 与实体相关的事实</p>
<p>历史: MUC-7(1997) 提出，ACE、KBP 扩展</p>
<p><strong>关系抽取方法</strong></p>
<ul>
<li>基于知识/规则的方法<ul>
<li>专家制定规则、模板(可以基于词汇、句法结构)</li>
</ul>
</li>
<li>机器学习方法<ul>
<li>半监督学习：基于人工标注数据训练模型</li>
<li>半监督学习：基于自举方法从种子样例中训练模型<ul>
<li>自举方法(Bootstrapping): DIPRE(Brin 1998, 双重迭代模板关系抽取)</li>
</ul>
</li>
<li>混合或交互系统<ul>
<li>专家与机器学习算法交互(e.g., active learning)从而迭代优化/扩展规则和模板</li>
<li>交互包括标注样例、修改规则等</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>属性抽取(Attribute Extraction)</strong></p>
<ul>
<li>基于无结构化文本</li>
<li>基于 HTML 表格</li>
<li>基于 Wikipedia Infobox</li>
</ul>
<h3 id="相关系统"><a href="#相关系统" class="headerlink" title="相关系统"></a>相关系统</h3><ul>
<li>NeedleSeek<ul>
<li>needleseek.msra.cn</li>
<li>基于 Web 大规模数据挖掘开放域语义知识</li>
<li>基于挖掘的语义知识回答/服务用户需求</li>
<li>语义卡片(Semantic Card)<ul>
<li>将输入的词或段誉映射为一个或多个概念</li>
</ul>
</li>
<li>语义图(Semantic Map)<ul>
<li>概念组织成语义类，并获取概念之间的语义关系</li>
</ul>
</li>
</ul>
</li>
<li>人立方系统<ul>
<li>renlifang.msra.cn 中文</li>
<li>entitycube.research.microsoft.com 英文</li>
<li>侧重人物、机构之间的关系</li>
</ul>
</li>
<li>谷歌知识图谱<ul>
<li>实体关系网络</li>
</ul>
</li>
</ul>
<h2 id="情感分析与观点挖掘"><a href="#情感分析与观点挖掘" class="headerlink" title="情感分析与观点挖掘"></a>情感分析与观点挖掘</h2><ul>
<li>应用<ul>
<li>产品比较与推荐</li>
<li>个人与机构声誉分析</li>
<li>电视节目满意度分析</li>
<li>互联网舆情分析<ul>
<li>利用文本情感计算技术深入分析人们对社会现实和现象的群体性情绪、观点、思想、心理、意志和要求</li>
</ul>
</li>
<li>反恐与维稳</li>
</ul>
</li>
<li>原型系统与产品<ul>
<li>英文: OpinionFinder, RapidMiner, TextMap, Bing 产品搜索, condersr.com, tweetfeel.com</li>
<li>中文: 爱搜车众评, 雅虎人物搜索</li>
</ul>
</li>
</ul>
<h3 id="研究框架"><a href="#研究框架" class="headerlink" title="研究框架"></a>研究框架</h3><ul>
<li>应用层：情感检索，情感摘要，情感问答</li>
<li>核心层：情感要素抽取，情感倾向性分析，主客观分析/观点文本识别</li>
<li>基础层：NLP 基本模块，情感资源收集与标注</li>
<li>来源：产品评论，电影评论，新闻评论，博客，微博</li>
</ul>
<h3 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h3><ul>
<li>将文本按照所表达的总体情感进行分类<ul>
<li>例如正面(Positive), 负面(Negative), 中性(neutral)</li>
</ul>
</li>
<li>基于话题的文本分类相似又不同<ul>
<li>话题词汇很重要</li>
<li>情感词汇更重要，如: great, excellent, horrible, bad, worst, …</li>
</ul>
</li>
</ul>
<p><strong>情感分析任务</strong></p>
<ul>
<li>主客观分析/观点文本识别<ul>
<li>客观：反映关于世界的事实信息</li>
<li>主观：反映个人情感、信念等</li>
</ul>
</li>
<li>倾向性分析(可看作主客观分析的细粒度处理)<ul>
<li>对包含观点的文本进行倾向性判断</li>
<li>一般分为三类：褒义、贬义、中性(在一些问题不考虑中性)</li>
</ul>
</li>
<li>情绪分析<ul>
<li>愤怒、高兴、喜好、悲哀、吃惊等等</li>
</ul>
</li>
<li>粒度<ul>
<li>词、句子、文档</li>
</ul>
</li>
</ul>
<p><strong>情感资源</strong></p>
<ul>
<li>情感分析的基础</li>
<li>英文资源较多<ul>
<li>情感词典: WentiWordNet, Inquirer 等<ul>
<li>包含词语、短语等</li>
<li>倾向性词语、主观性词语</li>
</ul>
</li>
<li>已标注语料库数量较多</li>
<li>开源情感分析工具 OpinionFinder</li>
</ul>
</li>
<li>中文资源较少，但逐年增多<ul>
<li>知网 Hownet 提供了部分情感词汇</li>
<li>近两年的评测提供了中文标注文本<ul>
<li>NTCIR, COAE, NLP&amp;CC 等</li>
</ul>
</li>
</ul>
</li>
<li>情感资源基本上跟领域、语言有关</li>
<li>主客观分析与倾向性分析的资源也不一样</li>
</ul>
<h3 id="情感词汇资源构建"><a href="#情感词汇资源构建" class="headerlink" title="情感词汇资源构建"></a>情感词汇资源构建</h3><ul>
<li>基于心理学的评价理论</li>
<li>任务<ul>
<li>确定词语的主观性(subjectivity)</li>
<li>确定词语的倾向(orientation)</li>
<li>确定词语态度的强度(strength)</li>
</ul>
</li>
<li>连接词方法(Conjunction Method)<ul>
<li>用 and 相连的形容词通常具有相同的倾向，而用 but 的则具有相反的倾向</li>
<li>对形容词按照不同倾向聚类</li>
</ul>
</li>
<li>PMI 方法(Pointwise Mutual Information 点互信息, Orientation, Subjectivity)<ul>
<li>判别倾向性：具有相似倾向性的词语倾向于在文档中共同出现</li>
<li>判别主观性：主观性形容词倾向于出现在其他主观性形容词周围</li>
<li>可基于 AltaVista 搜索引擎的 NEAR 操作符返回的结果数进行 PMI 的计算</li>
<li>预测词语的倾向性 SO(t)</li>
</ul>
</li>
<li>WordNet 扩展方法<ul>
<li>著名的英文词义关系计算资源，词义数据库，包含词义及其关系</li>
<li>wordnet.princeton.edu</li>
<li>WordNet 中基本单元为Synset(synonym set): (近似)同义集合<ul>
<li>WordNet 的基本单元</li>
<li>每个 synset 表示一个语义概念</li>
<li>Example synset: {hit, strike, impinge on, run into, collide with}</li>
</ul>
</li>
<li>每个词条包括多个 synsets, 注释，使用样例等信息</li>
<li>Synsets 通过不同的词义关系相连</li>
<li>使用词语之间的同义、反义关系</li>
<li>假设：形容词通常与其同义词具有相同的倾向性，而与反义词具有相反的倾向性</li>
<li>利用种子形容词集，能够获得 WordNet 中所有形容词的倾向性</li>
</ul>
</li>
<li>释义方法(Gloss Use Method)<ul>
<li>Orientation, Subjectivity, SentiWordNet</li>
<li>假设<ul>
<li>倾向性：具有相似倾向性的词语具有相似的释义</li>
<li>主观性：具有相似倾向性的词语具有相似的释义，不具有倾向性的词语具有无倾向的释义</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="观点抽取"><a href="#观点抽取" class="headerlink" title="观点抽取"></a>观点抽取</h3><p>一个观点表示为一个五元组(目标对象, 目标对象特征, 观点的情感值, 观点持有者, 观点表达时间)</p>
<p>观点抽取任务很困难，<strong>重点关注两个子任务</strong></p>
<ul>
<li>特征抽取与聚类(aspect extraction and grouping)<ul>
<li>抽取对象的所有特征表达，并将同义特征表达聚类。每个特征类表示了关于该对象的独一无二的某个特征</li>
</ul>
</li>
<li>特征情感分类(aspect sentiment classification)<ul>
<li>确定观点针对每个特征的情感倾向：正面、负面、中性</li>
</ul>
</li>
</ul>
<p><strong>对象特征抽取</strong></p>
<ul>
<li>频繁特征：被许多评论提及的特征</li>
<li>非频繁特征抽取：基于同一情感词被用来描述不同特征与对象</li>
</ul>
<h2 id="信息摘要"><a href="#信息摘要" class="headerlink" title="信息摘要"></a>信息摘要</h2><p>对海量数据内容进行<strong>提炼与总结</strong>，以<strong>简洁、直观</strong>的摘要来概括用户所关注的主要内容，方便用户快速了解与浏览海量内容</p>
<h3 id="文档摘要基本技术"><a href="#文档摘要基本技术" class="headerlink" title="文档摘要基本技术"></a>文档摘要基本技术</h3><ul>
<li>早期论文: Luhn. The Automatic Creation of Literature Abstracts(1958)</li>
<li>研究 50 多年，有一定进展，但仍不能令人满意</li>
<li>代表系统<ul>
<li><strong>NewsInEssence</strong> by University of Michigan</li>
<li><strong>NewsBlaster</strong> by Columnbia University</li>
</ul>
</li>
<li>摘要任务多样化：单文档摘要，多文档摘要，查询相关的多文档摘要</li>
<li>摘要方法分类<ul>
<li><strong>抽取式</strong><ul>
<li>从文档中抽取已有句子形成摘要</li>
<li>实现简单，能保证句子的可读性</li>
</ul>
</li>
<li><strong>生成式/混合式</strong><ul>
<li>生成新的句子，或者对已有句子进行压缩、重构与融合</li>
<li>难度更大，但更接近摘要的本质</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="抽取式文档摘要"><a href="#抽取式文档摘要" class="headerlink" title="抽取式文档摘要"></a>抽取式文档摘要</h3><p><strong>典型框架</strong></p>
<p>文档集 -&gt; 文档理解 -&gt; <strong>句子重要性计算与排名(利用词语句子的各类特征，基于机器学习)</strong> -&gt; 句子选择 -&gt; 摘要句子排序 -&gt; 摘要</p>
<p><strong>关键问题</strong></p>
<p>如何衡量句子的重要性</p>
<ul>
<li>影响句子重要性的因素<ul>
<li>句子长度</li>
<li>句子位置</li>
<li>句子中词语的 TFIDF</li>
<li>句子是否包括线索词</li>
<li>句子是否与标题相似</li>
</ul>
</li>
</ul>
<p><strong>不同方法</strong></p>
<ul>
<li>基于单一因素的摘要方法<ul>
<li>只考虑句子位置</li>
<li>Lead Baseline<ul>
<li>抽取一篇文档中前几句话形成摘要</li>
</ul>
</li>
<li>Coverage Baseline<ul>
<li>轮流从不同文档中抽取第一、第二…第 K 句话形成摘要</li>
</ul>
</li>
</ul>
</li>
<li>基于启发式规则<ul>
<li>基于经验性公式综合考虑少数几个因素</li>
<li>例如: centroid-based method, 考虑了句子包含词语权重、句子位置、句子与首句相似度</li>
</ul>
</li>
<li>基于有监督学习的方法<ul>
<li>可考虑众多因素，由机器学习算法确定最优组合</li>
<li>句子分类<ul>
<li>二类分类：句子是否隶属于摘要</li>
<li>SVM</li>
</ul>
</li>
<li>序列标注<ul>
<li>为每个句子打上标签</li>
<li>可考虑相邻句子之间的关系</li>
<li>HMM, CRF</li>
</ul>
</li>
<li>句子回归<ul>
<li>为每个句子预测一个反应重要性的分数</li>
<li>SVR</li>
</ul>
</li>
</ul>
</li>
<li>基于图排序的方法<ul>
<li>如 LexRank, TextRank</li>
<li>只依赖于句子相似度</li>
<li>基于 PageRank 算法或相似算法</li>
<li>步骤<ul>
<li>构建图 G = (V, E)，句子作为定点，句子之间有关系则构成边</li>
<li>应用 PageRank 算法或相似算法获得每个顶点的权重</li>
<li>基于句子权重选择句子形成摘要</li>
</ul>
</li>
</ul>
</li>
<li>摘要冗余去除<ul>
<li>多文档摘要中不同文档中的句子可能有重复内容</li>
<li>摘要很短，应该尽可能包括多样化内容</li>
<li>选择与摘要中已有句子冗余度小的句子</li>
<li>MMR 方法</li>
</ul>
</li>
<li>基于整数线性规划(ILP)的方法<ul>
<li>将摘要看做一个带约束的优化问题</li>
<li>基于 ILP 进行求解，可采用现成的 ILP 求解工具<ul>
<li>比如 IBM CPLEX Optimizer</li>
</ul>
</li>
<li>同时进行句子抽取与冗余去除</li>
</ul>
</li>
<li>摘要句子排序<ul>
<li>句子顺序直接影响摘要可读性</li>
<li>单文档摘要中句子顺序容易确定<ul>
<li>依据句子在原文档中的顺序即可</li>
</ul>
</li>
<li>多文档摘要中句子顺序较难确定<ul>
<li>可综合考虑句子所在上下文信息进行排序</li>
<li>先确定任何两句之间的先后顺序</li>
<li>再确定多个句子之间的整体顺序</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>目前摘要总体性能不高，需要方法上的突破。</p>
<h3 id="相关评测"><a href="#相关评测" class="headerlink" title="相关评测"></a>相关评测</h3><ul>
<li>DUC<ul>
<li>NIST 组织, 2000-2007</li>
<li>主要任务(传统)：单文档、多文档、查询相关摘要</li>
</ul>
</li>
<li>TAC Summarization Track<ul>
<li>NIST 组织, 2008-2011, 2014</li>
<li>主要任务(新型)<ul>
<li>Update Summarization</li>
<li>Opinion Summarization</li>
<li>Guided Summarization</li>
<li>AESOP(摘要自动评价)</li>
<li>Biomedical Summarization</li>
</ul>
</li>
</ul>
</li>
<li>NTCIR-9~10: 1click(新型)</li>
<li>TREC2013: Temporal Summarization Track(新型)</li>
</ul>
<h2 id="社交网络分析"><a href="#社交网络分析" class="headerlink" title="社交网络分析"></a>社交网络分析</h2><ul>
<li>社会媒体的特性<ul>
<li>用户生成内容多，富含观点</li>
<li>群体智慧</li>
<li>用户交互性强</li>
<li>异构网络</li>
</ul>
</li>
<li>社交媒体分析<ul>
<li>社交网络分析<ul>
<li>基于社交关系、结构进行挖掘</li>
<li>例如：社区检测、连接预测、影响力分析</li>
</ul>
</li>
<li>社交内容挖掘<ul>
<li>基于文本等内容数据进行挖掘</li>
<li>例如：摘要、关键词、情感分析</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>社交网络可以用矩阵表示</p>
<h3 id="社会网络"><a href="#社会网络" class="headerlink" title="社会网络"></a>社会网络</h3><ul>
<li>由相互关联的节点(个体或机构)组成的结构<ul>
<li>不同关系：例如好友关系、亲属关系等</li>
</ul>
</li>
<li>图表示<ul>
<li>节点 = 成员</li>
<li>边 = 关系</li>
</ul>
</li>
<li>现实例子<ul>
<li>好友网络(facebook, renren, wechat)</li>
<li>媒体分享(Flickr, Youtube)</li>
<li>社会标注(Del.icio.us)</li>
</ul>
</li>
<li>相关任务<ul>
<li>社交网络抽取(Social Network Extraction)<ul>
<li>从数据源中抽取、构建社交网络</li>
</ul>
</li>
<li><strong>网络中心性分析(Network Centrality Analysis)</strong><ul>
<li>识别社交网络上最重要的节点(重要性的定义由目的、环境所定)</li>
<li>输入：一个社交网络</li>
<li>输出：最重要的节点列表</li>
<li>方法：为节点计算分数或排序，反映节点的重要性/专业性/影响力</li>
<li>课采用链接分析领域的不同算法<ul>
<li>PageRank 算法及其变种</li>
<li>HITS 算法确定权威源</li>
</ul>
</li>
<li>可以采用<strong>网络中心性测度(Centrality measures)</strong>来评估节点重要性</li>
<li>代表性测度<ul>
<li>度数中心性 Degree Centrality -&gt; 入度中心性(indegree centrality, 有向图上的拓展)<ul>
<li>有最多朋友的人最重要</li>
</ul>
</li>
<li>中介中心性 Betweenness Centrality<ul>
<li>有多少对节点为了以最短路径到达彼此必须经过给定节点？</li>
</ul>
</li>
<li>亲近中心性 Closeness Centrality<ul>
<li>一个节点的亲近中心性基于该节点与图中所有节点的平均最短路径计算得到</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>社区检测(Community Detection)</strong><ul>
<li>一个社区由一个节点结合所表示，该集合中节点之间交互频繁</li>
<li>多种方法<ul>
<li>基于子图可达性(K-clique, K-club)</li>
<li>基于节点聚类<ul>
<li>节点相似性基于它们的交互模式的相似性而定义</li>
<li>两个节点结构相似(structurally equivalent)，如果它们连接到相同的其他节点</li>
<li>实际上使用向量相似性(cosine similary, Jaccard similarity, …)</li>
<li>应用 K-means Clustering Algorithm</li>
</ul>
</li>
<li>基于图分割<ul>
<li>不同社区之间的交互应该不频繁</li>
<li>图分割 Cut: 两个节点集之间的边的数量</li>
<li>目标: 最小化 Cut<ul>
<li>不足：经常获得包含一个节点的社区</li>
<li>需要考虑社区大小</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>分类(Classification)<ul>
<li>用户的行为倾向可表达为类别标签<ul>
<li>是否点击了一个广告</li>
<li>是否对特定的话题感兴趣</li>
<li>喜欢/不喜欢一个产品</li>
</ul>
</li>
<li>输入<ul>
<li>一个社交网络</li>
<li>部分节点的类别标签</li>
</ul>
</li>
<li>输出<ul>
<li>网络中其他节点的类别标签</li>
</ul>
</li>
</ul>
</li>
<li>链接预测(Link Prediction)<ul>
<li>给定一个社交网络，预测哪些节点相互连接</li>
<li>输出一个节点对列表</li>
<li>例如: facebook 中的好友推荐</li>
</ul>
</li>
<li>病毒式营销(Viral Marketing)<ul>
<li>找出若干用户，为其提供优惠或折扣，从而影响网络上的其他用户，使得收益最大化</li>
<li>找到能够覆盖网络的最小节点集合(可用贪心选择)</li>
</ul>
</li>
<li>网络建模(Network Modeling)<ul>
<li>发现网络的统计模式<ul>
<li>Small-world effect(e.g., 6 degrees of separation)</li>
<li>Power-law distribution(a.k.a. scale-free distribution)</li>
<li>Community structure(high clustering coefficient)</li>
</ul>
</li>
<li>对网络动力学进行建模</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="异构网络"><a href="#异构网络" class="headerlink" title="异构网络"></a>异构网络</h3><ul>
<li>网络中具有不同类型/模态的节点对象<ul>
<li>YouTube: Users, tags, videos, ads</li>
<li>Del.icio.us: Users, tags, bookmarks</li>
</ul>
</li>
<li>网络中节点之间具有不同类型/维度的交互<ul>
<li>Facebook: Send email, leave a message, write a comment, tag photos</li>
<li>Same users interacting at different sites: Facebook, YouTube, Twitter</li>
</ul>
</li>
<li>多模网络 Multi-Mode Network<ul>
<li>网络中包含多模态对象</li>
</ul>
</li>
<li>多维网络 Multi-Dimensional Network<ul>
<li>网络中包含节点间的异构链接</li>
</ul>
</li>
</ul>
<p>异构网络在现实中非常普遍，对异构网络的分析与挖掘更具挑战性，是当前数据挖掘领域研究的热点</p>
<h2 id="信息推荐"><a href="#信息推荐" class="headerlink" title="信息推荐"></a>信息推荐</h2><h3 id="推荐技术"><a href="#推荐技术" class="headerlink" title="推荐技术"></a>推荐技术</h3><ul>
<li>基于内容的过滤/推荐(Content-based Filtering/Recommendation)<ul>
<li>基于用户的历史推荐物品</li>
<li>基于内容特征描述来预测用户的偏好</li>
</ul>
</li>
<li>协同过滤/推荐(Collaborative Filtering/Recommendation)<ul>
<li>基于用户群体行为</li>
<li>基于其他具有相似行为的用户为给定用户推荐物品</li>
</ul>
</li>
<li>混合式</li>
</ul>
<h3 id="基于内容的推荐"><a href="#基于内容的推荐" class="headerlink" title="基于内容的推荐"></a>基于内容的推荐</h3><ul>
<li>主要思想：为一个用户推荐与该用户之前感兴趣的物品相似的物品<ul>
<li>电影推荐：推荐具有相同演员、导演、类型的电影</li>
<li>新闻推荐：推荐具有相似话题的新闻</li>
</ul>
</li>
<li>用户配置档案(User Profiling)是关键<ul>
<li>用户画像</li>
<li>基于用户的历史推荐物品</li>
<li>从样例内容特征描述中基于机器学习与数据挖掘算法获得关于用户兴趣爱好的配置档案</li>
</ul>
</li>
<li>物品表示(Item Representation)<ul>
<li>物品一般存储在数据库表中</li>
<li>结构化数据<ul>
<li>一定数量的属性</li>
<li>每个物品由同样的属性集所描述</li>
<li>属性的取值范围一般明确</li>
</ul>
</li>
<li>非结构化数据<ul>
<li>自由文本，可转为结构化表达</li>
<li>每个词可看做一个属性</li>
</ul>
</li>
</ul>
</li>
<li>用户配置档案(User Profile)<ul>
<li>包含两类信息<ul>
<li>用户兴趣模型，例如，一个能预测用户对某个物品感兴趣可能性的函数表达(可以机器学习建模)</li>
<li>用户交互历史，例如，用户浏览的物品，用户购买的物品等</li>
</ul>
</li>
<li>用户交互历史可用作训练数据，基于该训练数据采用机器学习算法创建用户兴趣模型</li>
<li>可由用户人工创建<ul>
<li>提供选项界面供用户构建</li>
<li>简单数据库查询课找到满足用户条件的物品并推荐给用户</li>
<li>不足<ul>
<li>耗费用户精力</li>
<li>不能及时反映兴趣爱好的变化</li>
<li>不能对推荐物品进行排序</li>
</ul>
</li>
</ul>
</li>
<li>基于机器学习的用户建模<ul>
<li>基于分类学习从用户历史中对用户兴趣建模<ul>
<li>应用分类函数能够获得用户对某物品感兴趣的概率</li>
</ul>
</li>
<li>多种分类学习算法可以应用<ul>
<li>决策树：构建一颗决策树表示用户兴趣爱好</li>
<li>最近邻：将所有训练样例存储于内存中，为新的物品找到 K 个最近邻的物品，从用户对这些最近邻物品的偏好计算用户对新物品的偏好</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="协同推荐"><a href="#协同推荐" class="headerlink" title="协同推荐"></a>协同推荐</h3><ul>
<li>基于用户群体</li>
<li>方法<ul>
<li>对于给定的用户，找到相似的用户</li>
<li>为给定用户推荐被相似用户高度评分的物品(未被给定用户评分)</li>
</ul>
</li>
<li>优势<ul>
<li>不需要分析内容</li>
<li>能够捕捉微妙的线索、关系</li>
</ul>
</li>
<li>三类主要的协同推荐<ul>
<li>基于用户<ul>
<li>思想：在过去对物品购买、评分一致的用户很可能再次一致</li>
<li>使用相似用户的意见预测特定用户对于一个物品的意见</li>
<li>用户相似性通过用户对其他物品的意见吻合程度来衡量<ul>
<li>算法1：使用整个矩阵</li>
<li>算法2：K 近邻</li>
</ul>
</li>
<li>用户冷启动(cold-start)问题<ul>
<li>不足以确定新用户的相似用户</li>
</ul>
</li>
<li>数据稀疏(sparsity)问题<ul>
<li>当物品数量很多时，用户通常只评价了极少一部分物品，同样难以找到相似用户</li>
</ul>
</li>
<li>扩展性问题<ul>
<li>当有百万用户和物品时，计算很慢</li>
</ul>
</li>
<li>物品冷启动问题<ul>
<li>不能为新物品预测用户评分，除非已有相似用户对该物品进行评分(注：基于内容的推荐中不存在该问题)</li>
</ul>
</li>
</ul>
</li>
<li>基于物品<ul>
<li>一个用户很可能对相似的物品具有相同的评分(类似基于内容的推荐)</li>
<li>物品相似性通过其他用户对物品的评分意见吻合程度来衡量(与基于内容的推荐不同)</li>
<li>相比于基于用户协同推荐的优势<ul>
<li>更好地处理用户冷启动问题</li>
<li>提高稳定性(物品相似性要比用户相似性更稳定)</li>
</ul>
</li>
</ul>
</li>
<li>基于矩阵分解<ul>
<li>将评分矩阵分解为两个矩阵(也可以是多个)，基于分解结果课计算获得用户对物品的评分(该评分在原始矩阵中不存在)</li>
<li>求解方法<ul>
<li>Alternating least squares</li>
<li>Stochastic gradient descent</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>需要一个评分矩阵<ul>
<li>矩阵每个元素表示一个用户针对一个物品的评分</li>
</ul>
</li>
<li>显式评分：用户评分，是否购买等</li>
<li>隐式评分：用户点击，页面浏览，浏览时间等</li>
</ul>
<h3 id="推荐结果的评估准则"><a href="#推荐结果的评估准则" class="headerlink" title="推荐结果的评估准则"></a>推荐结果的评估准则</h3><ul>
<li>根据不同的推荐问题和具体需求而选择<ul>
<li>Precision, Recall, MRR, Mean absolute error</li>
</ul>
</li>
</ul>
<h3 id="更多推荐技术"><a href="#更多推荐技术" class="headerlink" title="更多推荐技术"></a>更多推荐技术</h3><ul>
<li>基于社交网络的推荐</li>
<li>基于地理位置的推荐</li>
<li>多模态内容综合推荐</li>
<li>集合推荐/组合推荐</li>
</ul>
<h3 id="主要的开源工具"><a href="#主要的开源工具" class="headerlink" title="主要的开源工具"></a>主要的开源工具</h3><table>
<thead>
<tr>
<th style="text-align:left">Name</th>
<th style="text-align:right">Language</th>
<th style="text-align:right">Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">CofiRank</td>
<td style="text-align:right">C++</td>
<td style="text-align:right">collaborative filtering</td>
</tr>
<tr>
<td style="text-align:left">Crab</td>
<td style="text-align:right">Python</td>
<td style="text-align:right">recommendation</td>
</tr>
<tr>
<td style="text-align:left">EasyRec</td>
<td style="text-align:right">Java</td>
<td style="text-align:right">recommendation</td>
</tr>
<tr>
<td style="text-align:left">GraphLab</td>
<td style="text-align:right">C++</td>
<td style="text-align:right">high performance computation</td>
</tr>
<tr>
<td style="text-align:left">Lenskit</td>
<td style="text-align:right">Java</td>
<td style="text-align:right">recommendation</td>
</tr>
<tr>
<td style="text-align:left">Mahout</td>
<td style="text-align:right">Java</td>
<td style="text-align:right">general ML</td>
</tr>
<tr>
<td style="text-align:left">Python-recsys</td>
<td style="text-align:right">Python</td>
<td style="text-align:right">recommendation</td>
</tr>
<tr>
<td style="text-align:left">RapidMiner</td>
<td style="text-align:right">Java</td>
<td style="text-align:right">ML, NLP &amp; data mining</td>
</tr>
<tr>
<td style="text-align:left">SVDFeature</td>
<td style="text-align:right">C++</td>
<td style="text-align:right">matrix factorization</td>
</tr>
<tr>
<td style="text-align:left">Waffles</td>
<td style="text-align:right">C++</td>
<td style="text-align:right">ML and data mining</td>
</tr>
</tbody>
</table>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是我学习数据分析时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="笔记" scheme="http://wdxtub.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="数据分析" scheme="http://wdxtub.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>机器学习指南</title>
    <link href="http://wdxtub.com/2016/09/10/machine-learning-guide/"/>
    <id>http://wdxtub.com/2016/09/10/machine-learning-guide/</id>
    <published>2016-09-10T01:05:16.000Z</published>
    <updated>2016-09-10T15:53:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是我学习机器学习时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。</p>
<a id="more"></a>
<hr>
<h2 id="数学基础知识"><a href="#数学基础知识" class="headerlink" title="数学基础知识"></a>数学基础知识</h2><h3 id="线性代数-Linear-Algebra-："><a href="#线性代数-Linear-Algebra-：" class="headerlink" title="线性代数 (Linear Algebra)："></a>线性代数 (Linear Algebra)：</h3><p>这门学科对于Learning是必备的基础，对它的透彻掌握是必不可少的。</p>
<p><strong>Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang</strong></p>
<p>这本书是MIT的线性代数课使用的教材，也是被很多其它大学选用的经典教材。它的难度适中，讲解清晰，重要的是对许多核心的概念讨论得比较透彻。我个人觉得，学习线性代数，最重要的不是去熟练矩阵运算和解方程的方法——这些在实际工作中MATLAB可以代劳，关键的是要深入理解几个基础而又重要的概念：子空间(Subspace)，正交(Orthogonality)，特征值和特征向量(Eigenvalues and eigenvectors)，和线性变换(Linear transform)。从我的角度看来，一本线代教科书的质量，就在于它能否给这些根本概念以足够的重视，能否把它们的联系讲清楚。Strang的这本书在这方面是做得很好的。</p>
<p>而且，这本书有个得天独厚的优势。书的作者长期在MIT讲授线性代数课(18.06)，<a href="http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/CourseHome/index.htm" target="_blank" rel="external">课程的video</a>在MIT的Open courseware网站上有提供。有时间的朋友可以一边看着名师授课的录像，一边对照课本学习或者复习。</p>
<h3 id="概率和统计-Probability-and-Statistics"><a href="#概率和统计-Probability-and-Statistics" class="headerlink" title="概率和统计 (Probability and Statistics)"></a>概率和统计 (Probability and Statistics)</h3><p>Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。这代表了Machine Learning中最主流的两大类方法的基础。一种是以研究函数和变换为重点的代数方法，比如Dimension reduction，feature extraction，Kernel等，一种是以研究统计模型和样本分布为重点的统计方法，比如Graphical model, Information theoretical models等。它们侧重虽有不同，但是常常是共同使用的，对于代数方法，往往需要统计上的解释，对于统计模型，其具体计算则需要代数的帮助。以代数和统计为出发点，继续往深处走，我们会发现需要更多的数学。</p>
<p><strong>Applied Multivariate Statistical Analysis_(5th Ed.)  by Richard A. Johnson and Dean W. Wichern</strong></p>
<p>这本书是我在刚接触向量统计的时候用于学习的，我在香港时做研究的基础就是从此打下了。实验室的一些同学也借用这本书学习向量统计。这本书没有特别追求数学上的深度，而是以通俗易懂的方式讲述主要的基本概念，读起来很舒服，内容也很实用。对于Linear regression, factor analysis, principal component analysis (PCA), and canonical component analysis (CCA)这些Learning中的基本方法也展开了初步的论述。</p>
<p>之后就可以进一步深入学习贝叶斯统计和Graphical models。一本理想的书是</p>
<p><strong>Introduction to Graphical Models (draft version).  by M. Jordan and C.Bishop.</strong></p>
<p>这本书从基本的贝叶斯统计模型出发一直深入到复杂的统计网络的估计和推断，深入浅出，statistical learning的许多重要方面都在此书有清楚论述和详细讲解。</p>
<p>Graph Theory（图论)，图，由于它在表述各种关系的强大能力以及优雅的理论，高效的算法，越来越受到Learning领域的欢迎。经典图论，在Learning中的一个最重要应用就是graphical models了，它被成功运用于分析统计网络的结构和规划统计推断的流程。Graphical model所取得的成功，图论可谓功不可没。在Vision里面，maxflow(graphcut)算法在图像分割，Stereo还有各种能量优化中也广受应用。另外一个重要的图论分支就是Algebraic graph theory (代数图论)，主要运用于图的谱分析，著名的应用包括Normalized Cut和Spectral Clustering。近年来在semi-supervised learning中受到特别关注。</p>
<h3 id="分析-Analysis"><a href="#分析-Analysis" class="headerlink" title="分析 (Analysis)"></a>分析 (Analysis)</h3><p>微积分或者数学分析是很多学科的基础，值得推荐的教科书莫过于</p>
<p><strong>Principles of Mathematical Analysis, by Walter Rudin</strong></p>
<p>有点老，但是绝对经典，深入透彻。缺点就是比较艰深——这是Rudin的书的一贯风格，适合于有一定基础后回头去看。</p>
<p>Learning研究的大部分问题是在连续的度量空间进行的，无论代数还是统计，在研究优化问题的时候，对一个映射的微分或者梯度的分析总是不可避免。而在统计学中，Marginalization和积分更是密不可分——不过，以解析形式把积分导出来的情况则不多见。</p>
<p>在分析这个方向，接下来就是泛函分析(Functional Analysis)。</p>
<p><strong>Introductory Functional Analysis with Applications, by Erwin Kreyszig.</strong></p>
<p>适合作为泛函的基础教材，容易切入而不失全面。在分析这个方向，还有一个重要的学科是测度理论(Measure theory)，但是我看过的书里面目前还没有感觉有特别值得介绍的。</p>
<p>Measure Theory(测度理论)，这是和实分析关系非常密切的学科。但是测度理论并不限于此。从某种意义上说，Real Analysis可以从Lebesgue Measure（勒贝格测度）推演，不过其实还有很多别的测度体系——概率本身就是一种测度。测度理论对于Learning的意义是根本的，现代统计学整个就是建立在测度理论的基础之上——虽然初级的概率论教科书一般不这样引入。在看一些统计方面的文章的时候，你可能会发现，它们会把统计的公式改用测度来表达，这样做有两个好处：所有的推导和结论不用分别给连续分布和离散分布各自写一遍了，这两种东西都可以用同一的测度形式表达：连续分布的积分基于Lebesgue测度，离散分布的求和基于计数测度，而且还能推广到那种既不连续又不离散的分布中去（这种东西不是数学家的游戏，而是已经在实用的东西，在Dirchlet Process或者Pitman-Yor Process里面会经常看到)。而且，即使是连续积分，如果不是在欧氏空间进行，而是在更一般的拓扑空间（比如微分流形或者变换群），那么传统的黎曼积分（就是大学一年级在微积分课学的那种）就不work了，你可能需要它们的一些推广，比如Haar Measure或者 Lebesgue-Stieltjes积分。</p>
<h3 id="拓扑-Topology"><a href="#拓扑-Topology" class="headerlink" title="拓扑 (Topology)"></a>拓扑 (Topology)</h3><p>Topology（拓扑学)，这是学术中很基础的学科。它一般不直接提供方法，但是它的很多概念和定理是其它数学分支的基石。看很多别的数学的时候，你会经常接触这样一些概念：Open set / Closed set，set basis，Hausdauf, continuous function，metric space, Cauchy sequence, neighborhood, compactness, connectivity。</p>
<p><strong>Topology (2nd Ed.)  by James Munkres</strong></p>
<p>这本书是Munkres教授长期执教MIT拓扑课的心血所凝。对于一般拓扑学(General topology)有全面介绍，而对于代数拓扑(Algebraic topology)也有适度的探讨。此书不需要特别的数学知识就可以开始学习，由浅入深，从最基本的集合论概念（很多书不屑讲这个）到 Nagata-Smirnov Theorem和Tychonoff theorem等较深的定理（很多书避开了这个）都覆盖了。讲述方式思想性很强，对于很多定理，除了给出证明过程和引导你思考其背后的原理脉络，很多令人赞叹的亮点——我常读得忘却饥饿，不愿释手。很多习题很有水平。</p>
<p>当然，我们研究learning也许不需要深究这些数学概念背后的公理体系，但是，打破原来定义的概念的局限在很多问题上是必须的——尤其是当你研究的东西它不是在欧氏空间里面的时候——正交矩阵，变换群，流形，概率分布的空间，都属于此。</p>
<h3 id="流形理论-Manifold-theory"><a href="#流形理论-Manifold-theory" class="headerlink" title="流形理论 (Manifold theory)"></a>流形理论 (Manifold theory)</h3><p>对于拓扑和分析有一定把握时，方可开始学习流形理论，否则所学只能流于浮浅。我所使用的书是</p>
<p><strong>Introduction to Smooth Manifolds.  by John M. Lee</strong></p>
<p>虽然书名有introduction这个单词，但是实际上此书涉入很深，除了讲授了基本的manifold, tangent space, bundle, sub-manifold等，还探讨了诸如纲理论(Category theory)，德拉姆上同调(De Rham cohomology)和积分流形等一些比较高级的专题。对于李群和李代数也有相当多的讨论。行文通俗而又不失严谨，不过对某些记号方式需要熟悉一下。</p>
<p>Differential Manifold (微分流形)，通俗地说它研究的是平滑的曲面。一个直接的印象是它是不是可以用来fitting一个surface什么的——当然这算是一种应用，但是这是非常初步的。本质上说，微分流形研究的是平滑的拓扑结构。一个空间构成微分流形的基本要素是局部平滑：从拓扑学来理解，就是它的任意局部都同胚于欧氏空间，从解析的角度来看，就是相容的局部坐标系统。当然，在全局上，它不要求和欧氏空间同胚。它除了可以用于刻画集合上的平滑曲面外，更重要的意义在于，它可以用于研究很多重要的集合。一个n-维线性空间的全部k-维子空间</p>
<p>虽然李群论是建基于平滑流形的概念之上，不过，也可能从矩阵出发直接学习李群和李代数——这种方法对于急需使用李群论解决问题的朋友可能更加实用。而且，对于一个问题从不同角度看待也利于加深理解。下面一本书就是这个方向的典范：</p>
<p><strong>Lie Groups, Lie Algebras, and Representations: An Elementary Introduction.  by Brian C. Hall</strong></p>
<p>此书从开始即从矩阵切入，从代数而非几何角度引入矩阵李群的概念。并通过定义运算的方式建立 exponential mapping，并就此引入李代数。这种方式比起传统的通过“左不变向量场(Left-invariant vector field)“的方式定义李代数更容易为人所接受，也更容易揭示李代数的意义。最后，也有专门的论述把这种新的定义方式和传统方式联系起来。</p>
<p>群论在 Learning 中用得较多的是它的一个重要方向 Lie group。定义在平滑流形上的群，并且其群运算是平滑的话，那么这就叫李群。因为 Learning 和编码不同，更多关注的是连续空间，因为 Lie group 在各种群中对于 Learning 特别重要。各种子空间，线性变换，非奇异矩阵都基于通常意义的矩阵乘法构成李群。在李群中的映射，变换，度量，划分等等都对于 Learning 中代数方法的研究有重要指导意义。</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>主要是基本概念的辨析，都是最最基础和常规的</p>
<h3 id="监督学习与非监督学习"><a href="#监督学习与非监督学习" class="headerlink" title="监督学习与非监督学习"></a>监督学习与非监督学习</h3><ul>
<li>应用领域：企业数据</li>
</ul>
<p>监督学习需要标注数据(KNN, NB, SVM, DT, BP, RF, GBRT)，这类算法必须知道预测什么，即目标变量的分类信息。对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。</p>
<p>非监督学习(KMEANS, DL)数据没有类别信息，也不会给定目标值，对未标记的样本进行训练学习，比发现这些样本中的结构知识。将数据集合分成由类似的对象组成的多个类的过程被称为<strong>聚类</strong></p>
<h3 id="半监督式学习"><a href="#半监督式学习" class="headerlink" title="半监督式学习"></a>半监督式学习</h3><ul>
<li>应用领域：图像识别（存在大量非标识数据）</li>
</ul>
<p>在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法(Graph Inference)或者拉普拉斯支持向量机(Laplacian SVM)等。</p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>应用领域：机器人控制、系统控制</li>
</ul>
<p>在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括 Q-Learning 以及时间差学习(Temporal difference learning)。</p>
<h3 id="离散数据与连续数据"><a href="#离散数据与连续数据" class="headerlink" title="离散数据与连续数据"></a>离散数据与连续数据</h3><p>离散数据（标称型）的目标变量结果只在有限目标集中取值，比方说真与假，一般用于<strong>分类</strong></p>
<p>连续数据（数值型）目标变量主要用于<strong>回归</strong>分析，通过给定数据点的最优拟合曲线</p>
<h3 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h3><ul>
<li>例子：NB</li>
</ul>
<p>生成方法：由数据学习联合概率密度分布 <code>P(X,Y)</code>，然后求出条件概率分布 <code>P(Y|X)</code> 作为预测的模型，即生成模型：<code>P(Y|X)= P(X,Y) / P(X)</code>。基本思想是首先建立样本的联合概率概率密度模型 <code>P(X,Y)</code>，然后再得到后验概率 <code>P(Y|X)</code>，再利用它进行分类，就像上面说的那样。</p>
<p>生成方法学习联合概率密度分布 <code>P(X,Y)</code>，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布 <code>P(Y|X)</code>，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。</p>
<p><strong>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</strong></p>
<h3 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h3><ul>
<li>例子：k 近邻，决策树</li>
</ul>
<p>由数据直接学习决策函数 <code>Y=f(X)</code> 或者条件概率分布 <code>P(Y|X)</code> 作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。</p>
<p>判别方法直接学习的是决策函数 <code>Y=f(X)</code> 或者条件概率分布 <code>P(Y|X)</code>。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习 <code>P(Y|X)</code> 或 <code>P(X)</code>，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</p>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。</p>
<p>产生原因</p>
<ul>
<li>因为参数太多，导致模型复杂度上升，容易过拟合</li>
<li>权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征</li>
<li>解决方法：交叉验证法、减少特征、正则化、权值衰减、验证数据</li>
</ul>
<p><strong>泛化能力是指模型对未知数据的预测能力</strong></p>
<h3 id="线性分类器与非线性分类器"><a href="#线性分类器与非线性分类器" class="headerlink" title="线性分类器与非线性分类器"></a>线性分类器与非线性分类器</h3><ul>
<li>如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是</li>
<li>常见的线性分类器有：LR, 贝叶斯分类，单层感知机，线性回归</li>
<li>常见的非线性分类器：决策树、RF、GBDT、多层感知机</li>
<li>SVM两种都有(看线性核还是高斯核)</li>
<li>线性分类器速度快、编程方便，但是可能拟合效果不会很好</li>
<li>非线性分类器编程复杂，但是效果拟合能力强</li>
</ul>
<blockquote>
<p>特征比数据量还大时，选择什么样的分类器？</p>
</blockquote>
<p>线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</p>
<blockquote>
<p>对于维度很高的特征，你是选择线性还是非线性分类器？</p>
</blockquote>
<p>线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</p>
<blockquote>
<p>对于维度极低的特征，你是选择线性还是非线性分类器？</p>
</blockquote>
<p>非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分</p>
<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>极大似然估计，只是一种概率论在统计学中的应用，它是参数评估的方法之一。说的 已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计通过若干次实验，观察其结果，利用结果推出参数的大概值。极大似然估计是建立在这样的思想上的：已知某个参数能使这个样本出现的概率最大。我们当然不会再去选择其他其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p>
<h3 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h3><p>后验概率是信息论的基本概念之一。在一个通信系统中，在收到某个消息之后，接收端所了解到的该消息发送的概率称为后验证概率。后验概率是指在得到”结果“的信息后重新修正的概率，如贝叶斯公式中的。是执果寻因的问题。后验概率和先验概率有着不可分割的联系，后验的计算要以先验概率为基础，其实说白了后验概率其实就是条件概率。</p>
<h2 id="朴素贝叶斯-NB"><a href="#朴素贝叶斯-NB" class="headerlink" title="朴素贝叶斯(NB)"></a>朴素贝叶斯(NB)</h2><ul>
<li>优点<ul>
<li>对小规模的数据表现很好</li>
<li>适合多分类任务</li>
<li>适合增量式训练</li>
</ul>
</li>
<li>缺点<ul>
<li>对输入数据的表达形式很敏感</li>
</ul>
</li>
<li>适用数据范围<ul>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>分类算法</li>
</ul>
</li>
</ul>
<p>朴素贝叶斯是贝叶斯理论的一部分，贝叶斯决策理论的核心思想，即选择具有高概率的决策。朴素贝叶斯之所以冠以朴素开头，是因为其在贝叶斯理论的基础上做出了两点假设：</p>
<ol>
<li>每个特征之间相互独立。</li>
<li>每个特征同等重要。</li>
</ol>
<p>贝叶斯准则是构建在条件概率的基础之上的，其公式如下：</p>
<p>$$P(H|X)=\frac{P(X|H)}{P(X)}$$</p>
<p><code>P(H|X）</code>是根据 <code>X</code> 参数值判断其属于类别 <code>H</code> 的概率，称为后验概率。<code>P(H)</code> 是直接判断某个样本属于 <code>H</code> 的概率，称为先验概率。<code>P(X|H)</code> 是在类别 <code>H</code> 中观测到 <code>X</code> 的概率（后验概率），<code>P(X)</code> 是在数据库中观测到X的概率。可见贝叶斯准则是基于条件概率并且和观测到样本的先验概率和后验概率是分不开的。</p>
<p>总结：对于分类而言，使用概率有事要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法。可以通过特征之间的条件独立性假设，降低对数据量的需求。尽管条件独立性的假设并不正确，但是朴素贝叶斯仍然是一种有效的分类器。</p>
<p>一些要注意的地方：</p>
<ul>
<li>给出的特征向量长度可能不同，所以需要归一化为统一长度的向量。比如说文本分类，如果特征是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数</li>
<li>利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率。如果其中一个概率值为 0，那么最后乘积也为 0。为了降低这种影响，可以将所有词出现数字初始化为 1，并将分母初始化为 2。拉普拉斯平滑法将每个 k 值出现次数事先都加 1，通俗讲就是假设他们都出现过一次。</li>
<li>另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的，这里取对数，就可以把乘法变为加法，并且对最后结果没有影响。</li>
<li>遇到特征之间不独立问题，参考改进的贝叶斯网络，使用 DAG 来进行概率图的描述</li>
</ul>
<h2 id="线性回归-Linear-Regression"><a href="#线性回归-Linear-Regression" class="headerlink" title="线性回归(Linear Regression)"></a>线性回归(Linear Regression)</h2><ul>
<li>优点<ul>
<li>结果易于理解</li>
<li>计算上不复杂。</li>
</ul>
</li>
<li>缺点<ul>
<li>对非线性数据拟合不好。</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>回归算法</li>
</ul>
</li>
</ul>
<p>在统计学中，线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。</p>
<p>线性方程的模型函数的向量表示形式为： $h_\theta(x)=\theta^TX$</p>
<p>通过训练数据集寻找向量系数的最优解，即为求解模型参数。其中求解模型系数的优化器方法可以用“最小二乘法”、“梯度下降”算法，来求解损失函数的最优解：</p>
 $$J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})$$  
 $$min_\theta \; J_\theta$$ 
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>将训练特征表示为 <code>X</code> 矩阵，结果表示成 <code>y</code> 向量，仍然是线性回归模型，误差函数不变。那么 <code>θ</code> 可以直接由下面公式得出</p>
<p>$$\theta=(X^TX)^{-1}X^Ty$$</p>
<p>这里 <code>y</code> 是向量，此方法要求 <code>X</code> 是列满秩的，而且求矩阵的逆比较慢。</p>
<p>而在 LWLR（局部加权线性回归）中，参数的计算表达式为:</p>
<p>$$\theta=(X^TX)^{-1}X^TWy$$</p>
<p>因为此时优化的是  $\sum_i w^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$ </p>
<p>由此可见 LWLR 与 LR 不同，LWLR 是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。</p>
<h3 id="岭回归（ridge-regression）"><a href="#岭回归（ridge-regression）" class="headerlink" title="岭回归（ridge regression）"></a>岭回归（ridge regression）</h3><p>岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。</p>
<p>岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数 <code>K</code>（1&gt;K&gt;0），并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。</p>
<p>总结：与分类一样，回归也是预测目标值的过程。回归与分类的不同点在于，前者预测连续型的变量，而后者预测离散型的变量。回归是统计学中最有力的工具之一。在回归方程里，求得特征对应的最佳回归系统的方法是最小化误差的平方和。</p>
<h2 id="k-近邻算法-kNN"><a href="#k-近邻算法-kNN" class="headerlink" title="k-近邻算法(kNN)"></a>k-近邻算法(kNN)</h2><ul>
<li>优点<ul>
<li>精度高</li>
<li>对异常值不敏感</li>
<li>无数据输入假定</li>
</ul>
</li>
<li>缺点<ul>
<li>计算复杂度高</li>
<li>空间复杂度高</li>
</ul>
</li>
<li>适用数据范围<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>分类算法。</li>
</ul>
</li>
</ul>
<p>存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中对应的特征进行比较，然后算法提取样本集中前 k 个最相似的数据。最后，选择 k 个最相似数据中出现次数最多的分类，作为新数据的分类。</p>
<p>为了避免不同的特征的数值不同所导致的影响不同，可能需要进行归一化，也就是把特征值转换成 [0,1] 值</p>
<p>k-近邻算法是分类数据最简单最有效的算法，必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用可能非常耗时。<strong>k决策树</strong>是其优化版本，可以节省大量的计算开销。</p>
<p>另一个却显示它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。使用<strong>概率测量方法</strong>可以解决这个问题。</p>
<p>三要素</p>
<ul>
<li>k值的选择</li>
<li>距离的度量（常见的距离度量有欧式距离，马氏距离等）</li>
<li>分类决策规则 （多数表决规则）</li>
</ul>
<p><code>k</code> 值的选择</p>
<ul>
<li><code>k</code> 值越小表明模型越复杂，更加容易过拟合</li>
<li>但是k值越大，模型越简单，如果 <code>k=N</code> 的时候就表明无论什么点都是训练集中类别最多的那个类</li>
</ul>
<p>所以一般 <code>k</code> 会取一个较小的值，然后用过交叉验证来确定。这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如 95% 训练，5% 预测，然后 <code>k</code> 分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的k</p>
<h3 id="KD树"><a href="#KD树" class="headerlink" title="KD树"></a>KD树</h3><p>KD 树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索（那KNN计算的时候不需要对全样本进行距离的计算了）</p>
<p>在k维的空间上循环找子区域的中位数进行划分的过程。假设现在有K维空间的数据集 <code>T={x1,x2,x3,…xn},xi={a1,a2,a3..ak}</code></p>
<ol>
<li>首先构造根节点，以坐标 <code>a1</code> 的中位数 <code>b</code> 为切分点，将根结点对应的矩形局域划分为两个区域，区域 1 中 <code>a1b</code></li>
<li>构造叶子节点，分别以上面两个区域中 <code>a2</code> 的中位数作为切分点，再次将他们两两划分，作为深度1的叶子节点，（如果 a2 = 中位数 ，则 a2 的实例落在切分面）</li>
<li>不断重复 2 的操作，深度为j的叶子节点划分的时候，索取的<code>ai</code> 的 <code>i=j%k+1</code>，直到两个子区域没有实例时停止</li>
</ol>
<p>KD树的搜索</p>
<ol>
<li>首先从根节点开始递归往下找到包含x的叶子节点，每一层都是找对应的xi</li>
<li>将这个叶子节点认为是当前的“近似最近点”</li>
<li>递归向上回退，如果以 <code>x</code> 圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与 <code>x</code> 更近的点，则进入另一个子区域中查找该点并且更新”近似最近点“</li>
<li>重复 3 的步骤，直到另一子区域与球体不相交或者退回根节点</li>
<li>最后更新的”近似最近点“与 <code>x</code> 真正的最近点</li>
</ol>
<p><strong>KD树进行KNN查找</strong></p>
<p>通过 KD 树的搜索找到与搜索目标最近的点，这样 KNN 的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。</p>
<p><strong>KD树搜索的复杂度</strong></p>
<p>当实例随机分布的时候，搜索的复杂度为 log(N)，N 为实例的个数，KD 树更加适用于实例数量远大于空间维度的 KNN 搜索，如果实例的空间维度与实例个数差不多时，它的效率基于等于线性扫描。</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树的主要优势在于数据形式非常容易理解。决策树很多任务都是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，机器学习算法最终将使用这些机器从数据集中创造的规则。</p>
<ul>
<li>优点<ul>
<li>计算复杂度不高</li>
<li>输出结果易于理解</li>
<li>对中间值的缺失不敏感</li>
<li>可以处理不相关特征数据</li>
</ul>
</li>
<li>缺点<ul>
<li>容易过拟合（后面出现了随机森林）</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>分类算法。</li>
</ul>
</li>
<li>数据要求<ul>
<li>树的构造只适用于标称型的数据，因此数值型数据必须离散化。</li>
</ul>
</li>
</ul>
<p>在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。</p>
<p>创建分支的伪代码如下：</p>
<pre><code>检测数据集中的每个子项是否属于同一分类：
   if so return 类标签；
   else
       寻找数据集的最好特征
       划分数据集
       创建分支结点
           for 每个划分的子集
               调用函数createBranch并增加返回结果到分支结点中
           return 分支结点
</code></pre><p>划分数据集的大原则是：将无序的数据变得更加有序。组织杂乱无章数据的一种方法就是使用信息论度量信息。<strong>信息增益(information gain)</strong>和<strong>熵(entropy)</strong></p>
<p>信息熵的计算公式为：</p>
 $$H=-\sum P(x_i)log_2P(x_i)$$ 
<p>其中  $P(x_i)$ 表示选择该分类的概率。</p>
<p>决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据时，我们首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的所有数据属于同一分类。</p>
<p>ID3 算法可以用于划分标称型数据集。构造决策树时，我们通常采用递归的方法将数据集转化为决策树。</p>
<p>决策树可能会产生过多的数据集划分，从而产生过度匹配数据集的问题。我们可以通过裁剪决策树，合并相邻的无法产生大量信息增益的叶节点，消除过度匹配的问题。</p>
<h3 id="ID3、C4-5-amp-CART"><a href="#ID3、C4-5-amp-CART" class="headerlink" title="ID3、C4.5&amp;CART"></a>ID3、C4.5&amp;CART</h3><p>其实不同的决策树学习算法只是它们选择特征的依据不同，决策树的生成过程都是一样的（根据当前环境对特征进行贪婪的选择）。</p>
<p>ID3 算法的核心是在决策树各个节点上应用信息增益准则选择特征，每一次都选择使得信息增益最大的特征进行分裂，递归地构建决策树。</p>
<p>ID3 算法以信息增益作为划分训练数据集的特征，有一个致命的缺点。选择取值比较多的特征往往会具有较大的信息增益，所以 ID3 偏向于选择取值较多的特征。</p>
<p>针对 ID3 算法的不足，C4.5 算法根据信息增益比来选择特征，对这一问题进行了校正。</p>
<p>CART 指的是分类回归树，它既可以用来分类，又可以被用来进行回归。CART 用作回归树时用平方误差最小化作为选择特征的准则，用作分类树时采用基尼指数最小化原则，进行特征选择，递归地生成二叉树。</p>
<p>决策树的剪枝：我们知道，决策树在生成的过程中采用了贪婪的方法来选择特征，从而达到对训练数据进行更好地拟合（其实从极端角度来看，决策树对训练集的拟合可以达到零误差）。而决策树的剪枝是为了简化模型的复杂度，防止决策树的过拟合问题。具体的决策树剪枝策略可以参见李航的《统计学习方法》。 </p>
<h2 id="Logistic-回归"><a href="#Logistic-回归" class="headerlink" title="Logistic 回归"></a>Logistic 回归</h2><ul>
<li>优点<ul>
<li>计算代价不高</li>
<li>易于理解和实现</li>
</ul>
</li>
<li>缺点<ul>
<li>容易欠拟合</li>
<li>分类精度可能不高</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类别<ul>
<li>分类算法</li>
</ul>
</li>
<li>适用场景<ul>
<li>二分类问题</li>
</ul>
</li>
</ul>
<p>Logistic 回归的目的是寻找一个非线性函数 Sigmoid 的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。</p>
<p>logistic函数表达式为：</p>
 $$h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}$$ 
<p>Sigmoid 函数的定义为 </p>
<p>$$g(z) = \frac{1}{1+e^{-z}}$$</p>
<p>导数形式为</p>
<p>$$g’(z)=g(z)(1-g(z))$$</p>
<p>函数值域范围(0,1)。可以用来做分类器。Sigmoid函数的函数曲线如下：</p>
<p><img src="/images/14734728090031.gif" alt=""></p>
<p>逻辑回归模型分解如下：</p>
<p>(1)首先将不同维度的属性值和对应的一组权重加和，公式如下： </p>
 $$z=w_0+w_1x_1+w_2x_2+\dots+w_mx_m$$ 
<p>其中 $x_1,x_2,\dots,x_m$是某样本数据的各个特征，维度为m</p>
<p>这里就是一个线性回归。W权重值就是需要经过训练学习到的数值，具体W向量的求解，就需要用到极大似然估计和将似然估计函数代入到 优化算法来求解。最常用的最后化算法有 梯度上升算法。</p>
<p>单个样本的后验概率为：</p>
 $$p(y\;|\;x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{(1-y)}$$ 
<p>整个样本的后验概率：</p>
 $$L(\theta)=\sum_{i=1}^mp(y^{(i)}\;|\;x^{(i)},\theta)$$ 
<p>其中 </p>
 $$P(y=1\;|\;x, \theta)=h_\theta(x)$$ 
 $$P(y=0\;|\;x, \theta)=1-h_\theta(x)$$ 
<p>对整个样本的后验概率取对数得到：</p>
 $$\ell(\theta)=logL(\theta)=\sum_{i=1}^my^{(i)}log\;h(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))$$ 
<p>然后利用梯度下降来进行求解，得到最终的 $\theta$</p>
<p>由上面可见：逻辑回归函数虽然是一个非线性的函数，但其实其去除Sigmoid映射函数之后，其他步骤都和线性回归一致。</p>
<p>(2)然后将上述的线性目标函数 <code>z</code> 代入到 sigmoid 逻辑回归函数，可以得到值域为（0,0.5)和（0.5,1）两类值，等于 0.5 的怎么处理还以自己定。这样其实就得到了2类数据，也就体现了二分类的概念。</p>
<p>总结：Logistic 回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，参数的求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法有可以简化为随机梯度上升算法。</p>
<p>随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。此外，随机梯度上升是一个在线算法，它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批处理运算。</p>
<p>处理数据中的缺失值的技巧：</p>
<ul>
<li>使用可用特征的均值来填补缺失值</li>
<li>使用特殊值来填补缺失值，如 -1</li>
<li>忽略有缺失值的样本</li>
<li>使用相似样本的均值填补缺失值</li>
<li>使用另外的机器学习算法预测缺失值</li>
</ul>
<h3 id="关于-LR-的过拟合问题"><a href="#关于-LR-的过拟合问题" class="headerlink" title="关于 LR 的过拟合问题"></a>关于 LR 的过拟合问题</h3><p>如果我们有很多的特性，在训练集上拟合得很好，但是在预测集上却达不到这种效果</p>
<ol>
<li>减少 feature 个数（人工定义留多少个 feature、算法选取这些 feature）</li>
<li>正则化（留下所有的 feature，但对于部分 feature 定义其 parameter 非常小）</li>
</ol>
<h3 id="关于LR的多分类-softmax"><a href="#关于LR的多分类-softmax" class="headerlink" title="关于LR的多分类 softmax"></a>关于LR的多分类 softmax</h3><p>softmax 假设离散型随机变量Y的取值集合是 {1,2,..,k},则多分类的LR为</p>
 $$P(Y=a\;|\;x)=exp(w_ax)/(1-\sum_{i=1}^kw_ix)$$ 
<p>这里会输出当前样本下属于哪一类的概率，并且满足全部概率加起来=1</p>
<h3 id="关于-softmax-和-k-个-LR-的选择"><a href="#关于-softmax-和-k-个-LR-的选择" class="headerlink" title="关于 softmax 和 k 个 LR 的选择"></a>关于 softmax 和 k 个 LR 的选择</h3><p>如果类别之间是否互斥（比如音乐只能属于古典音乐、乡村音乐、摇滚月的一种）就用softmax</p>
<p>否则类别之前有联系（比如一首歌曲可能有影视原声，也可能包含人声，或者是舞曲），这个时候使用k个LR更为合适</p>
<h2 id="树回归"><a href="#树回归" class="headerlink" title="树回归"></a>树回归</h2><ul>
<li>优点<ul>
<li>可以对复杂和非线性的数据建模。</li>
</ul>
</li>
<li>缺点<ul>
<li>结果不易理解。</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>算法类型<ul>
<li>回归算法</li>
</ul>
</li>
</ul>
<p>简述：线性回归方法可以有效的拟合所有样本点(局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的回归算法是比较困难的。此外，实际中很多问题为非线性的，例如常见的分段函数，不可能用全局线性模型类进行拟合。树回归将数据集切分成多份易建模的数据，然后利用线性回归进行建模和拟合。较为经典的树回归算法为 CART(classification and regreesion trees 分类回归树)。</p>
<p>分类回归树(Classification And Regression Tree)是一个决策二叉树，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：</p>
<ul>
<li>分类树：基尼指数最小化(<code>gini_index</code>)</li>
<li>回归树：平方误差最小化</li>
</ul>
<p>分类树：</p>
<ol>
<li>首先是根据当前特征计算他们的基尼增益</li>
<li>选择基尼增益最小的特征作为划分特征</li>
<li>从该特征中查找基尼指数最小的分类类别作为最优划分点</li>
<li>将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于</li>
<li>针对这两类递归进行上述的划分工作，直达所有叶子指向同一样本目标或者叶子个数小于一定的阈值</li>
</ol>
<p>GINI 用来度量分布不均匀性（或者说不纯），总体的类别越杂乱，GINI 指数就越大（跟熵的概念很相似）</p>
<h3 id="解决决策树的过拟合"><a href="#解决决策树的过拟合" class="headerlink" title="解决决策树的过拟合"></a>解决决策树的过拟合</h3><ol>
<li>剪枝<ol>
<li>前置剪枝：在分裂节点的时候设计比较苛刻的条件，如不满足则直接停止分裂（这样干决策树无法到最优，也无法得到比较好的效果）</li>
<li>后置剪枝：在树建立完之后，用单个节点代替子树，节点的分类采用子树中主要的分类（这种方法比较浪费前面的建立过程）</li>
</ol>
</li>
<li>交叉验证</li>
<li>随机森林</li>
</ol>
<h2 id="随机森林-RF"><a href="#随机森林-RF" class="headerlink" title="随机森林 RF"></a>随机森林 RF</h2><p><strong>优缺点</strong></p>
<ol>
<li>能够处理大量特征的分类，并且还不用做特征选择</li>
<li>能够处理具有高维特征的输入样本，而且不需要降维</li>
<li>在训练完成之后能给出哪些feature的比较重要</li>
<li>在生成过程中，能够获取到内部生成误差的一种无偏估计</li>
<li>对于缺省值问题也能够获得很好得结果</li>
<li>训练速度很快，很容易并行</li>
<li>实现相对来说较为简单</li>
</ol>
<p>随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习(Ensemble Learning)方法。随机森林的名称中有两个关键词，一个是“随机”，一个就是“森林”。“森林”我们很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，这样的比喻还是很贴切的，其实这也是随机森林的主要思想–集成思想的体现。</p>
<p>其实从直观角度来解释，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的 Bagging 思想。</p>
<p>bagging 的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以 bagging 改进了预测准确率但损失了解释性。</p>
<p>一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>
<p>随机森林分类效果（错误率）与两个因素有关：</p>
<ul>
<li>森林中任意两棵树的相关性：相关性越大，错误率越大；</li>
<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。</li>
</ul>
<p>减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</p>
<p>构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率 oob error（out-of-bag error）。</p>
<p>随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。</p>
<p>我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的 oob 样本。<br>而这样的采样特点就允许我们进行 oob 估计，它的计算方式如下：</p>
<ol>
<li>对每个样本，计算它作为 oob 样本的树对它的分类情况（约1/3的树）；</li>
<li>然后以简单多数投票作为该样本的分类结果；</li>
<li>最后用误分个数占样本总数的比率作为随机森林的 oob 误分率。</li>
</ol>
<p><strong>学习过程</strong></p>
<ol>
<li>现在有 N 个训练样本，每个样本的特征为 M 个，需要建 K 颗树</li>
<li>从 N 个训练样本中有放回的取 N 个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差），每棵树的训练集都是不同的，而且里面包含重复的训练样本</li>
<li>从 M 个特征中取 m 个特征左右子集特征(<code>m&lt;&lt;M</code>)，随机地从 M 个特征中选取 m 个特征子集，每次树进行分裂时，从这 m 个特征中选择最优的；</li>
<li>对采样的数据使用完全分裂的方式来建立决策树，这样的决策树每个节点要么无法分裂，要么所有的样本都指向同一个分类，每棵树都尽最大程度的生长，并且没有剪枝过程。</li>
<li>重复 2 的过程 K 次，即可建立森林</li>
</ol>
<p><strong>预测过程</strong></p>
<ol>
<li>将预测样本输入到K颗树分别进行预测</li>
<li>如果是分类问题，直接使用投票的方式选择分类频次最高的类别</li>
<li>如果是回归问题，使用分类之后的均值作为结果</li>
</ol>
<p><strong>参数问题</strong></p>
<ol>
<li>这里的一般取 <code>m=sqrt(M)</code></li>
<li>关于树的个数 K，一般都需要成百上千，但是也有具体的样本有关（比如特征数量）</li>
<li>树的最大深度，（太深可能可能导致过拟合）</li>
<li>节点上的最小样本数、最小信息增益</li>
</ol>
<p><strong>学习算法</strong></p>
<ol>
<li>ID3 算法：处理离散值的量</li>
<li>C45 算法：处理连续值的量</li>
<li>Cart 算法：离散和连续 </li>
</ol>
<p>随机森林是一种集成学习+决策树的分类模型，它可以利用集成的思想（投票选择的策略）来提升单颗决策树的分类性能（通俗来讲就是“三个臭皮匠，顶一个诸葛亮”）。</p>
<p>集集成学习和决策树于一身，随机森林算法具有众多的优点，其中最为重要的就是在随机森林算法中每棵树都尽最大程度的生长，并且没有剪枝过程。</p>
<p>随机森林引入了两个随机性——随机选择样本（bootstrap sample）和随机选择特征进行训练。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）</p>
<h3 id="GDBT"><a href="#GDBT" class="headerlink" title="GDBT"></a>GDBT</h3><p>迭代决策树 GBDT(Gradient Boosting Decision Tree）也被称为是 MART(Multiple Additive Regression Tree)或者是 GBRT(Gradient Boosting Regression Tree)，也是一种基于集成思想的决策树模型，但是它和 Random Forest 有着本质上的区别。不得不提的是，GBDT 是目前竞赛中最为常用的一种机器学习算法，因为它不仅可以适用于多种场景，更难能可贵的是，GBDT 有着出众的准确率。这也是为什么很多人称 GBDT 为机器学习领域的“屠龙刀”。</p>
<p>这么牛叉的算法，到底是怎么做到的呢？说到这里，就不得不说一下 GBDT 中的 “GB”（Gradient Boosting）。Gradient Boosting 的原理相当的复杂，但是看不懂它也不妨碍我们对 GBDT 的理解和认识</p>
<p>Boosting，迭代，即通过迭代多棵树来共同决策。这怎么实现呢？难道是每棵树独立训练一遍，比如 A 这个人，第一棵树认为是 10 岁，第二棵树认为是 0 岁，第三棵树认为是 20 岁，我们就取平均值 10 岁做最终结论？当然不是！且不说这是投票方法并不是 GBDT，只要训练集不变，独立训练三次的三棵树必定完全相同，这样做完全没有意义。之前说过，GBDT 是把所有树的结论累加起来做最终结论的，所以可以想到每棵树的结论并不是年龄本身，而是年龄的一个累加量。GBDT 的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是 18 岁，但第一棵树的预测年龄是 12 岁，差了 6 岁，即残差为 6 岁。那么在第二棵树里我们把 A 的年龄设为 6 岁去学习，如果第二棵树真的能把 A 分到 6 岁的叶子节点，那累加两棵树的结论就是 A 的真实年龄；如果第二棵树的结论是 5 岁，则 A 仍然存在 1 岁的残差，第三棵树里 A 的年龄就变成 1 岁，继续学。这就是 Gradient Boosting 在 GBDT 中的意义。</p>
<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><ul>
<li>优点<ul>
<li>泛化错误率低</li>
<li>计算开销不大</li>
<li>结果易解释</li>
</ul>
</li>
<li>缺点<ul>
<li>对参数调节和核函数的选择敏感</li>
<li>原始分类器不加修改仅适用于处理二元分类问题</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
<li>类别<ul>
<li>分类算法</li>
</ul>
</li>
<li>适用场景<ul>
<li>解决二分类问题。</li>
</ul>
</li>
</ul>
<p>将数据集分隔开来的直线称为<strong>分隔超平面(separating hyperplane)</strong>。如果数据对象是 1024 维的，那么就需要一个1023维的某某对象来对数据进行分隔，这个对象就叫<strong>超平面(hyperplane)</strong>，也就是分类的决策边界。分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。</p>
<p>我们希望能找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里点到分隔面的距离被称为<strong>间隔(margin)</strong>。我们希望间隔尽可能大，因为如果我们犯错或者在有限数据上训练分类器的话，我们希望分类器尽可能健壮。</p>
<p><strong>支持向量(support vector)</strong>就是离分隔超平面最近的那些点。接下来要试着最大化支持向量到分隔面的距离。</p>
<p>分隔超平面的形式可以写成 $w^T+b$。要计算点 A 到分隔超平面的距离，就必须给出点到分隔面的法线或垂线的长度，值为 $$\frac{|w^TA+b|}{||w||}$$。这里的常数 b 类似于 Logistic 回归中的截距 $w_0$。</p>
<p>当计算数据点到分隔面的距离并确定分隔面的放置位置时，间隔是通过 $label\times(w^T+b)$来计算的。如果数据点处于正方向(+1)，$w^Tx+b$ 会是一个很大的正数，同时 $label\times(w^T+b)$也会是一个很大的正数；而处于负方向时(-1)，$label\times(w^T+b)$ 仍然会是一个很大的正数。</p>
<p>现在的目标就是找出分类器定义中的 w 和 b。为此，我们必须找到具有最小间隔的数据点，而这些数据点也就是前面提到的支持向量。一旦找到具有最小间隔的数据点，我们就需要对该间隔最大化：</p>
 $$arg max_{w,b}\{min_n(label·(w^Tx+b))·\frac{1}{||w||}\}$$ 
<p>直接求解上述问题相当困难，所以需要将它转换成为另一种更容易求解的形式。</p>
<p>先考察一下大括号中的部分。由于对乘积进行优化是一件很讨厌的事情，因此我们要做的是固定其中一个因子而最大化其他因子。如果令所有支持向量的 $label\times(w^T+b)$  都为 1，那么就可以通过求 <code>||w||</code> 的最大值来得到最终解。但是，并非所有数据点的 $label\times(w^T+b)$  都等于 1，只有那些离分割超平面最近的点得到的值才为 1。而离超平面越远的数据点，其 $label\times(w^T+b)$  的值也就越大。</p>
<p>这里的约束条件就是 $label\times(w^T+b) \ge 1$ 。对于这类优化问题，有一个非常著名的求解方法，拉格朗日乘子法。通过引入拉格朗日乘子，我们就可以基于约束条件来表述原来的问题。由于这里的约束条件都是基于数据点的，因此我们就可以将超平面写成数据点的形式，优化函数就变成</p>
 $$max_\alpha[\sum_{i=1}^m\alpha-\frac{1}{2}label^{(i)}·label^{(j)}·\alpha_i·\alpha_j\langle x^{(i)},x^{(j)}\rangle]$$ 
<blockquote>
<p>$label\times(w^T+b)$ 被称为点到分隔面的函数间隔，$\frac{|w^TA+b|}{||w||}$ 称为点到分隔面的几何间隔<br>尖括号表示两个向量的内积</p>
</blockquote>
<p>约束条件为</p>
 $$\alpha \ge 0 \; and \; \sum_{i=1}^m \alpha_i·label^{(i)} = 0$$ 
<p>考虑到数据不可能非常完美，就需要引入<strong>松弛变量(slack variable)</strong>来允许有些数据点可以处于分隔面错误的一侧。这样我们的优化目标就能保持仍然不变，但约束条件变成：</p>
 $$C \ge\alpha \ge 0 \; and \; \sum_{i=1}^m \alpha_i·label^{(i)} = 0$$
<p>这里的常数 C 用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0”这两个目标的权重。在优化算法的实现代码中，常数 C 是一个参数，因此我们就可以通过调节该参数得到不同的结果。一旦求出了所有的 α，那么分隔超平面就可以通过这些 α 来表达。这一结论十分直接，SVM 中的主要工作就是求解这些 α。</p>
<h3 id="SMO-高效优化算法（Sequential-Minimal-Optimization，SMO）"><a href="#SMO-高效优化算法（Sequential-Minimal-Optimization，SMO）" class="headerlink" title="SMO 高效优化算法（Sequential Minimal Optimization，SMO）"></a>SMO 高效优化算法（Sequential Minimal Optimization，SMO）</h3><p><strong>Platt 的 SMO 算法</strong></p>
<p>SMO 表示<strong>序列最小化(Sequential Minimal Optimization)</strong>。Platt 的 SMO 算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果是完全一致的。</p>
<p>SMO 算法的目标是求出一系列 α 和 b，一旦求出了这些 α，就很容易计算出权重向量 w 并得到分隔超平面。</p>
<p>它选择凸二次规划的两个变量，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度，关于这两个变量：</p>
<ol>
<li>其中一个是严重违反KKT条件的一个变量</li>
<li>另一个变量是根据自由约束确定，好像是求剩余变量的最大化来确定的。</li>
</ol>
<p>SMO 算法的工作原理是：每次循环中选择两个 α 进行优化处理。一旦找到一对合适的 α，那么就增大其中一个同时减小另一个。这里所谓的“合适”就是指两个 α 必须要符合一定的条件，条件之一就是这两个 α 必须要在间隔边界之外，第二个条件则是这两个 α 还没有进行过区间化处理或者不在边界上。</p>
<p>Platt SMO 算法中的外循环确定要优化的最佳 α 对。而简化版会跳过这一部分，首先在数据集上遍历每一个 α，然后在剩下的 α 集合中随机选择另一个 α，从而构建 α 对。这一点相当重要，要同时改变，因为我们有一个约束条件：</p>
 $$\alpha \ge 0 \; and \; \sum_{i=1}^m \alpha_i·label^{(i)} = 0$$
<p>由于改变一个 α 可能会导致该约束条件失效，因此我们总是同时改变两个 α。</p>
<p>伪代码：</p>
<pre><code>创建一个 α 向量并将其初始化为 0 向量
当迭代次数小于最大迭代次数时(外循环)
    对数据集中的每个数据向量(内循环):
        如果该数据向量可以被优化:
            随机选择另外一个数据向量
            同时优化这两个向量
            如果两个向量都不能被优化，退出内循环
    如果所有向量都没有被优化，增加迭代数量，继续下一次循环
</code></pre><p><strong>完整的Platt SMO算法</strong></p>
<p>在在选择第一个 α 值后，算法会通过一个内循环来选择第二个 α 值。在优化过程中，会通过<strong>最大化步长</strong>的方式来获得第二个 α 值。</p>
<h3 id="在复杂数据上应用核函数"><a href="#在复杂数据上应用核函数" class="headerlink" title="在复杂数据上应用核函数"></a>在复杂数据上应用核函数</h3><p>核函数(kernel) 和 径向基函数(fadial basis function)</p>
<h4 id="利用核函数将数据映射到高维空间"><a href="#利用核函数将数据映射到高维空间" class="headerlink" title="利用核函数将数据映射到高维空间"></a>利用核函数将数据映射到高维空间</h4><p>从某个特征空间到另一个特征空间的映射是通过核函数来实现的。可以把核函数想象成一个<strong>包装器(wrapper)</strong>或者是<strong>接口(interface)</strong>，它能把数据从某个很难处理的形式转换成另一种较易处理的形式。</p>
<p>SVM 优化中一个特别好的地方是，所有的运算都可以写成<strong>内积(inner product)</strong>的形式。向量的内积指的是两个向量相乘，之后得到单个标量或者数值。我们可以把内积运算替换成核函数，而不必做简化处理。将内积替换成核函数的方式被称为<strong>核技巧(kernel trick)</strong>或者<strong>核变电(kernel substation)</strong>。</p>
<h4 id="径向基核函数"><a href="#径向基核函数" class="headerlink" title="径向基核函数"></a>径向基核函数</h4><p>采用向量作为自变量的函数，基于向量距离运算输出一个标量。这个距离可以是从<0, 0="">向量或者其他向量开始计算的距离，我们使用径向基函数的高斯版本，公式如下：</0,></p>
<p>$$k(x,y)=exp(\frac{-||x-y||^2}{2\sigma^2})$$</p>
<p>上述高斯核函数将数据从其特征空间映射到更高维的空间，具体来说这里是映射到一个无穷维的空间。</p>
<p>支持向量的数目存在一个最优值。SVM 的优点在于它能对数据进行高效分类。如果支持向量太少，就可能会得到一个很差的决策边界；如果支持向量太多，也就相当于每次都利用整个数据集进行分类，这种分类方法称为 k近邻。</p>
<p>可以这么看 SVM 比 k 近邻好的地方在于，从很多数据中找到最有代表性的数据点来作为分类的依据，可以有效减少多余的计算。</p>
<h3 id="SVM-多分类方法"><a href="#SVM-多分类方法" class="headerlink" title="SVM 多分类方法"></a>SVM 多分类方法</h3><p><strong>一对多</strong></p>
<p>其中某个类为一类，其余 n-1 个类为另一个类，比如 A,B,C,D 四个类，第一次 A 为一个类，{B,C,D} 为一个类训练一个分类器，第二次 B 为一个类,{A,C,D} 为另一个类,按这方式共需要训练 4 个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x), f2(x), f3(x) 和 f4(x),取其最大值为分类器(这种方式由于是 1 对 M 分类，会存在偏置，很不实用)</p>
<p><strong>一对一(libsvm实现的方式)</strong></p>
<p>任意两个类都训练一个分类器，那么n个类就需要 <code>n*(n-1)/2</code> 个svm分类器。</p>
<p>还是以 A,B,C,D 为例,那么需要 {A,B},{A,C},{A,D},{B,C},{B,D},{C,D} 为目标共6个分类器，然后在预测的将测试样本通过这6个分类器之后进行投票选择最终结果。（这种方法虽好，但是需要 <code>n*(n-1)/2</code> 个分类器代价太大，不过有好像使用循环图来进行改进）</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>支持向量机的泛化错误率较低，也就是说它具有良好的学习能力，且学到的结果具有很好的推广性。这些优点使得向量机十分流行，有些人认为它是监督学习中最好的定式算法。</p>
<p>支持向量机试图通过求解一个二次优化问题来最大化分类间隔。</p>
<p>和方法或者说核技巧会将数据(有时是非线性数据)从一个低维空间映射到一个高维空间，可以将一个在低维空间中的非线性问题转换成高维空间下的线性问题来求解。和方法不止在 SVM 中适用，还可以用于其他算法中。而其中径向基函数是一个常用的度量两个向量距离的核函数。</p>
<p>支持向量机是一个二元分类器。当用其解决多元问题时，则需要额外的方法对其进行扩展。SVM 的效果也对优化参数和所用核函数中的参数敏感。</p>
<h2 id="Bagging-和-Boosting"><a href="#Bagging-和-Boosting" class="headerlink" title="Bagging 和 Boosting"></a>Bagging 和 Boosting</h2><p>使用机器学习方法解决问题时，有较多模型可供选择。 一般的思路是先根据数据的特点，快速尝试某种模型，选定某种模型后， 再进行模型参数的选择（当然时间允许的话，可以对模型和参数进行双向选择）</p>
<p>因为不同的模型具有不同的特点， 所以有时也会将多个模型进行组合，以发挥”三个臭皮匠顶一个诸葛亮的作用”，这样的思路，反应在模型中，主要有两种思路：Bagging和Boosting</p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>Bagging 可以看成是一种圆桌会议，或是投票选举的形式，其中的思想是：”群众的眼光是雪亮的”，可以训练多个模型，之后将这些模型进行加权组合，一般这类方法的效果，都会好于单个模型的效果。 在实践中，在特征一定的情况下，大家总是使用Bagging的思想去提升效果。例如kaggle上的问题解决，因为大家获得的数据都是一样的，特别是有些数据已经过预处理。</p>
<p>基本的思路比较简单，就是：训练时，使用 replacement 的 sampling 方法， sampling 一部分训练数据k次并训练 k 个模型；预测时，使用 k 个模型，如果为分类，则让 k 个模型均进行分类并选择出现次数最多的类(每个类出现的次数占比可以视为置信度)；如为回归，则为各类器返回的结果的平均值。</p>
<p>在该处，Bagging 算法可以认为每个分类器的权重都一样。</p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>在 Bagging 方法中，我们假设每个训练样本的权重都是一致的； 而 Boosting 算法则更加关注错分的样本，越是容易错分的样本，约要花更多精力去关注。对应到数据中，就是该数据对模型的权重越大，后续的模型就越要拼命将这些经常分错的样本分正确。 最后训练出来的模型也有不同权重，所以 boosting 更像是会整，级别高，权威的医师的话语权就重些。</p>
<p>Bagging 和 Boosting 都可以视为比较传统的集成学习思路。 现在常用的 Random Forest，GBDT，GBRank 其实都是更加精细化，效果更好的方法</p>
<h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><ul>
<li>优点<ul>
<li>泛化错误率低</li>
<li>易编码，可以应用在大部分分类器上</li>
<li>无参数调整</li>
</ul>
</li>
<li>缺点<ul>
<li>对离群点敏感</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
<li>标称型</li>
</ul>
</li>
</ul>
<p>boosting 是一种与 bagging 很类似的技术。不论是在 boosting 还是 bagging 当中，所使用的多个分类器的类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting 是通过集中关注被已有分类器错分的那些数据来获得新的分类器。</p>
<p>由于 boosting 分类的结果是基于所有分类器的加权求和结果的，因此 boosting 与 bagging 不太一样。bagging 中的分类器权重是相等的，而 boosting 中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。</p>
<p>boosting 方法拥有多个版本，这里只关注最流行的 AdaBoost</p>
<p>能否使用弱分类器和多个实例来构建一个强分类器？这是一个非常有趣的理论问题。这里的“弱”意味着分类器的性能比所及猜测要略好，但是也不会好太多。AdaBoost 算法即脱胎于上述理论问题。</p>
<p>AdaBoost 是 adaptive boosting(自适应 boosting)的缩写，其运行过程如下：训练数据中的每个样本各有一个权重，这些权重构成了向量 D。一开始，这些权重都初始化成相等值。首先在训练数据上训练处一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。</p>
<p>为了从所有弱分类器中得到最终的分类结果，AdaBoost 为每个分类器都分配了一个权重值 α，这些 α 值是基于每个弱分类器的错误率进行计算的。错误率和 α 的公式为</p>
<p>$$\epsilon=\frac{未正确分类的样本数目}{所有样本数目}$$</p>
<p>$$\alpha=\frac{1}{2}ln(\frac{1-\epsilon}{\epsilon})$$</p>
<p>计算出 α 值之后，可以对权重向量 D 进行更新，以使那些正确分类的样本的权重降低而错分样本的权重升高。D 的计算方法如下：</p>
<p>正确情况：</p>
 $$D_i^{(i+1)}=\frac{D_i^{(i)}e^{-\alpha}}{\sum D}$$
<p>错误情况：</p>
 $$D_i^{(i+1)}=\frac{D_i^{(i)}e^{\alpha}}{\sum D}$$
<p>计算出 D 之后，AdaBoost 又开始进入下一轮迭代。AdaBoost 算法会不断地重复训练和调整权重的过程，直到训练错误率为 0 或者弱分类器的数目达到用户的指定值为止。</p>
<p>构建弱分类器：<strong>单层决策树(decision stump, 决策树桩)</strong>是一种简单的决策树，仅基于单个特征来做决策。</p>
<h2 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h2><ul>
<li>优点<ul>
<li>容易实现</li>
<li>对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是 O(nkt)，其中 n 是所有对象的数目，k 是簇的数目,t 是迭代的次数。通常 k&lt;&lt;n。这个算法通常局部收敛</li>
<li>算法尝试找出使平方误差函数值最小的 k 个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好</li>
</ul>
</li>
<li>缺点<ul>
<li>可能收敛到局部最小值</li>
<li>在大规模数据集上收敛较慢</li>
<li>k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合</li>
<li>要求用户必须事先给出要生成的簇的数目 k</li>
<li>不适合于发现非凸面形状的簇，或者大小差别很大的簇</li>
<li>对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响</li>
</ul>
</li>
<li>使用数据类型<ul>
<li>数值型</li>
</ul>
</li>
<li>算法类型<ul>
<li>聚类算法</li>
</ul>
</li>
</ul>
<p>K-Means的基本步骤：</p>
<ol>
<li>从数据对象中随机的初始化K个初始点作为质心。然后将数据集中的每个点分配到一个簇中，具体来讲每个点找到距其最近的质心，并将其分配给该质心所对应的簇。</li>
<li>计算每个簇中样本点的均值，然后用均值更新掉该簇的质心。然后划分簇结点。</li>
<li>迭代重复（2）过程，当簇对象不再发生变化时，或者误差在评测函数预估的范围时，停止迭代。</li>
</ol>
<p>选择批次距离尽可能远的K个点</p>
<p>首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的点作为第三个中心点，以此类推，直至选取大k个</p>
<p>选用层次聚类或者 Canopy 算法进行初始聚类</p>
<p>聚类属于无监督学习，以往的回归、朴素贝叶斯、SVM 等都是有类别标签 y 的，也就是说样例中已经给出了样例的分类。而聚类的样本中却没有给定 y，只有特征 x，比如假设宇宙中的星星可以表示成三维空间中的点集(x,y,z)。聚类的目的是找到每个样本x潜在的类别y，并将同类别y的样本x放在一起。</p>
<p>在聚类问题中，给我们的训练样本是 {x(1),…,x(m)}，每个 x(i)∈ R^n，没有了 y。</p>
<p>下面累述一下 K-means 与 EM 的关系，首先回到初始问题，我们目的是将样本分成 k 个类，其实说白了就是求每个样例 x 的隐含类别 y，然后利用隐含类别将 x 归类。由于我们事先不知道类别 y，那么我们首先可以对每个样例假定一个 y吧，但是怎么知道假定的对不对呢？怎么评价假定的好不好呢？我们使用样本的极大似然估计来度量，这里是就是x和y的联合分布 P(x,y)了。如果找到的 y 能够使 P(x,y) 最大，那么我们找到的 y 就是样例 x 的最佳类别了，x 顺手就聚类了。但是我们第一次指定的 y 不一定会让 P(x,y) 最大，而且 P(x,y) 还依赖于其他未知参数，当然在给定 y 的情况下，我们可以调整其他参数让 P(x,y) 最大。但是调整完参数后，我们发现有更好的 y 可以指定，那么我们重新指定 y，然后再计算 P(x,y) 最大时的参数，反复迭代直至没有更好的 y 可以指定。</p>
<p>这个过程有几个难点，第一怎么假定 y？是每个样例硬指派一个 y 还是不同的 y 有不同的概率，概率如何度量。第二如何估计P(x,y)，P(x,y) 还可能依赖很多其他参数，如何调整里面的参数让 P(x,y) 最大。</p>
<p>这里只是指出 EM 的思想，E 步就是估计隐含类别 y 的期望值，M 步调整其他参数使得在给定类别 y 的情况下，极大似然估计 P(x,y) 能够达到极大值。然后在其他参数确定的情况下，重新估计 y，周而复始，直至收敛。</p>
<p>上面的阐述有点费解，对应于K-$x^{(i)}$对应隐含变量也就是最佳类别 $c^{(i)}$。最开始可以随便指定一个 $c^{(i)}$ 给它，然后为了让 P(x,y) 最大（这里是要让 J 最小），我们求出在给定 c 情况下，J最小时的  $u_j$（前面提到的其他未知参数），然而此时发现，可以有更好的 $c^{(i)}$（质心与样例$x^{(i)}$ 距离最小的类别）指定给样例 $x^{(i)}$，那么 $c^{(i)}$ 得到重新调整，上述过程就开始重复了，直到没有更好的 $c^{(i)}$ 指定。</p>
<p>这样从K-means里我们可以看出它其实就是EM的体现，E步是确定隐含类别变量 $c^{(i)}$，M步更新其他参数 <code>u</code> 来使J最小化。这里的隐含类别变量指定方法比较特殊，属于硬指定，从k个类别中硬选出一个给样例，而不是对每个类别赋予不同的概率。总体思想还是一个迭代优化过程，有目标函数，也有参数变量，只是多了个隐含变量，确定其他参数估计隐含变量，再确定隐含变量估计其他参数，直至目标函数最优。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>如果将样本看作观察值，潜在类别看作是隐藏变量，那么聚类问题也就是参数估计问题，只不过聚类问题中参数分为隐含类别变量和其他参数，这犹如在 x-y 坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，E步估计隐含变量，M步估计其他参数，交替将极值推向最大。EM中还有“硬”指定和“软”指定的概念，“软”指定看似更为合理，但计算量要大，“硬”指定在某些场合如 K-means 中更为实用（要是保持一个样本点到其他所有中心的概率，就会很麻烦）。</p>
<p>另外，EM 的收敛性证明方法确实很牛，能够利用 log 的凹函数性质，还能够想到利用创造下界，拉平函数下界，优化下界的方法来逐步逼近极大值。而且每一步迭代都能保证是单调的。最重要的是证明的数学公式非常精妙，硬是分子分母都乘以z的概率变成期望来套上Jensen不等式，前人都是怎么想到的。</p>
<h2 id="PCA-主成分分析"><a href="#PCA-主成分分析" class="headerlink" title="PCA 主成分分析"></a>PCA 主成分分析</h2><ul>
<li>优点<ul>
<li>降低数据的复杂性</li>
<li>识别最重要的多个特征。</li>
</ul>
</li>
<li>缺点<ul>
<li>不一定需要</li>
<li>且可能损失有用信息。</li>
</ul>
</li>
<li>适用适用类型<ul>
<li>数值型</li>
</ul>
</li>
<li>技术类型<ul>
<li>降维技术</li>
</ul>
</li>
</ul>
<p>按照一定的数学变换方法，把给定的一组相关变量（维度）通过线性变换转成另一组不相关的变量，这些新的变量按照方差依次递减的顺序排列。在数学变换中保持变量的<code>总方差</code>不变，使<code>第一变量</code>具有<code>最大的方差</code>，称为<code>第一主成分</code>，<code>第二变量</code>的<code>方差次大</code>，并且和第一变量不相关，称为第二主成分。依次类推，I个变量就有I个主成分。</p>
<p><strong>通过低维表征的向量和特征向量矩阵，可以基本重构出所对应的原始高维向量</strong></p>
<h3 id="总结与讨论"><a href="#总结与讨论" class="headerlink" title="总结与讨论"></a>总结与讨论</h3><p>PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>
<p>PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p>
<p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<h2 id="SVD-singular-value-decomposition-奇异值分解"><a href="#SVD-singular-value-decomposition-奇异值分解" class="headerlink" title="SVD(singular value decomposition) 奇异值分解"></a>SVD(singular value decomposition) 奇异值分解</h2><ul>
<li>优点<ul>
<li>简化数据</li>
<li>去除噪声</li>
<li>提高算法的结果。</li>
</ul>
</li>
<li>缺点<ul>
<li>数据转换可能难以理解。</li>
</ul>
</li>
<li>适用数据类型<ul>
<li>数值型</li>
</ul>
</li>
<li>SVD是矩阵分解的一种类型。</li>
</ul>
<p>总结：SVD是一种强大的降维工具，我们可以利用SVD来逼近矩阵并从中提取重要特征。通过保留矩阵 80%~90% 的能量，就可以得到重要的特征并去掉噪声。SVD已经运用到多个应用中，其中一个成功的应用案例就是推荐引擎。推荐引擎将物品推荐给用户，协同过滤则是一种基于用户喜好和行为数据的推荐和实现方法。协同过滤的核心是相似度计算方法，有很多相似度计算方法都可以用于计算物品或用户之间的相似度。通过在低维空间下计算相似度，SVD提高了推荐引擎的效果。</p>
<p>SVD将矩阵分解为三个矩阵的乘积，公式如下所示：</p>
 $$Data_{m\times n}=U_{m\times m}\Sigma_{m\times n}V_{n\times n}^T$$
<p>中间的矩阵sigma为对角矩阵，对角元素的值为Data矩阵的奇异值(注意奇异值和特征值是不同的)，且已经从大到小排列好了。即使去掉特征值小的那些特征，依然可以很好的重构出原始矩阵。</p>
<p>如果 m 代表商品的个数，n 代表用户的个数，则 U 矩阵的每一行代表商品的属性，现在通过降维 U 矩阵（取深色部分）后，每一个商品的属性可以用更低的维度表示（假设为 k 维）。这样当新来一个用户的商品推荐向量 X，则可以根据公式  $X'U_1\Sigma_1^{-1}$ 得到一个 k 维的向量，然后在 V’ 中寻找最相似的那一个用户（相似度测量可用余弦公式等），根据这个用户的评分来推荐（主要是推荐新用户未打分的那些商品）。</p>
<p>SVD++ 可以说是SVD模型的加强版，除了打分关系，SVD++还可以对隐含的回馈(implicit feedback) 进行建模。</p>
<p>这种隐含的回馈可以是打分动作（谁对某个商品打过分），或者是浏览记录等。 只要有类似的隐含回馈，客观上也表示了 user 对某个 item 的偏好。 毕竟，user 不会无缘无故地浏览一个 item，肯定有什么原因， 比如 user 喜欢紫色，恰恰这个 item 也是紫色的，那通过隐含回馈就可以对 user 对紫色的偏好建模出来。</p>
<p>现实中，隐含回馈的原因比较复杂，专门给一部分参数空间去建模，肯定对用户的建模有一些帮助。</p>
<p>除了在 SVD 中定义的向量外，每个 item 对应一个向量 yi ，来通过 user 隐含回馈过的 item 的集合来刻画用户的偏好。</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>特征工程大概包括两个部分：特征提取和特征选择。</p>
<p>一般来说，领域内的知识主要是应用在特征提取。举个例子的话，比如说基于内容的购物推荐，性别就是一个很重要的领域知识，男性和女性关注的物品差别就比较大，推荐也应该体现出这种差别，那么这个特征就是这个问题一个重要特征，应该重点从已知数据提取，甚至专门为它再构建一个机器学习问题。</p>
<p>一般简单且常见的都是卡方检验，互信息和信息增益这三种</p>
<p>关于特征工程(Feature Engineering)，已经是很古老很常见的话题了，坊间常说：“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。纵观 Kaggle、KDD 等国内外大大小小的比赛，每个竞赛的冠军其实并没有用到很高深的算法，大多数都是在特征工程这个环节做出了出色的工作，然后使用一些常见的算法，比如LR，就能得到出色的性能。遗憾的是，在很多的书籍中并没有直接提到 Feature Engineering，更多的是 Feature selection。这也并不，很多ML书籍都是以讲解算法为主，他们的目的是从理论到实践来理解算法，所以用到的数据要么是使用代码生成的，要么是已经处理好的数据，并没有提到特征工程。在这篇文章，我打算自我总结下特征工程，让自己对特征工程有个全面的认识。在这我要说明一下，我并不是说那些书写的不好，其实都很有不错，主要是因为它们的目的是理解算法，所以直接给出数据相对而言对于学习和理解算法效果更佳。</p>
<p>特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。</p>
<p>简而言之，特征工程就是一个把原始数据转变成特征的过程，这些特征可以很好的描述这些数据，并且利用它们建立的模型在未知数据上的表现性能可以达到最优（或者接近最佳性能）。从数学的角度来看，特征工程就是人工地去设计输入变量X。</p>
<p>（1）特征越好，灵活性越强</p>
<p>只要特征选得好，即使是一般的模型（或算法）也能获得很好的性能，因为大多数模型（或算法）在好的数据特征下表现的性能都还不错。好特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易理解和维护。</p>
<p>（2）特征越好，构建的模型越简单</p>
<p>有了好的特征，即便你的参数不是最优的，你的模型性能也能仍然会表现的很nice，所以你就不需要花太多的时间去寻找最有参数，这大大的降低了模型的复杂度，使模型趋于简单。</p>
<p>（3）特征越好，模型的性能越出色</p>
<p>显然，这一点是毫无争议的，我们进行特征工程的最终目的就是提升模型的性能。</p>
<h3 id="特征选择-Feature-Selection"><a href="#特征选择-Feature-Selection" class="headerlink" title="特征选择 Feature Selection"></a>特征选择 Feature Selection</h3><p>首先，从特征开始说起，假设你现在有一个标准的Excel表格数据，它的每一行表示的是一个观测样本数据，表格数据中的每一列就是一个特征。在这些特征中，有的特征携带的信息量丰富，有的（或许很少）则属于无关数据(irrelevant data)，我们可以通过特征项和类别项之间的相关性（特征重要性）来衡量。比如，在实际应用中，常用的方法就是使用一些评价指标单独地计算出单个特征跟类别变量之间的关系。如Pearson相关系数，Gini-index（基尼指数），IG（信息增益）等，下面举Pearson指数为例，它的计算方式如下：</p>
 $$r_{xy}^2=(\frac{con(x,y)}{\sqrt{var(x)var(y)}})$$
<p>其中，x 属于 X，X 表一个特征的多个观测值，y 表示这个特征观测值对应的类别列表。</p>
<p>Pearson 相关系数的取值在 0 到 1 之间，如果你使用这个评价指标来计算所有特征和类别标号的相关性，那么得到这些相关性之后，你可以将它们从高到低进行排名，然后选择一个子集作为特征子集（比如top 10%），接着用这些特征进行训练，看看性能如何。此外，你还可以画出不同子集的一个精度图，根据绘制的图形来找出性能最好的一组特征。</p>
<p>这就是特征工程的子问题之一——特征选择，它的目的是从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果。</p>
<p>做特征选择的原因是因为这些特征对于目标类别的作用并不是相等的，一些无关的数据需要删掉。做特征选择的方法有多种，上面提到的这种特征子集选择的方法属于 filter（刷选器）方法，它主要侧重于单个特征跟目标变量的相关性。优点是计算时间上较高效,对于过拟合问题也具有较高的鲁棒性。缺点就是倾向于选择冗余的特征,因为他们不考虑特征之间的相关性,有可能某一个特征的分类能力很差，但是它和某些其它特征组合起来会得到不错的效果。另外做特征子集选取的方法还有 wrapper（封装器）和 Embeded(集成方法)。wrapper 方法实质上是一个分类器，封装器用选取的特征子集对样本集进行分类，分类的精度作为衡量特征子集好坏的标准,经过比较选出最好的特征子集。常用的有逐步回归(Stepwise regression)、向前选择(Forward selection)和向后选择(Backward selection)。它的优点是考虑了特征与特征之间的关联性，缺点是：当观测数据较少时容易过拟合，而当特征数量较多时,计算时间又会增长。对于 Embeded 集成方法，它是学习器自身自主选择特征，如使用 Regularization 做特征选择，或者使用决策树思想，细节这里就不做介绍了。这里还提一下，在做实验的时候，我们有时候会用 Random Forest 和 Gradient boosting 做特征选择，本质上都是基于决策树来做的特征选择，只是细节上有些区别。</p>
<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>原则上来讲，特征提取应该在特征选择之前。特征提取的对象是原始数据(raw data)，它的目的是自动地构建新的特征，将原始特征转换为一组具有明显物理意义(Gabor、几何特征[角点、不变量]、纹理[LBP HOG])或者统计意义或核的特征。比如通过变换特征取值来减少原始数据中某个特征的取值个数等。对于表格数据，你可以在你设计的特征矩阵上使用主要成分分析(Principal Component Analysis，PCA)来进行特征提取从而创建新的特征。对于图像数据，可能还包括了线或边缘检测。</p>
<p>常用的方法有：</p>
<ul>
<li>PCA (Principal component analysis，主成分分析)</li>
<li>ICA (Independent component analysis，独立成分分析)</li>
<li>LDA （Linear Discriminant Analysis，线性判别分析）</li>
</ul>
<p>对于图像识别中，还有SIFT方法。</p>
<p>用中文来说就是：特征工程是一个超集，它包括特征提取、特征构建和特征选择这三个子模块。在实践当中，每一个子模块都非常重要，忽略不得。根据答主的经验，他将这三个子模块的重要性进行了一个排名，即：特征构建&gt;特征提取&gt;特征选择。</p>
<p>事实上，真的是这样，如果特征构建做的不好，那么它会直接影响特征提取，进而影响了特征选择，最终影响模型的性能。</p>
<h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>我们先说什么是深度学习。其实从整体上来讲，Deep Learning 就是曾经的多层神经网络，整体的思想认为每一个层次都可以被作为一个独立的特征抽象存在，所以最广泛地被用作特征工程上，而 GPU 的存在更是解决了几十年前的 ANN 的训练效率问题。那么简单来说，Deep Learning 可以对抽取出的特征进行非线性组合形成更有效的特征表示。确实，从这一点来说，Deep Learning 确实从理论上很好的解决了机器学习领域很麻烦的“特征抽取”问题，但是在实际的工业界，“特征工程”到底有多复杂？我们看看 Deep Learning 表现最好的 IR 领域吧，曾经是怎么做的呢？据了解微软有个小 Team 专门做的事儿就是从图片上找各种各样的特征，因为算法本身其实已经被锁死在 Random Forest上了，往往特征的微调就能带来算法效果的极大提升，那么 Deep Learning 的出现当然可以很好地取代这项工作(实际效果确实无法得知)，那么总结下Deep Learning的好处：从海量的特征中通过特征工程抽取出有效的特征组合。</p>
<p>但是刨除掉语音和图像领域，转向离我们更近的工作，无论是推荐系统还是数据挖掘，特征是怎么出来的呢？对于一个电影，对于一个用户，满打满算一共就那么多特征，这个时候 Deep Learning 根本无从发挥。那么再退一步说，就算把 User 对于 Item 的标定作为 Item 的特征，由于在实际中大部分的缺失值存在，那么如果你希望用 Deep Learning 来对该矩阵做特征重组，第一件事情就是如何填充缺失值，而这恰恰是比特征工程更困难的事情。</p>
<h2 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h2><p>大数据其实意味着大样本量，那么大样本量带来的是高置信度以及广覆盖度。例如从 FM 来说，大数据量意味着更全面地了解一个用户的听歌品位，从金融互联网的信用风险评估来说，大数据量意味着不仅仅从消费记录而包含了社交网络信息去对用户做更全面的评价，从用户画像来说意味着建立全面的兴趣图谱和知识图谱，这些都是大数据带给我们的实际意义。说得学术一些，我们不妨认为大数据是频率学派对于贝叶斯学派一次强有力的逆袭。那么既然说到这个份上了，我们不妨思考一下，我们是不是有希望在回归贝叶斯学派，利用先验信息+小数据完成对大数据的反击呢？</p>
<p>另外，既然我们已经说到了大数据的广覆盖度，就针对这个再额外说一下吧。诚然，大数据能够全面地覆盖到所有信息，但是从实际的工业界来看，考虑到实际的计算能力以及效果，大多数公司都会对大数据做“去噪”，那么在去噪的过程中去除的不仅仅是噪音，也包括“异常点”，而这些“异常点”，恰恰把大数据的广覆盖度给降低了，于是利用大数据反而比小数据更容易产生趋同的现象。尤其对于推荐系统来说，这些“异常点”的观察其实才是“个性化”的极致。</p>
<h2 id="用户画像"><a href="#用户画像" class="headerlink" title="用户画像"></a>用户画像</h2><p>任何系统不要脱离产品而存在。先吐个槽，之前在某个公司面试，某个公司上来就问我，你觉得我们的用户画像应该怎么做？这个问题是非常业余的(这个问题就像是有人问我我们网站有性能问题，你说咋办；好吧，这个问题也是这个公司问我的)，任何数据系统都是强产品关联的，这也是太多公司去做数据系统的误区，在这里我还是用户画像为例。 用户画像到底是什么，其实说简单了他就是一个用户宽表，如果偏要我说需要注意的，就是在选择数据库的时候一定要选择列容易扩充的数据库。如果要说具体需要哪些字段，我还真的没法说，我只能把他归类成用户元属性数据，行为统计数据，潜在挖掘数据，至此而已。因为数据系统从来不是一个事先规划好的系统，而是需要随着业务增长来逐渐填充的系统，这也是数据平台难做的原因。 所以我真心无法理解有一些不太大的公司成立了一个部门，这个部门专门做用户画像(例如PPTV)。</p>
<h2 id="实体识别"><a href="#实体识别" class="headerlink" title="实体识别"></a>实体识别</h2><p>命名实体识别(Named EntitiesRecognition, NER)是自然语言处理(Natural LanguageProcessing, NLP)的一个基础任务。其目的是识别语料中人名、地名、组织机构名等命名实体。由于这些命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的一些规律性，因而,通常把对这些词的识别从词汇形态处理(如汉语切分)任务中独立处理，称为命名实体识别。命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。</p>
<p>命名实体是命名实体识别的研究主体，一般包括3大类(实体类、时间类和数字类)和7小类(人名、地名、机构名、时间、日期、货币和百分比)命名实体。评判一个命名实体是否被正确识别包括两个方面：实体的边界是否正确；实体的类型是否标注正确。主要错误类型包括文本正确，类型可能错误；反之，文本边界错误,而其包含的主要实体词和词类标记可能正确。</p>
<p>命名实体识别的主要技术方法分为：基于规则和词典的方法、基于统计的方法、二者混合的方法等。</p>
<p>1.基于规则和词典的方法</p>
<p>基于规则的方法多采用语言学专家手工构造规则模板,选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词(如尾字)、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。基于规则和词典的方法是命名实体识别中最早使用的方法，一般而言，当提取的规则能比较精确地反映语言现象时，基于规则的方法性能要优于基于统计的方法。但是这些规则往往依赖于具体语言、领域和文本风格，编制过程耗时且难以涵盖所有的语言现象，特别容易产生错误，系统可移植性不好，对于不同的系统需要语言学专家重新书写规则。基于规则的方法的另外一个缺点是代价太大，存在系统建设周期长、移植性差而且需要建立不同领域知识库作为辅助以提高系统识别能力等问题。</p>
<p>2.基于统计的方法</p>
<p>基于统计机器学习的方法主要包括：隐马尔可夫模型(HiddenMarkovMode,HMM)、最大熵(MaxmiumEntropy,ME)、支持向量机(Support VectorMachine,SVM)、条件随机场(ConditionalRandom Fields,CRF)等。</p>
<p>在这4种学习方法中，最大熵模型结构紧凑，具有较好的通用性，主要缺点是训练时间复杂性非常高，有时甚至导致训练代价难以承受，另外由于需要明确的归一化计算，导致开销比较大。而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架，但同时存在收敛速度慢、训练时间长的问题。一般说来，最大熵和支持向量机在正确率上要比隐马尔可夫模型高一些，但是隐马尔可夫模型在训练和识别时的速度要快一些，主要是由于在利用Viterbi算法求解命名实体类别序列的效率较高。隐马尔可夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用,如短文本命名实体识别。</p>
<p>基于统计的方法对特征选取的要求较高，需要从文本中选择对该项任务有影响的各种特征，并将这些特征加入到特征向量中。依据特定命名实体识别所面临的主要困难和所表现出的特性，考虑选择能有效反映该类实体特性的特征集合。主要做法是通过对训练语料所包含的语言信息进行统计和分析，从训练语料中挖掘出特征。有关特征可以分为具体的单词特征、上下文特征、词典及词性特征、停用词特征、核心词特征以及语义特征等。</p>
<p>基于统计的方法对语料库的依赖也比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少。</p>
<p>3.混合方法</p>
<p>自然语言处理并不完全是一个随机过程,单独使用基于统计的方法使状态搜索空间非常庞大，必须借助规则知识提前进行过滤修剪处理。目前几乎没有单纯使用统计模型而不使用规则知识的命名实体识别系统，在很多情况下是使用混合方法：</p>
<ol>
<li>统计学习方法之间或内部层叠融合。</li>
<li>规则、词典和机器学习方法之间的融合，其核心是融合方法技术。在基于统计的学习方法中引入部分规则，将机器学习和人工知识结合起来。</li>
<li>将各类模型、算法结合起来，将前一级模型的结果作为下一级的训练数据，并用这些训练数据对模型进行训练，得到下一级模型。</li>
</ol>
<p>这种方法在具体实现过程中需要考虑怎样高效地将两种方法结合起来，采用什么样的融合技术。由于命名实体识别在很大程度上依赖于分类技术,在分类方面可以采用的融合技术主要包括如Voting, XVoting, GradingVa, l Grading等。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。</p>
<p>奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。</p>
<p>作用是：</p>
<ol>
<li>数值上更容易求解；</li>
<li>特征数目太大时更稳定；</li>
<li>控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。</li>
<li>减小参数空间；参数空间越小，复杂度越低。</li>
<li>系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）。</li>
<li>可以看成是权值的高斯先验。</li>
</ol>
<h3 id="L1和L2正则的区别，如何选择L1和L2正则"><a href="#L1和L2正则的区别，如何选择L1和L2正则" class="headerlink" title="L1和L2正则的区别，如何选择L1和L2正则"></a>L1和L2正则的区别，如何选择L1和L2正则</h3><blockquote>
<p>他们都是可以防止过拟合，降低模型复杂度</p>
</blockquote>
<ul>
<li>L1 是在 loss function 后面加上模型参数的1范数（也就是|xi|）</li>
<li>L2 是在 loss function 后面加上模型参数的2范数（也就是<code>sigma(xi^2)</code>），注意L2范数的定义是 <code>sqrt(sigma(xi^2))</code>，在正则项上没有添加 sqrt 根号是为了更加容易优化</li>
<li>L1 会产生稀疏的特征</li>
<li>L2 会产生更多地特征但是都会接近于0</li>
</ul>
<p>L1 会趋向于产生少量的特征，而其他的特征都是 0，而 L2 会选择更多的特征，这些特征都会接近于 0。L1 在特征选择时候非常有用，而L2就只是一种规则化而已。</p>
<h2 id="过拟合-1"><a href="#过拟合-1" class="headerlink" title="过拟合"></a>过拟合</h2><p>一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好的拟合数据。此时我们就叫这个假设出现了 overfit 的现象。</p>
<p>过拟合产生的原因：</p>
<p>出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少。</p>
<p>预防或克服措施：</p>
<ol>
<li>增大数据量</li>
<li>减少 feature 个数（人工定义留多少个 feature 或者算法选取这些 feature）</li>
<li>正则化（留下所有的 feature，但对于部分 feature 定义其 parameter 非常小）</li>
<li>交叉验证</li>
</ol>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>对偶性是优化问题中一个非常重要的性质，它能够神奇地将许多非凸的优化问题转化成凸的问题</p>
<p>很多凸优化问题都是通过解对偶问题来求解的，线性规划只是其中一个特例而已。在求解一个规划问题（不限于线性规划）的时候，我们常常需要知道这个问题有没有可行解（有时候约束条件很复杂，不要说最优解，找到可行解都很难），或者是估计一下目前的解离最优解还有多远（大型问题多用迭代解法，如果能大致估计当前的解的质量，就对算法什么时候运行结束有一个大致的把握，如果得到了可接受的近似解也可以提前停止），以及判断原问题的最优解是否无界（万一出现这种情况迭代就停不下来了）。</p>
<p>而对偶问题就是回答这些问题的利器：弱对偶定理给原问题的最优解定了一个界，强对偶定理给出了原问题最优解的一个判定条件。同时，还有很多别的优良性质：例如可以化难为易（把难以求解的约束条件扔到目标函数的位置上去），如果问题的形式合适还可以通过把约束变量和对偶变量互换来把大规模问题转换成小规模问题。线性规划的对偶问题也只是其中的一个特例而已。并且，假如原问题是可行的，还可以给对偶问题找到一个直观解释（经济学中的影子价格）。</p>
<p>举个例子：</p>
 $$min_{x,y} \; px+qy\;\; s.t. x+y \ge 2,x \ge 0,y \ge 0$$
<p>我们令 $a+b = p, a+c=q$ 于是 $px+qy=a(x+y)+bx+cy \ge 2a,a \ge 0, b\ge0, c\ge0$</p>
<p>2a 为元问题的下界，于是我们可以求下面这个问题：</p>
 $$max_{a,b,c}\;2a\;\; s.t.\;a+b=p,a+c=q,a\ge0,b\ge0,c\ge0$$
<p>后一个问题的解正是前一个问题的解，这后面那个问题就叫做原问题的对偶问题。</p>
<h2 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h2><p>在机器学习中往往是最终要求解某个函数的最优值，但是一般情况下，任意一个函数的最优值求解比较困难，但是对于凸函数来说就可以有效的求解出全局最优值。</p>
<p><strong>凸集</strong></p>
<p>一个集合 C 是，当前仅当任意 x,y 属于 C 且 0&lt;=theta&lt;=1，都有 $\theta x+(1-\theta)y$ 属于C</p>
<p>用通俗的话来说C集合线段上的任意两点也在C集合中</p>
<p><strong>凸函数</strong></p>
<p>一个函数 f 其定义域(D(f))是凸集，并且对任意 x,y 属于 D(f) 和 0&lt;=theta&lt;=1 都有<br>$f(\theta x+(1-\theta)y)&lt;=\theta f(x)+(1-\theta)f(y)$ —这个叫做jensen不等式</p>
<p>用通俗的话来说就是曲线上任意两点的割线都在曲线的上方</p>
<p><strong>常见的凸函数有</strong></p>
<ul>
<li>指数函数 $f(x)=a^x$ a&gt;1</li>
<li>负对数函数  $-log_a x$ a&gt;1,x&gt;0</li>
<li>开口向上的二次函数等</li>
</ul>
<p><strong>凸函数的判定</strong></p>
<ol>
<li>如果 f 是一阶可导，对于任意数据域内的 x,y 满足 f(y)&gt;=f(x)+f’(x)(y-x)</li>
<li>如果f是二阶可导</li>
</ol>
<p><strong>凸优化应用举例</strong></p>
<ul>
<li>SVM：其中由 max|w| 转向 <code>min(1/2*|w|^2)</code></li>
<li>最小二乘法？</li>
<li>LR的损失函数 <code>sigma(yi*log(hw(x))+(1-yi)*(log(1-hw(x))))</code></li>
</ul>
<h2 id="特征向量"><a href="#特征向量" class="headerlink" title="特征向量"></a>特征向量</h2><h3 id="归一化方法"><a href="#归一化方法" class="headerlink" title="归一化方法"></a>归一化方法</h3><ol>
<li>线性函数转换，表达式如下：<code>y=(x-MinValue)/(MaxValue-MinValue)</code></li>
<li>对数函数转换，表达式如下：<code>y=log10 (x)</code></li>
<li>反余切函数转换 ，表达式如下：<code>y=arctan(x)*2/PI</code></li>
<li>减去均值，乘以方差：<code>y=(x-means)/ variance</code></li>
</ol>
<h3 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h3><p>用均值或者其他统计量代替</p>
<h2 id="ROC、AUC"><a href="#ROC、AUC" class="headerlink" title="ROC、AUC"></a>ROC、AUC</h2><p>ROC和AUC通常是用来评价一个二值分类器的好坏</p>
<h3 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h3><p>曲线坐标上：</p>
<ul>
<li>X 轴是 FPR（表示假阳率-预测结果为 positive，但是实际结果为 negitive，FP/(N)）</li>
<li>Y 轴式 TPR（表示真阳率-预测结果为 positive，而且的确真实结果也为 positive 的, TP/P）</li>
</ul>
<p>那么平面的上点(X,Y)：</p>
<ul>
<li>(0,1)表示所有的 positive 的样本都预测出来了，分类效果最好</li>
<li>(0,0)表示预测的结果全部为 negitive</li>
<li>(1,0)表示预测的错过全部分错了，分类效果最差</li>
<li>(1,1)表示预测的结果全部为 positive</li>
</ul>
<p>针对落在 x=y 上点，表示是采用随机猜测出来的结果</p>
<h3 id="ROC-曲线建立"><a href="#ROC-曲线建立" class="headerlink" title="ROC 曲线建立"></a>ROC 曲线建立</h3><p>一般默认预测完成之后会有一个概率输出 p，这个概率越高，表示它对 positive 的概率越大。</p>
<p>现在假设我们有一个 threshold，如果 p&gt;threshold，那么该预测结果为 positive，否则为 negative，按照这个思路，我们多设置几个 threshold,那么我们就可以得到多组 positive 和 negative 的结果了，也就是我们可以得到多组 FPR 和 TPR 值将这些 (FPR,TPR) 点投射到坐标上再用线连接起来就是 ROC 曲线了</p>
<p>当threshold取1和0时，分别得到的就是(0,0)和(1,1)这两个点。（threshold=1，预测的样本全部为负样本，threshold=0，预测的样本全部为正样本）</p>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p>AUC(Area Under Curve) 被定义为 ROC 曲线下的面积，显然这个面积不会大于1（一般情况下 ROC 会在 x=y 的上方，所以0.5&lt;AUC&lt;1）.</p>
<p>AUC越大说明分类效果越好</p>
<h3 id="为什么要使用ROC和AUC"><a href="#为什么要使用ROC和AUC" class="headerlink" title="为什么要使用ROC和AUC"></a>为什么要使用ROC和AUC</h3><p>因为当测试集中的正负样本发生变化时，ROC 曲线能基本保持不变，但是 precision 和 recall 可能就会有较大的波动。</p>
<h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>在信息论中，熵表示的是不确定性的量度。信息论的创始人香农在其著作《通信的数学理论》中提出了建立在概率统计模型上的信息度量。他把信息定义为”用来消除不确定性的东西“。熵的定义为信息的期望值。</p>
<p>在信息论和概率统计中，熵是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为：</p>
 $$P(X=x_i)=p_i, i=1,2, ... , n$$
<p>则随机变量X的熵定义为：<br> $$H(X)= - \sum p_ilogp_i, i=1,2, ... , n$$</p>
<p>熵只依赖 X 的分布，和 X 的取值没有关系，熵是用来度量不确定性，当熵越大，概率说  $X=x_i$ 的不确定性越大，反之越小，在机器学期中分类中说，熵越大即这个类别的不确定性更大，反之越小。</p>
<p>当 p=0 或 p=1 时，H(p)=0，随机变量完全没有不确定性，当p=0.5时，H(p)=1,此时随机变量的不确定性最大。</p>
<p>条件熵(conditional entropy)：表示在一直随机变量X的条件下随机变量Y的不确定性度量。</p>
<p>设随机变量(X, Y)，其联合概率分布为  $P(X, Y) = p_{ij}(i=1,2, ... , n; j=1,2, ... , m)$，随机变量 X 给定的条件下随机变量Y的条件熵 H(Y|X)，定义为 X 给定条件下 Y 的条件概率分布的熵对 X 的数学期望：</p>
 $$H(Y|X)=\sum p_iH(Y|X=x_i)$$
<p>这里， $p_i=P(X=x_i), i=1,2, ... , n$</p>
<p>熵指的是体系的混乱程度，它在控制论，概率论，数论，天体物理，生命科学等领域都有重要的应用，在不同的学科中也有引申出更为具体的定义，是各个领域十分重要的参量。熵由鲁道夫.克劳修斯提出，并应用在热力学中。后来在，克劳德.埃尔伍德.香农 第一次将熵的概念引入到信息论中来。</p>
<h3 id="信息增益（information-gain）"><a href="#信息增益（information-gain）" class="headerlink" title="信息增益（information gain）"></a>信息增益（information gain）</h3><p>信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。</p>
<p>特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为集合 D 的经验熵 H(D)与特征 A 给定条件下 D 的经验条件熵 H(D|A)之差，即</p>
<p>g(D, A)=H(D)-H(D|A)</p>
<p>信息增益大的特征具有更强的分类能力。</p>
<h3 id="信息增益比（information-gain-ratio）"><a href="#信息增益比（information-gain-ratio）" class="headerlink" title="信息增益比（information gain ratio）"></a>信息增益比（information gain ratio）</h3><p>信息增益比  $g_R(D, A)$定义为其信息增益 g(D, A) 与训练数据集 D 关于特征 A 的值的熵  $H_A(D)$之比，即</p>
 $g_R(D, A)=g(D, A)/H_A(D)$
<p>其中， $H_A(D)=-\sum \frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$, n 是特征 A 取值的个数。</p>
<h3 id="基尼指数（gini-index）"><a href="#基尼指数（gini-index）" class="headerlink" title="基尼指数（gini index）"></a>基尼指数（gini index）</h3><p>分类问题中，假设有 K 个类，样本属于第 k 类的概率为 $p_k$，则概率分布的基尼指数定义为：</p>
 $$Gini(p)=\sum p_k(1-p_k)=1-\sum p_k^2$$
<p>对于二分类问题，若样本点属于第 1 个类的概率是 p，则概率分布的基尼指数为：</p>
<p>Gini(p)=2p(1-p)</p>
<p>对于给定的样本集合D，其基尼指数为：</p>
 $$Gini(D)=1-\sum(\frac{|C_k|}{|D|})^2$$
<p>这里， $C_k$是 D 中属于第 k 类的样本子集，k 是类的个数。</p>
<p>如果样本集合 D 根据特征 A 是否取到某一可能值 a 被分割成 D1 和 D2 两部分，则在特征 A 的条件下，集合 D 的基尼指数定义为：</p>
 $$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$
<p>基尼指数 Gini(D) 表示集合 D 的不确定性，基尼指数越大，样本集合的不确定性也就越大，这一点与熵相似。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是我学习机器学习时候的一些笔记和总结，因为时间比较长了，参考链接都遗失了，在这里感谢各位老司机在我学习过程中给我的帮助。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="总结" scheme="http://wdxtub.com/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="机器学习" scheme="http://wdxtub.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="知识点" scheme="http://wdxtub.com/tags/%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    
  </entry>
  
  <entry>
    <title>第十三周 - 上海三月</title>
    <link href="http://wdxtub.com/2016/09/10/march-in-shanghai/"/>
    <id>http://wdxtub.com/2016/09/10/march-in-shanghai/</id>
    <published>2016-09-09T16:15:35.000Z</published>
    <updated>2016-09-09T17:06:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>甜甜莓茶醉，悠悠春雨喂，问寻难走心中一片白；风飞进，灯光残，忘却茶香独生叹，唯有时光难消亡。</p>
<a id="more"></a>
<hr>
<p>这周工作特别忙，三个项目并行，虽然只有其中一个我是负责人，但是另外俩需要开会沟通协调催促，不得不占据我许多时间，实话说，还不如写代码来得轻松愉快。不过借此机会认识了不少不同部门的同事，找机会还是可以多去请教下各方面的知识，也是挺不错的。</p>
<p>事情一多，原来习惯的依赖记忆和感觉来做事儿的方法已经不堪重负。虽然也还没有溢出，但是我依然不喜欢这种无头苍蝇哪个急做哪个的做事方式，所以也开始用任务管理软件了。就这两天的体验来说，非常不错，至少各个项目的需求能够一一理清并且完成了（实在完成不了的就往后排期嘛）。</p>
<p>因为写代码的时间非常宝贵，所以我会在设计上花更多的时间，争取每一行代码都发挥真正的价值，而不是拍脑袋设计最终导致成倍时间的浪费。不过在公司里写代码，有的时候直接给出没什么 bug 线上可以一直稳定运行的程序反而会让不明真相的人觉得你的工作没什么技术含量，因为一切都显得理所当然。奇怪的是，那些线上整天出问题，大家忙忙叨叨不过是为了把本应该一次做好的工作做好，在不懂的人看来竟是『上心』和『努力』的象征，这就有点搞笑了。</p>
<p>所以说一个好的技术人员不但应该技术做得好，更应该让别人知道之所以线上服务能够快速上线并且不出问题不是因为问题简单，而是因为经验积累和全面思考得到的解决方案靠谱，更不是天天低效加班做那些『看起来很努力』的事情能够比得上的。如果不扭转这种外行看内行的心态，恐怕只会劣币驱逐良币，最终大家事不关己高高挂起，毕竟人往高处走嘛。</p>
<p>最近两周，在我负责的项目上，平时对接的美国团队基本撒手不管了。这™就很尴尬了，从需求到构思设计，从开发到部署测试，一人分饰多角。不过这样也好，沟通成本基本为零了，我也很注意文档和注释的编写，一个人扛就一个人扛呗，唯一影响的可能就是项目进度，毕竟在质量上我还是对自己有要求的。</p>
<p>周中原来的 CMU 同学从美国回来办事儿，一起在公司附近吃了一顿饭，有朋自远方来，不亦乐乎。毕业之后老同学再想见面真的很需要缘分，只是希望能聚的时候，不要因为懒或者天气而轻易放弃。这段时间在公司里也交到了不少新朋友，情投意合且都有强迫症洁癖的同事合作起来非常轻松愉快，能够感受到的技术积累和深入思考，三人行必有我师，要以老司机为榜样，努力提升自己。</p>
<p>说到这个，这周起也开始了自己在代码上比较系统的技术积累（之前以写博客居多），也参考各种技能树大概制定了自己技术发展的路径。这种又有新挑战开启的感觉，既刺激又紧张还能收获很多，想想还有点小激动呢。希望能在一年之内取得让自己满意的成绩。</p>
<p>制定目标 -&gt; 努力训练 -&gt; 坚持不懈 -&gt; 最终突破，这其实是非常靠谱的成长方式，这周我的跑步目标终于初步达成，配速终于在四分三十秒之内了，比大学和研究生的时候每公里都快了一分多钟。看到自己成绩的时候，真的有些不可思议，原本以为要到今年年底才能达到的速度，居然提前三个多月完成了，我算了一下，只要每三步比原来的自己快一秒，每公里所花费的时间就可以少一分钟。不要想着目标有多大有多难，专注每一次呼吸和每一次步伐，一直向前就好了。</p>
<p>鞋脏了又如何，袜子湿了又如何，刮风下雨又如何？大国重器，真正的成绩，真正的改变，就是要经得起各种考验。</p>
<p>下周就要转正了，很高兴能在这个关头完成新的改变，我也做好了所有的准备，等待真正成长起来的那一天。</p>
<p>是雨潺潺，不问窗外寒；孤衾影，长夜莫过知己难，往事已故此景谁还在；世事漫随流水，算来一梦浮生。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;甜甜莓茶醉，悠悠春雨喂，问寻难走心中一片白；风飞进，灯光残，忘却茶香独生叹，唯有时光难消亡。&lt;/p&gt;
    
    </summary>
    
      <category term="Gossip" scheme="http://wdxtub.com/categories/Gossip/"/>
    
    
      <category term="朋友" scheme="http://wdxtub.com/tags/%E6%9C%8B%E5%8F%8B/"/>
    
      <category term="周记" scheme="http://wdxtub.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>个人技能发展指南</title>
    <link href="http://wdxtub.com/2016/09/09/wdx-skill-set/"/>
    <id>http://wdxtub.com/2016/09/09/wdx-skill-set/</id>
    <published>2016-09-08T22:28:58.000Z</published>
    <updated>2016-09-09T15:08:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>工作也有一段时间了，基本上也参与到了公司除硬件外的各条业务与研发线，在具体的学习和开发过程中也逐渐有了对未来的认知。这里以技能树的方式来给自己定简单的规划。</p>
<a id="more"></a>
<hr>
<p>本文的主要技能点来自 StuQ 的云计算工程师/研发工程师/大数据工程师必备技能这三种，按照我个人的喜欢和判断进行了糅合。</p>
<h2 id="开发"><a href="#开发" class="headerlink" title="开发"></a>开发</h2><ul>
<li>命令行工具: tmux(screen), vim, zsh(oh-my-zsh), git</li>
<li>操作系统: Ubuntu, macOS</li>
<li>语言: Go, Java(maven, gradle), Python(pip, ipython), Javascript(node.js), Ruby(gem)</li>
<li>文档: markdown</li>
<li>编辑器: Visual Studio Code</li>
<li>流程: Scrum, Crystal, FDD</li>
<li>持续集成: Jenkins</li>
<li>协作: Teambition, Slack, Trello</li>
</ul>
<h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><ul>
<li>数据结构: JSON, cPickle, protobuf</li>
<li>网络: TCP/IP, VLANs, DNS, CDN, HTTP/HTTPS 协议</li>
<li>调度<ul>
<li>crontab 最原生的定时调度</li>
<li>基于 redis 实现的分布式调度</li>
<li>基于 rpyc 实现的分布式调度</li>
<li>celery/gearman 等调度</li>
</ul>
</li>
<li>并发<ul>
<li>协程 gevent</li>
<li>线程池</li>
<li>多进程 os.fork, idea multiprocessing</li>
</ul>
</li>
<li>调试<ul>
<li>pdb, logging, Sentry, lsof, strace, trace</li>
<li>top, htop, free, iostat, vmstat, ifconfig, iftop</li>
</ul>
</li>
<li>算法<ul>
<li>一致性: Paxos, Raft, Gossip</li>
<li>数据结构: 栈、队列、链表, 散列表, 二叉树、红黑树、B 树, 图</li>
<li>常用算法: 插入排序, 桶排序, 堆排序, 快速排序, 最大子数组, 最长公共子序列, 最小生成树, 最短路径, 矩阵的存储和运算</li>
</ul>
</li>
</ul>
<h2 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h2><ul>
<li>云计算: SaaS/PaaS/Iaas, Openstack, Docker</li>
<li>大数据通用处理平台: Spark, Hadoop, ELK</li>
<li>资源调度: Yarn, Mesos</li>
<li>SQL: MySQL, Sqlite, AWS RDS, PostgreSQL </li>
<li>NoSQL: MongoDB, Cassandra, DynamoDB, MongoDB, HBase</li>
<li>缓存: Memcached, Redis, AWS ElastiCache</li>
<li>检索: Solr, ElasticSearch, AWS ElasticSearch </li>
<li>数据分析: Pig, Hive, Spark SQL, Spark DataFrame, Impala, Phoenix,  ELK</li>
<li>消息队列: Kafka, RocketMQ, ZeroMQ, ActiveMQ, RabbitMQ</li>
<li>流式计算: Storm/JStorm, Spark Streaming, AWS Kinesis</li>
<li>日志收集: ELK, Scribe, Flume, Fluentd, AWS CloudTrail</li>
<li>机器学习: Mahout, Spark Mlib, TensorFlow(Google), Amazon Machine Learning, DMTK(MS), scikit learn</li>
</ul>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><ul>
<li>计算<ul>
<li>自动扩展: AWS Autoscaling, OpenStack/Hoo!</li>
<li>负载均衡: AWS ELB, HAProxy, Nginx</li>
<li>虚拟化: Hypervisor, Xen, KVM, Hyper-V</li>
<li>容器: Docker, CoreOS, UnixLXC</li>
</ul>
</li>
<li>分布式消息<ul>
<li>消息队列: ZeroMQ, ActiveMQ, RabbitMQ, AWS SQS</li>
<li>事件/消息驱动: AWS SWS, AWS Lambda, AKKA</li>
<li>RPC: Thrift, Protobuf</li>
</ul>
</li>
<li>存储<ul>
<li>网络存储: AWS EBS, NFS v4, Ceph, Apache CloudStack</li>
<li>对象存储: AWS S3, OpenStack Swift</li>
<li>块存储: SAN, AWS EBS, RAID 概念</li>
<li>灾难恢复</li>
<li>文件系统: ext4, XFS</li>
</ul>
</li>
<li>安全: Firewall, DDoS, iptables, WAF, IDS/IPS, VPN</li>
<li>身份认证: SAML, OpenID, Microsoft AD, AWS IAM</li>
<li>监控: ZABBIX, OBSERVIUM, INICGA, AWS CloudWatch<ul>
<li>系统, 日志, 流量, 接口, 数据库</li>
</ul>
</li>
<li>理论: Microservices, RESTful, CAP</li>
<li>设计: 扩展性, 可用性, 可靠性, 一致性, 负载均衡, 过载保护</li>
<li>协议: 二进制协议, 文本协议</li>
<li>接入层: DNS 轮训, 动静态分离, 静态化, 反向代理, LVS, F5, CDN<ul>
<li>nginx, apache, lighttpd, tomcat</li>
</ul>
</li>
<li>逻辑层: 连接池, 串行化, 批量写入, 配置中心, 去中心化</li>
<li>数据层: 缓存优化, DAO, ORM, 双主架构, 主从同步, 读写分离</li>
<li>同步通讯: RPC, RMI</li>
<li>异步通讯: MQ, Cron</li>
<li>性能优化<ul>
<li>代码层: 关联代码优化, cache 对齐, 分之预测, Copy on Write, 内联优化</li>
<li>工具: OProfile, Gprof, JDK 工具</li>
<li>系统优化: 缓存, 延迟计算, 数据预读, 异步, 轮询与通知, 内存池, 模块化</li>
</ul>
</li>
<li>测试: 单元测试, 接口测试, 性能测试, 集成测试</li>
</ul>
<h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><ul>
<li>核心: Docker, LXC, RunC, Rkt, Systemd-nspawn, Hyper, VMWare Photon, Jetpack, Kurma, Bosh</li>
<li>监控: Sysdig, Guardrail, cAdvisor</li>
<li>管理: DockerUI, Kitematic, Decking.io, Shipyard, StackEngine, Panamx, Fabric8, Triton</li>
<li>基础设施集成: Nova-docker, Magnum, Clocker, Machine, MaestroNG, CloudFoundry Containers Service Broker, Mesos, Fit2Cloud, Boot2Docker</li>
<li>编排调度: Crane, Compose, Swarm, Yarn, Kubernets, Fleet, Marathon, OpenShift, GearD, Rancher</li>
<li>平台: Alauda, DaoCloud, TenxCloud, CSphere, AWS Container Service, Google Container Engine, StackDock, Orchard, Quay.io, Baremetal.io, Tutum, Giant Swarm</li>
<li>服务发现: Consul, Etcd, Zookeeper, SkyDNS, Skydock</li>
<li>日志收集: Splunk, Elasticsearch, Logstach, Kibana, Heka, Fluent, Flume</li>
<li>相关发行版: CoreOS, Project Atomic, RancherOS, ClearLinux</li>
<li>容器 PaaS: Dokku, Deis, Voxoz, Flynn, Octohost</li>
<li>容器网络: Pipework, Flannel, Calico, Weave, Socketplane.io, Pertino, Nuage</li>
<li>容器安全: Notary, SELinux on docker</li>
<li>数据持久化: Flocker, Ceph</li>
<li>开发流程工具: Drone.io, Shippable, Runnable, NodeChecker, Jenkins Docker plugin, Wercker, Totem, Packet, Docker Repository, Packer</li>
</ul>
<h2 id="运维"><a href="#运维" class="headerlink" title="运维"></a>运维</h2><ul>
<li>DevOps: SSH 证书, Fabric, SaltStack, puppet, pssh/dsh, 运维进阶</li>
<li>部署: 蓝绿部署, 灰度发布, 金丝雀发布, Canary 部署, PHOENIX 部署, AWS CloudFormation</li>
<li>基础服务: LAMP/LNMP, FTP, DNS, SAMBA, EMAIL, NTP, DHCP</li>
<li>配置: Chef, Puppet, Ansible, AWS OpsWorks, Nagios, Zabbix, Cacti, SaltStack, pssh/dsh, Fabric</li>
<li>安全: iptables, ipset</li>
<li>网络: TCP/IP, tcpdump</li>
</ul>
<p><img src="/images/14734336749159.jpg" alt=""></p>
<p>引用自 <a href="http://www.brendangregg.com/linuxperf.html" target="_blank" rel="external">Brendan Gregg <linux performance="" analysis="" and="" tools=""></linux></a></p>
<h2 id="前端"><a href="#前端" class="headerlink" title="前端"></a>前端</h2><ul>
<li>语言: JavaScript/Node.js, TypeScript</li>
<li>编辑器: Vim, Visual Studio Code</li>
<li>调试工具: Chrome Dev Tools</li>
<li>框架: Vue.js, React, jQuery, Botostrap</li>
<li>规范: HTTP/1.1 RFCs 7230-7235, HTTP/2, ECMAScript 5/6/7, DOM/BOM/XHTML/XML/JSON/JSONP, CommonJS Modules, MicroData/RDFa</li>
<li>文档: JSDoc, Dox/Doxmate/Grunt-Doxmate</li>
<li>构建工具: make/ant, GYP, Grunt, Gulp, Yeoman, FIS, Mod, Webpack</li>
<li>安全: CSRF/XSS, CSP, Same-origin policy, ADsafe/Caja/Sandbox</li>
<li>移动: HTML5/CSS3, 响应式网页设计, Zeptojs/iScroll, React Native/Week</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://github.com/TeamStuQ/skill-map" target="_blank" rel="external">StuQ 程序员技能图谱</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;工作也有一段时间了，基本上也参与到了公司除硬件外的各条业务与研发线，在具体的学习和开发过程中也逐渐有了对未来的认知。这里以技能树的方式来给自己定简单的规划。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="云计算" scheme="http://wdxtub.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="技能树" scheme="http://wdxtub.com/tags/%E6%8A%80%E8%83%BD%E6%A0%91/"/>
    
      <category term="研发" scheme="http://wdxtub.com/tags/%E7%A0%94%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Nginx 指南</title>
    <link href="http://wdxtub.com/2016/09/08/nginx-guide/"/>
    <id>http://wdxtub.com/2016/09/08/nginx-guide/</id>
    <published>2016-09-08T12:54:54.000Z</published>
    <updated>2016-09-08T12:56:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近需要把负责的项目部署到云端，采用了现在业界流行的 Nginx 反向代理方案，这样后端的 Go 程序只需要专注于业务逻辑和功能。本文记录具体的部署方案。</p>
<a id="more"></a>
<hr>
<h2 id="手动安装"><a href="#手动安装" class="headerlink" title="手动安装"></a>手动安装</h2><p>虽然云端环境是 Ubuntu 14.04，不过为了后面维护的方便，决定不采用 <code>apt-get</code> 而是手动进行安装。整个过程我已经配置成为了一个脚本，接下来通过讲解脚本来介绍具体安装的过程</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 运行之前需要切换到 root 用户</span></div><div class="line">serviceDir=/data/home/username</div><div class="line"></div><div class="line"><span class="comment"># 安装配置依赖，这里直接用 apt-get</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Installing make g++"</span></div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span></div><div class="line">apt-get install make</div><div class="line">apt-get install g++</div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Done(1/7)"</span></div><div class="line"></div><div class="line"><span class="comment"># 安装 openssl，其中 openssl-1.0.2 是长期支持版本，所以我采用这个版本</span></div><div class="line"><span class="comment"># 更多信息请访问 https://www.openssl.org/source/</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Installing openssl"</span> </div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span></div><div class="line">wget https://www.openssl.org/<span class="built_in">source</span>/openssl-1.0.2h.tar.gz</div><div class="line">tar -xzvf openssl-1.0.2h.tar.gz</div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span>/openssl-1.0.2h</div><div class="line">./config</div><div class="line">make</div><div class="line">make install</div><div class="line">ldconfig</div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Done(2/7)"</span></div><div class="line"></div><div class="line"><span class="comment"># 安装 Pcre，为了保证兼容我们这里使用较老的版本</span></div><div class="line"><span class="comment"># 源用的是 stanford 的（因为 pcre.org 我这里打不开）</span></div><div class="line"><span class="comment"># 源：http://ftp.cs.stanford.edu/pub/exim/pcre/</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Installing pcre"</span></div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span></div><div class="line">wget http://ftp.cs.stanford.edu/pub/exim/pcre/pcre-8.37.tar.gz</div><div class="line">tar -xzvf pcre-8.37.tar.gz</div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span>/pcre-8.37</div><div class="line">./configure</div><div class="line">make</div><div class="line">make install</div><div class="line">ldconfig</div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Done(3/7)"</span></div><div class="line"></div><div class="line"><span class="comment"># 安装 zlib，用的就是最新的 1.2.8</span></div><div class="line"><span class="comment"># 源 http://zlib.net/zlib-1.2.8.tar.gz</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Installing zlib"</span></div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span></div><div class="line">wget http://zlib.net/zlib-1.2.8.tar.gz</div><div class="line">tar -xzvf zlib-1.2.8.tar.gz</div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span>/zlib-1.2.8</div><div class="line">./configure</div><div class="line">make</div><div class="line">make install</div><div class="line">ldconfig</div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Done(4/7)"</span></div><div class="line"></div><div class="line"><span class="comment"># 安装 Nginx</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Installing Nginx"</span></div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span></div><div class="line">wget https://nginx.org/download/nginx-1.10.1.tar.gz</div><div class="line">tar -xzvf nginx-1.10.1.tar.gz</div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span>/nginx-1.10.1</div><div class="line">./configure --prefix=<span class="variable">$serviceDir</span>/nginx-server --with-openssl=<span class="variable">$serviceDir</span>/openssl-1.0.2h --with-http_ssl_module --with-http_stub_status_module --with-stream</div><div class="line">make</div><div class="line">make install</div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span>/nginx-server/conf</div><div class="line">rm -rf nginx.conf</div><div class="line"><span class="comment"># 这里用的是已经配置好的配置文件</span></div><div class="line">wget http://xssz.oss-cn-shenzhen.aliyuncs.com/server_software/nginx.conf</div><div class="line">ln <span class="_">-s</span> <span class="variable">$serviceDir</span>/nginx-server/sbin/nginx /usr/<span class="built_in">local</span>/bin/nginx</div><div class="line">mkdir <span class="variable">$serviceDir</span>/nginx-server/run</div><div class="line">mkdir <span class="variable">$serviceDir</span>/nginx-config</div><div class="line">ln <span class="_">-s</span> <span class="variable">$serviceDir</span>/nginx-server/sbin/nginx /usr/<span class="built_in">local</span>/bin/nginx</div><div class="line">nginx -c <span class="variable">$serviceDir</span>/nginx-server/conf/nginx.conf</div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Done(5/7)"</span></div><div class="line"></div><div class="line"><span class="comment"># 安装守护</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Config daemon"</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$serviceDir</span><span class="string">'/nginx-server/sbin/nginx -c '</span><span class="variable">$serviceDir</span><span class="string">'/nginx-server/conf/nginx.conf'</span> &gt; /etc/rc.local</div><div class="line"><span class="built_in">echo</span> <span class="string">'exit 0'</span> &gt;&gt; /etc/rc.local</div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Done(6/7)"</span></div><div class="line"></div><div class="line"><span class="comment"># 清理工作，把所有的安装包保存到 software 文件夹中</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Clean up all the mess"</span></div><div class="line"><span class="built_in">cd</span> <span class="variable">$serviceDir</span></div><div class="line">mkdir <span class="variable">$serviceDir</span>/software</div><div class="line">mv *.gz <span class="variable">$serviceDir</span>/software</div><div class="line"><span class="built_in">echo</span> <span class="string">"[INFO] Done(7/7)"</span></div><div class="line"></div><div class="line"><span class="built_in">echo</span> <span class="string">"All Done. You can now continue your work."</span></div></pre></td></tr></table></figure>
<p>注释应该已经写得比较清楚了，这里就不再赘述</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>配置的部分，基本上根据默认的指引就可以完成基本的配置。这里介绍一下我从老司机那里学来的方法，把公共部分和自定义部分分离，看起来更加清晰。</p>
<p>首先，因为是手动安装的 nginx 的，默认的 nginx 配置在 <code>~/nginx-server/conf/nginx.conf</code> 中，我们来看看里面的条目：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">user www-data;</div><div class="line">worker_processes 1;</div><div class="line">worker_rlimit_nofile 262140;</div><div class="line">worker_cpu_affinity 1;</div><div class="line">error_log logs/error.log;</div><div class="line">pid run/nginx.pid;</div><div class="line"></div><div class="line">events</div><div class="line">&#123;</div><div class="line">    use epoll;</div><div class="line">    worker_connections 65535;</div><div class="line">&#125;</div><div class="line"></div><div class="line">http</div><div class="line">&#123;</div><div class="line">    include mime.types;</div><div class="line">    default_type application/octet-stream;</div><div class="line">    </div><div class="line">    sendfile on;</div><div class="line">    #aio on;</div><div class="line">    directio 512;</div><div class="line">    output_buffers 1 128k;</div><div class="line">    log_not_found off;</div><div class="line">    keepalive_timeout 65;</div><div class="line">    server_tokens off;</div><div class="line">    </div><div class="line">    gzip on;</div><div class="line">    gzip_comp_level 6;</div><div class="line">    gzip_min_length 1k;</div><div class="line">    gzip_buffers 4 8k;</div><div class="line">    gzip_disable &quot;MSIE [1-6]\.(?!.*SV1)&quot;;</div><div class="line">    gzip_types text/plain application/x-javascript text/css application/xml text/javascript application/javascript application/json;</div><div class="line">    </div><div class="line">    log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot; &quot;$request_time&quot; &quot;$upstream_response_time&quot;&apos;;</div><div class="line">    access_log logs/$&#123;server_name&#125;.access.log main;</div><div class="line">    fastcgi_intercept_errors on;</div><div class="line">    error_page 500 502 503 504 /50x.html;</div><div class="line">    </div><div class="line">    server_names_hash_max_size 4096;</div><div class="line">    </div><div class="line">    server</div><div class="line">    &#123;</div><div class="line">        listen 80 default;</div><div class="line">        server_name _;</div><div class="line">        access_log off;</div><div class="line">        </div><div class="line">        location /</div><div class="line">        &#123;</div><div class="line">            return 403;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    include /data/home/user/nginx-config/*.http;</div><div class="line">&#125;</div><div class="line"></div><div class="line">stream&#123;</div><div class="line">    include /data/home/user/nginx-config/*.tcp;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>前面的都不重要，重要的是最后两句 <code>include</code> 语句，这里我们就把需要自己配置的部分抽取了出来，放到了文件夹 <code>nginx-config</code> 中。</p>
<p>接着，只要我们在 <code>nginx-config</code> 文件夹中，针对不同的域名和应用进行配置即可，比方说我的 Go 应用跑在本机的 12345 端口上，那么可以这么配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">upstream http_pool&#123;</div><div class="line">    server 127.0.0.1:12345 weight=1 max_fails=3 fail_timeout=30s;</div><div class="line">&#125;</div><div class="line"></div><div class="line">server&#123;</div><div class="line">    server_name test.xxx.com;</div><div class="line">    listen 80;</div><div class="line">    ssl off;</div><div class="line">    </div><div class="line">    gzip on;</div><div class="line">    gzip_min_length 1k;</div><div class="line">    gzip_buffers 16 64k;</div><div class="line">    gzip_comp_level 9;</div><div class="line">    gzip_types text/plain text/css application/json application/x-javascript application/xml application/xml+rss text/javascript application/atom+xml;</div><div class="line">    gzip_vary on;</div><div class="line">    </div><div class="line">    location /</div><div class="line">    &#123;</div><div class="line">        proxy_next_upstream http_404 http_502 http_504 http_500 error timeout invalid_header;</div><div class="line">        proxy_pass http://http_pool;</div><div class="line">        proxy_set_header X-Forwarded-For $remote_addr;</div><div class="line">        proxy_set_header Host $http_host;</div><div class="line">        client_max_body_size 5000k;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这样我们就完成了从 test.xxx.com 到 127.0.0.1:12345 的代理了。注意，上面的配置文件有很多地方需要优化，这里只是一个简单的示例。关于具体怎么优化还有很多需要做的，会在之后慢慢更新。</p>
<p>关于 Nginx 的进阶配置，参考链接中的三篇系列文章写得非常好，这里我就简单带过，需要的同学可以前往继续研究。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>Nginx 的使用我比较常用的其实在 <code>nginx -h</code> 中已经有介绍，一般来说我就用一个命令 <code>nginx -s [stop|quit|reopen|reload]</code>，其实也就是 <code>sudo nginx -s reload</code></p>
<p>另外有一个需要注意的地方就是，在 nginx 中配置的 80(http) 和 8080(tcp) 端口不能被占用，不然会一直冲突。另外需要注意配置路径的时候不要弄错了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Nginx 是我一直以来不太熟悉的，借着这次机会能从安装到部署实战一下，还有老司机指导，感觉还是很不错的。相比于 Apache 这种重量级选手，Nginx 的灵活轻便让我非常钟意，就像 Go 一样。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://www.openssl.org/" target="_blank" rel="external">OpenSSL</a></li>
<li><a href="http://ftp.cs.stanford.edu/pub/exim/pcre/" target="_blank" rel="external">PCRE 源</a></li>
<li><a href="http://www.zlib.net/" target="_blank" rel="external">Zlib</a></li>
<li><a href="https://nginx.org/en/download.html" target="_blank" rel="external">Nginx</a></li>
<li><a href="https://www.zybuluo.com/phper/note/89391" target="_blank" rel="external">nginx的配置、虚拟主机、负载均衡和反向代理（1）</a></li>
<li><a href="https://www.zybuluo.com/phper/note/90310" target="_blank" rel="external">nginx的配置、虚拟主机、负载均衡和反向代理（2）</a></li>
<li><a href="https://www.zybuluo.com/phper/note/133244" target="_blank" rel="external">nginx的配置、虚拟主机、负载均衡和反向代理（3）</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近需要把负责的项目部署到云端，采用了现在业界流行的 Nginx 反向代理方案，这样后端的 Go 程序只需要专注于业务逻辑和功能。本文记录具体的部署方案。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="服务器" scheme="http://wdxtub.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
      <category term="反向代理" scheme="http://wdxtub.com/tags/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/"/>
    
      <category term="Nginx" scheme="http://wdxtub.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>GeoHash 指南</title>
    <link href="http://wdxtub.com/2016/09/07/geohash-guide/"/>
    <id>http://wdxtub.com/2016/09/07/geohash-guide/</id>
    <published>2016-09-07T12:17:34.000Z</published>
    <updated>2016-09-07T12:19:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>做任何跟地理位置相关的服务，位置如何表示及存储是绝对绕不开的问题之一。位置的表示倒是可以用经纬度，但是索引和检索的时候，经纬度这种二维表示法就比较麻烦了，这时我们就可以利用 GeoHash 进行『降维攻击』来解决这个问题了。</p>
<a id="more"></a>
<hr>
<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>前几年 LBS 的特性像一股龙卷风一样席卷了整个 App 圈，任何应用都迫不及待地加入了基于地理位置的相关特性。这之中一个非常火的功能就是——查看附近的人/地点/事情，比方说查看附近的餐馆、景点、朋友等等。那么问题就来了，怎么判断是不是附近呢？怎么样定义这个『附近』呢？</p>
<p>就用『附近的人』这个功能来举例，假如我要找自己身边的人，最简单粗暴的办法就是把我跟所有人的距离算一次，然后选一个阈值，在这个阈值范围内的，认为是『附近』。但是问题来了，如果我们的数据库中有一亿人，那不是每次都要计算一亿次？我们得想个办法减少计算量。</p>
<p>因为知道自己的经纬度，所以可以知道自己在哪里，比方说深圳市南山区，那么我只需要计算同在南山区的人即可，考虑到我可能在边界上，那么多加周边的几个区进行计算即可。这样一来就可以过滤掉大部分的无用计算了。</p>
<p>这种方式有一个问题，就是需要很多额外的信息，比方说我得知道南山区，不同国家不同地区各种区域划分，而且有时候区域还会变化（比方说萝岗和南沙并入广州），这样就引入了许多不必要的复杂度。不过即使如此，这种利用特定区域划分来减少计算范围的方法，非常有借鉴意义，类似于搜素剪枝，也就是索引。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>因为要使用索引的思想，那么就需要确定拿来建立索引的字段，可是经纬度这种二维的数据很难通过一维索引来高效检索，于是我们可以利用 GeoHash 来进行转换。</p>
<p>不过在此之前，我们先来看看另一个概念 —— <a href="https://zh.wikipedia.org/wiki/Trie" target="_blank" rel="external">字典树 Trie</a>。</p>
<blockquote>
<p>在计算机科学中，trie，又称前缀树或字典树，是一种有序树，用于保存关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。一般情况下，不是所有的节点都有对应的值，只有叶子节点和部分内部节点所对应的键才有相关的值。</p>
</blockquote>
<p>这里我们需要在意的就是一点，相同父节点数目越多的子节点，从词的角度上相似度越高。接下来的 GeoHash 算法就部分满足这种特性。</p>
<p>在介绍具体的算法之前，我们先从较高层级来理解 GeoHash:</p>
<ul>
<li>GeoHash 可以把经纬度转换成一个字符串，把二维变成一维</li>
<li>每个 GeoHash 出来的字符串表示的是一个矩形区域，虽然不够精确，但是一定程度上反而能够保护隐私。Hash 值越长，表示的区域越小</li>
<li>越往左的编码表示的范围越大，可以利用这个特性来缩小或扩大检索范围</li>
</ul>
<p>比方说，北京城区的 GeoHash 大概是这样的（本文部分图片来源<a href="http://www.cnblogs.com/LBSer" target="_blank" rel="external">这里</a>，感谢作者制图）：</p>
<p><img src="/images/14732507518837.jpg" alt=""></p>
<p>可以看到，每个格子都有一个编号，具体编号的顺序也有具体的算法，比较常用的是 Peano 曲线、Hilbert 曲线和 Z-order 曲线。</p>
<p><img src="/images/14732507430572.jpg" alt=""></p>
<p>至于选择哪种算法，在地理位置相关的应用中，主要考虑的是实现的难度和区域的突变特性。这里以 Peano 填充曲线来做例子，具体的编码方式是从左下角开始的，然后逐步进行递归细化。</p>
<p><img src="/images/14732507303613.jpg" alt=""></p>
<p>Peano 曲线的主要问题是突变性，比方说从 0111 到 1000，虽然数值只变了 1，但是具体的区域就是一上一下两个极端（观察上面图片 Hilbert 曲线的变化幅度就小很多）。</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>了解了基本概念，现在我们可以来看看到底怎么把经纬度转换成 GeoHash 字符串了。主要分三个步骤：</p>
<ol>
<li>分别计算经纬度，转换成二进制字符串</li>
<li>合并字符串并分组</li>
<li>根据编码表转换为 Base32 字符串</li>
</ol>
<p>这里就以广州的坐标为例（北纬 23.1291，东经 113.2644），看看如何转换为 GeoHash 字符串。这里多说一句，纬度的范围是 [-90, 90]，经度的范围是 [-180, 180]，其中负数代表南纬和西经。</p>
<p>先来处理纬度 23.1291，下面的表格中左值和右值分别代表区间的范围，中值用作二分法，这里如果落在左值和中值之间，则对应位为 0，反之为 1：</p>
<table>
<thead>
<tr>
<th style="text-align:center">左值</th>
<th style="text-align:center">中值</th>
<th style="text-align:center">右值</th>
<th style="text-align:center">位</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">-90</td>
<td style="text-align:center">0</td>
<td style="text-align:center">90</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">45</td>
<td style="text-align:center">90</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">22.5</td>
<td style="text-align:center">45</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">22.5</td>
<td style="text-align:center">33.75</td>
<td style="text-align:center">45</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">22.5</td>
<td style="text-align:center">28.125</td>
<td style="text-align:center">33.75</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">22.5</td>
<td style="text-align:center">25.3125</td>
<td style="text-align:center">28.125</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">22.5</td>
<td style="text-align:center">23.90625</td>
<td style="text-align:center">25.3125</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">22.5</td>
<td style="text-align:center">23.203125</td>
<td style="text-align:center">23.90625</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">22.5</td>
<td style="text-align:center">22.8515625</td>
<td style="text-align:center">23.203125</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">22.8515625</td>
<td style="text-align:center">23.02734375</td>
<td style="text-align:center">23.203125</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
<p>然后再来算经度 113.2644，同样的道理：</p>
<table>
<thead>
<tr>
<th style="text-align:center">左值</th>
<th style="text-align:center">中值</th>
<th style="text-align:center">右值</th>
<th style="text-align:center">位</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">-180</td>
<td style="text-align:center">0</td>
<td style="text-align:center">180</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">90</td>
<td style="text-align:center">180</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">90</td>
<td style="text-align:center">135</td>
<td style="text-align:center">180</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">90</td>
<td style="text-align:center">112.5</td>
<td style="text-align:center">135</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">112.5</td>
<td style="text-align:center">123.75</td>
<td style="text-align:center">135</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">112.5</td>
<td style="text-align:center">118.125</td>
<td style="text-align:center">123.75</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">112.5</td>
<td style="text-align:center">115.3125</td>
<td style="text-align:center">118.125</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">112.5</td>
<td style="text-align:center">113.90625</td>
<td style="text-align:center">115.3125</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">112.5</td>
<td style="text-align:center">113.203125</td>
<td style="text-align:center">113.90625</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">113.203125</td>
<td style="text-align:center">113.5546875</td>
<td style="text-align:center">113.90625</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
<p>组合一下，纬度的编码是 <code>10100 00011</code>，经度的编码是 <code>11010 00010</code>。我们需要把这两个字符串组合起来，<strong>奇数位放经度，偶数位放纬度</strong>（这里要注意，网上很多文章这里没有写对），组合起来就是 <code>11100 11000 00000 01101</code>，也就是 <code>28 24 0 13</code>，对照 Base32 编码</p>
<table>
<thead>
<tr>
<th style="text-align:center">Decimal</th>
<th style="text-align:center">Base 32</th>
<th style="text-align:center">Decimal</th>
<th style="text-align:center">Base 32</th>
<th style="text-align:center">Decimal</th>
<th style="text-align:center">Base 32</th>
<th style="text-align:center">Decimal</th>
<th style="text-align:center">Base 32</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2</td>
<td style="text-align:center">3</td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">4</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">6</td>
<td style="text-align:center">6</td>
<td style="text-align:center">7</td>
<td style="text-align:center">7</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">8</td>
<td style="text-align:center">9</td>
<td style="text-align:center">9</td>
<td style="text-align:center">10</td>
<td style="text-align:center">b</td>
<td style="text-align:center">11</td>
<td style="text-align:center">c</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">d</td>
<td style="text-align:center">13</td>
<td style="text-align:center">e</td>
<td style="text-align:center">14</td>
<td style="text-align:center">f</td>
<td style="text-align:center">15</td>
<td style="text-align:center">g</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">h</td>
<td style="text-align:center">17</td>
<td style="text-align:center">j</td>
<td style="text-align:center">18</td>
<td style="text-align:center">k</td>
<td style="text-align:center">19</td>
<td style="text-align:center">m</td>
</tr>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:center">n</td>
<td style="text-align:center">21</td>
<td style="text-align:center">p</td>
<td style="text-align:center">22</td>
<td style="text-align:center">q</td>
<td style="text-align:center">23</td>
<td style="text-align:center">r</td>
</tr>
<tr>
<td style="text-align:center">24</td>
<td style="text-align:center">s</td>
<td style="text-align:center">25</td>
<td style="text-align:center">t</td>
<td style="text-align:center">26</td>
<td style="text-align:center">u</td>
<td style="text-align:center">27</td>
<td style="text-align:center">v</td>
</tr>
<tr>
<td style="text-align:center">28</td>
<td style="text-align:center">w</td>
<td style="text-align:center">29</td>
<td style="text-align:center">x</td>
<td style="text-align:center">30</td>
<td style="text-align:center">y</td>
<td style="text-align:center">31</td>
<td style="text-align:center">z</td>
</tr>
</tbody>
</table>
<p>可以得到广州的坐标（北纬 23.1291，东经 113.2644）经过 GeoHash 之后的前四位是 <code>ws0e</code>，我们在 GeoHash 的<a href="http://geohash.org/ws0e96s8g" target="_blank" rel="external">官方网站</a> 检验一下：</p>
<p><img src="/images/14732507181227.jpg" alt=""></p>
<p>可以看到前四位确实是 <code>ws0e</code>，我们手算的结果是正确的！不过因为计算精度的问题，这里只算了前四位，实际上是可以根据我们的需要来进行更多位数的计算的，网页中的编码长度是 9 位，精度基本达到 2 米的数量级（8 位的话则是 19 米，7 位是 76 米，6 位是 610 米）。如果是要做『附近』功能的话，至少要到 6 位的精度。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>了解了具体的 GeoHash 算法之后，我们可以来看看具体在实际应用中可能遇到的各种问题：</p>
<ul>
<li>GeoHash 编码对应的是矩形区域，在边界处需要处理临近区域，但是具体区域的编码并不完全跟 hash 之后的字符串一致（参考前面 0111 和 1000的例子）。为此，我们需要使用周围八个区域的 GeoHash 编码，通过有限扩大搜索范围的方法来解决这个问题</li>
<li>如果是用传统关系型数据库，可以直接利用 GeoHash 的前缀进行检索，比方说 <code>select * from locations where geohash like &#39;ws03%&#39;</code></li>
<li>如果需要兼顾速度与精确度，那么同时保存经纬度和 GeoHash 即可，利用 GeoHash 来缩小范围，再利用经纬度进行精确计算</li>
<li>计算周围 8 个矩形区域，利用原始的 GeoHash 字符串显然是不行的（考虑分别处于赤道两边且很相近的两个点）对于经度一个维度来说，无论切分几次，它的左邻和右邻都只会和它相差1。画一下就知道它是一棵有序的01满二叉树。根据当前矩形的经度串，很容易就获得了它的两个东西邻接经度串。同理，可以根据其纬度串获取南北邻接纬度串。连同当前矩形的经度串和纬度串，就能组合得到周边的8个矩形的二进制串了。Base32 编码后的到 geohash 值，即是所需要的8个索引了（此段<a href="http://evthoriz.com/2015/07/02/Geohash%20%E7%AE%97%E6%B3%95%E7%9A%84%E8%BE%B9%E7%95%8C%E9%97%AE%E9%A2%98/" target="_blank" rel="external">来源</a>）</li>
<li>Base32 是一种简单的加密算法，详情请参考后文链接</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文我们见到了解了 GeoHash 的相关概念和应用，具体的使用过程中还需要根据具体需求来进行调整（比方说不同的曲线填充算法），但是要保证具体实现的一致性。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/Geohash" target="_blank" rel="external">Geohash - From Wikipedia, the free encyclopedia</a></li>
<li><a href="http://geohash.org/ws0e96s8g" target="_blank" rel="external">geohash.org</a></li>
<li><a href="http://charlee.li/geohash-intro.html" target="_blank" rel="external">geohash：用字符串实现附近地点搜索</a></li>
<li><a href="http://www.cnblogs.com/LBSer/p/3310455.html" target="_blank" rel="external">GeoHash核心原理解析</a></li>
<li><a href="https://github.com/mmcloughlin/geohash" target="_blank" rel="external">mmcloughlin/geohash(包含其他几个实现，可以对比下速度)</a></li>
<li><a href="https://github.com/wdxtub/geohash" target="_blank" rel="external">wdxtub/geohash(我自己在项目中使用的)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Base32" target="_blank" rel="external">Base32 - From Wikipedia, the free encyclopedia</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;做任何跟地理位置相关的服务，位置如何表示及存储是绝对绕不开的问题之一。位置的表示倒是可以用经纬度，但是索引和检索的时候，经纬度这种二维表示法就比较麻烦了，这时我们就可以利用 GeoHash 进行『降维攻击』来解决这个问题了。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="索引" scheme="http://wdxtub.com/tags/%E7%B4%A2%E5%BC%95/"/>
    
      <category term="位置" scheme="http://wdxtub.com/tags/%E4%BD%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>Destiny 游戏分析</title>
    <link href="http://wdxtub.com/2016/09/07/destiny-game/"/>
    <id>http://wdxtub.com/2016/09/07/destiny-game/</id>
    <published>2016-09-06T23:04:08.000Z</published>
    <updated>2016-09-06T23:11:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>Destiny 是我在留学时候最常玩的主机游戏之一，虽然随着游戏自然热度下降很多时候成了鬼服，但是有着 Bungie 在后面撑着，游戏的设计还是颇精妙的，下面是我当时玩的时候的一些笔记。</p>
<a id="more"></a>
<hr>
<h2 id="社区建设"><a href="#社区建设" class="headerlink" title="社区建设"></a>社区建设</h2><ul>
<li>Facebook, Twitch 的无缝链接<ul>
<li>可以在游戏启动前查看截图/视频(观众较多，被赞较多)</li>
</ul>
</li>
<li>有哪些朋友在玩</li>
<li>可以查看路人的信息，加好友等</li>
<li>方便的组队系统</li>
</ul>
<h2 id="武器系统"><a href="#武器系统" class="headerlink" title="武器系统"></a>武器系统</h2><ul>
<li>主武器<ul>
<li>Auto Rifle</li>
<li>Canon</li>
<li>Scout Rifle</li>
<li>Pulse Rifle</li>
</ul>
</li>
<li>特殊武器<ul>
<li>Sniper Rifle</li>
<li>Shotgun</li>
<li>Fusion Rifle</li>
</ul>
</li>
<li>重武器<ul>
<li>Machine Gun</li>
<li>Rocket Launcher</li>
</ul>
</li>
</ul>
<h2 id="故事背景"><a href="#故事背景" class="headerlink" title="故事背景"></a>故事背景</h2><p>地球因为战争而毁灭，只剩最后一个城市，幸存的人类成为守护者来保护最后的希望。</p>
<h2 id="游戏表现"><a href="#游戏表现" class="headerlink" title="游戏表现"></a>游戏表现</h2><ul>
<li>非常清晰的任务指示<ul>
<li>进度，奖励，追踪，设置一目了然</li>
</ul>
</li>
<li>任务基本都可以重复挑战，但只有第一次有经验加成<ul>
<li>剧情任务：用于推进剧情，有等级限制</li>
<li>无尽任务：等级限制较低，但敌人等级会根据参加当前任务的玩家等级进行调整。<ul>
<li>地图上有随机出现的任务领取点，可以一直领任务</li>
<li>杀死特定敌人收集资源</li>
<li>清除某个区域所有敌人</li>
<li>探索某些地点</li>
</ul>
</li>
<li>随机任务：在剧情和无尽任务中可能会碰到的突发任务，可以自由选择是否参加</li>
</ul>
</li>
<li>Tower，Earht 最后一个城市<ul>
<li>也就是主城，物品鉴定/任务领取/副本支线/各种 NPC</li>
</ul>
</li>
<li>Old Russia, Earth<ul>
<li>一开始的战场，相当于一幅地图</li>
<li>有十多个子任务</li>
</ul>
</li>
<li>Ocean of Storms, Moon<ul>
<li>第二个开放地图</li>
<li>有十多个子任务</li>
</ul>
</li>
<li>The Crucible<ul>
<li>竞技场，类似于普通的枪战游戏</li>
</ul>
</li>
<li>Venus<ul>
<li>十级开启</li>
</ul>
</li>
<li>画面属于次世代中规中矩，没有丢光环的脸</li>
<li>人物动作扎实，剧情有 Ghost 指引</li>
<li>每个人都有一个 Ghost，可以保存最后的记忆</li>
<li>有很多中文元素，例如标语和路牌</li>
<li>游戏音效很有未来感</li>
<li>有一些任务需要 PlayStation Plus 才可以玩</li>
</ul>
<h2 id="装备设定"><a href="#装备设定" class="headerlink" title="装备设定"></a>装备设定</h2><ul>
<li>在提供多样性的前提下尽可能减少了复杂度，没有太多繁杂的属性</li>
</ul>
<h2 id="经济系统"><a href="#经济系统" class="headerlink" title="经济系统"></a>经济系统</h2><ul>
<li>游戏通用货币：GLIMMER</li>
<li>用钱买的货币：SILVER<ul>
<li>500 = $4.99</li>
<li>1000(+100) = $9.99</li>
<li>2000(+300) = $19.99</li>
</ul>
</li>
</ul>
<h2 id="人物设定"><a href="#人物设定" class="headerlink" title="人物设定"></a>人物设定</h2><ul>
<li>三种职业，每种职业有三种子职业（战士，远程，法师）</li>
<li>满级 40 级</li>
<li>装备分为不同等级：普通（白），特别（绿），罕见（蓝）</li>
<li>非普通装备需要鉴定后才可以使用（也就是不可以在战场上捡到就用）</li>
<li>武器<ul>
<li>主武器：Rifle，Patrol</li>
<li>特殊武器：狙击枪，霰弹枪</li>
<li>重武器：重机枪，火箭筒</li>
<li>武器在到达一定熟练度可以升级</li>
</ul>
</li>
<li>防具<ul>
<li>头盔</li>
<li>护臂</li>
<li>胸甲</li>
<li>足甲</li>
<li>披风</li>
<li>装饰品（40级开放）</li>
</ul>
</li>
<li>Ghost Shell<ul>
<li>任务指示，剧情推进</li>
</ul>
</li>
</ul>
<h2 id="移动客户端"><a href="#移动客户端" class="headerlink" title="移动客户端"></a>移动客户端</h2><ul>
<li>新闻频道</li>
<li>查看朋友状态</li>
<li>游戏中通知</li>
<li>与朋友的聊天</li>
<li>个人信息<ul>
<li>职业</li>
<li>上一次任务</li>
<li>游戏数据<ul>
<li>非常详尽的数据统计</li>
</ul>
</li>
<li>装备</li>
<li>物品</li>
<li>仓库</li>
</ul>
</li>
<li>任务栏<ul>
<li>任务</li>
<li>成就</li>
<li>目标</li>
<li>Bonus</li>
<li>声望</li>
</ul>
</li>
<li>NPC 相关</li>
<li>搜索：用户、玩家、小组、论坛、新闻、帮助</li>
<li>周边商店</li>
</ul>
<p>几乎游戏内的所有东西都可以在移动客户端找到</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Destiny 是我在留学时候最常玩的主机游戏之一，虽然随着游戏自然热度下降很多时候成了鬼服，但是有着 Bungie 在后面撑着，游戏的设计还是颇精妙的，下面是我当时玩的时候的一些笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="Game" scheme="http://wdxtub.com/categories/Game/"/>
    
    
      <category term="Destiny" scheme="http://wdxtub.com/tags/Destiny/"/>
    
      <category term="射击" scheme="http://wdxtub.com/tags/%E5%B0%84%E5%87%BB/"/>
    
  </entry>
  
  <entry>
    <title>经济机器是怎样运行的 - 文字稿</title>
    <link href="http://wdxtub.com/2016/09/07/how-economic-works/"/>
    <id>http://wdxtub.com/2016/09/07/how-economic-works/</id>
    <published>2016-09-06T22:57:11.000Z</published>
    <updated>2016-09-06T23:01:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是视频<a href="https://www.youtube.com/watch?v=rFV7wdEX-Mo" target="_blank" rel="external">『经济机器是怎样运行的』</a>的自制文字稿，当初花了差不多三个小时才完成。这个视频用简单清晰的方式把经济这个大事儿说清楚了，我感觉还是非常不错的。正好在整理笔记，就分享给大家。</p>
<a id="more"></a>
<hr>
<p>经济虽然看起来复杂，其实是以简单和机械的方式运行的。经济有几个简单的零部件和无数次重复的简单交易组成。这些交易首先是由人的天性驱动的，因而形成三股主要的经济动力：</p>
<ol>
<li>生产率的提高</li>
<li>短期债务周期</li>
<li>长期债务周期</li>
</ol>
<p>我们来看看这三个动力是如何相互作用的。</p>
<h2 id="交易"><a href="#交易" class="headerlink" title="交易"></a>交易</h2><p>交易可以看做是无数交易的总和，而交易是一件非常简单的事情，交易时刻都在发生，每次买东西都是进行一笔交易。在每次交易中，买方使用货币或信用，向卖方交换商品、服务或金融资产。货币加上信用等于支出总额。</p>
<p>支出总额是经济的驱动力。</p>
<p>如果用支出总额除以产销总量，就得出价格。交易是经济最基本零件，所有的经济周期都是交易造成的。所以理解了交易，就理解了整个经济。</p>
<p>一个市场，由买卖所有商品的所有买方和卖方构成。经济就是由所有市场内的全部交易构成。把全部市场的总支出和销量加在一起，就得到了了解经济运行状况的所有信息。</p>
<p>个人、企业、银行和政府都在以上述方式从事交易，用货币和信用，交换商品、服务和金融资产。政府是最大的买方和卖方，而政府有两个组成部分，即收税和花钱的中央政府和中央银行。央行控制着经济中的货币和信贷数量，因此不同于其他买方和卖方。央行通过利率和发行更多货币来实现这种控制。因此，央行在信贷流通中发挥着重要作用。</p>
<h2 id="信贷"><a href="#信贷" class="headerlink" title="信贷"></a>信贷</h2><p>信贷是经济中最重要的组成部分，但也许是人们最不了解的部分。它之所以重要，是因为它是经济中最大也最变幻莫测的部分。贷款人和借款人，与市场交易中的买方和卖方没有两样。通常，贷款人希望自己的钱生出更多的钱，而借款人是想购买当前无法承担的东西。借贷可以同时满足贷款人和借款人的需要。</p>
<p>借款人保证偿还借款，称为本金，并支付额外的款额，称为利息。利率高时借贷就会减少，因为利息昂贵，反过来，利率低时借贷就会增加，因为利息便宜。如果借款人保证偿还债务，而贷款人相信这一承诺，信贷就产生了。任何两个人都可以通过协定，凭空创造出信贷。</p>
<p>信贷看似简单，实则复杂。信贷一旦产生，立即成为债务。债务是贷款人的资产，是借款人的负债。等到借款人之后偿还了贷款，并支付了利息，这些资产和负债将会消失，交易得以完成。</p>
<p>借款人一旦获得了信贷，便可以增加自己的支出，而支出是经济的驱动力，因为一个人的支出是另一个人的收入。如果某人的收入增加，其信用度就会提高，贷款人就更愿意把钱交给他。</p>
<p>信用良好的借款人具备两个条件：</p>
<ol>
<li>偿还能力</li>
<li>不能还债时的抵押物</li>
</ol>
<p>收入与债务的比率高，借款人就具有良好的偿还能力。如果不能偿还，借款人还可以用有价值可以出售的资产作为抵押物。这样贷款人可以放心地把钱借给他们。</p>
<p>所以收入增加，借贷也增加，从而能够增加支出，由于一个人的支出是另一个人的收入，这将导致借贷进一步增加，并不断循环。这一自我驱动的模式导致经济增长，也正是因为如此，才产生了经济周期。</p>
<h2 id="周期"><a href="#周期" class="headerlink" title="周期"></a>周期</h2><p>在一项交易中，为了获得某样东西，你必须付出另一样东西。长期来看，你得到多少取决于你生产了多少。我们的知识虽时间而逐渐增多，知识的积累会提高我们的生活水平，我们将此称为生产率的提高。</p>
<p>一个善于创新和勤奋的人，会比那些自满且懒惰的人更快提高生产率和生活水平，但在短期内不一定体现出来。</p>
<p>生产率在长期内最关键。信贷在短期内最重要。这是因为生产率的提高不会剧烈波动，因此不是经济起伏的重要动力。但是债务是这样定义的，借贷时消费超过产出，还债是消费低于产出。</p>
<p>债务量的波动有两大周期，其中一个周期持续大约 5-8 年，另一个大约持续 75-100 年。虽然每个人都在波动中，因为距离太近，往往意识不到。</p>
<p>如上所示，经济波动不是取决于创新，而是跟信贷更有关系。我们先想象一个没有信贷的经济运行。在这样的环境里，增加支出的唯一方式是增加收入，因此需要提高生产率和工作量，提高生产率是经济增长的唯一途径。由于我的支出是另一个人的收入，当我或另一个人提高生产率的时候，经济就会增长，而且是一个稳步增长。</p>
<p>但是由于我们借债，于是产生了周期，原因并不是任何法规，而是人的天性和信贷的运作方式。借债不过是提前消费，为了购买现在买不起的东西，你的支出必然超过收入，因此你需要借钱，实际上是像未来的自己借钱，这样马上就形成了一个周期。通常，一旦你借钱，就制造了一个周期。对个人是这样，对整个经济运行也是这样。这就是为什么需要理解信贷，因为信贷创造了一系列机械的和可以预料的将在未来发生的事件。这就是信贷不同于货币的地方。</p>
<p>完成交易需要使用货币，交易立即完成。利用信用开始交易，买卖双方创造了信贷，只有当偿还债务之后，交易才算完成。</p>
<p>现实生活中，大部分所谓的钱，其实都是信贷。美国国内的信贷总额大约为 50 万亿美元，而货币总额只有大约 3 万亿美元。在有信贷的经济运行中，不仅可以通过提高生产率提高支出，还可以通过借贷来提高支出。因此信贷可以使收入增长在短期内超过生产率的增长，但在长期内并非如此。</p>
<p>信贷并不是坏事，只是会导致周期性变化。</p>
<p>信贷如果造成超过偿还能力的过度消费，就是不良信贷。但是，如果信贷高效率地分配资源产生收入并偿还债务，就是良性信贷。在有信贷的经济运行中，我们可以观察各种交易，了解信贷如何带来经济增长，是一个链式增长的过程，但借债形成周期。</p>
<h2 id="短期经济周期"><a href="#短期经济周期" class="headerlink" title="短期经济周期"></a>短期经济周期</h2><p>随着经济活动的增加，出现了扩张，这是短期债务周期的第一阶段，支出继续增加，价格开始上涨。原因是，导致支出增加的是信贷，而信贷可以立刻凭空产生。如果支出和收入的增长速度超过所出售的商品的生产速度，价格就会上涨，我们把价格的上涨称为通货膨胀。</p>
<p>央行不希望通货膨胀过高，因为这会导致许多问题。央行在看到价格上涨时就会提高利率，随着利率的上升，有能力借钱的人就会减少，同时，现有的债务成本也会上升，每个月信用卡的还款额会增加。由于人们减少借债，并且还款额度增长，所以剩下来用于支出的资金将会减少，因此支出速度放慢，而由于一个人的支出是另一个人的收入，环环相扣，人们的收入将下降。由于支出减少，价格就会下跌，我们称之为通货紧缩，经济活动减少，经济便陷入衰退。如果衰退过于严重，而通货膨胀不再成为问题，央行将降低利率，使经济活动重新加速。随着利率降低，偿债成本下降，借债和支出增加，出现另一次经济扩张。</p>
<p>在短期债务周期中，限制支出的唯一因素，是贷款人和借款人的贷款意愿。如果信贷易于获得，经济就会扩张，如果信贷不易获得，经济就会衰退。请注意，这个周期主要由央行控制。</p>
<p>短期债务周期通常持续 5-8 年，在几十年里不断重复。但是请注意在每个周期的低谷和高峰后，经济增长和债务都超过前一个周期。为什么会这样，这是人促成的。人具有借更多钱和花更多钱的倾向而不喜欢偿还债务，这是人的天性，因此在长期内，债务增加的速度超过收入，从而形成长期债务周期。</p>
<h2 id="长期债务周期"><a href="#长期债务周期" class="headerlink" title="长期债务周期"></a>长期债务周期</h2><p>尽管人们的债务增加，但贷款人会提供更宽松的借贷条件，这是为什么？这是因为大家都以为形势一片大好，因为仅注意最近出现的情况。最近出现的情况是什么呢？收入一直在增加，资产价值不断上升，股票市场欣欣向荣，现在是繁荣时期，用借来的钱购买各类资产，很划算。当人们过度借贷消费时，泡沫便产生了。</p>
<p>因此，尽管债务一直增加，但是收入也已相近的速度增加，从而抵消了债务。我们把债务与收入比例称为债务负担，只要收入继续上升，债务负担就可以承受。于此同时资产价格迅猛上升，人们大量借钱来购买资产，因为投资促使资产价格日益升高，人们感觉自己很富有，因此即使积累了大量债务，收入和资产价值的上升帮助借款人在长期内保持良好的信用。但是这种情况显然无法永久持续下去。</p>
<p>几十年来，债务负担不断增加使偿贷成本越来越高，到了一定的时候，偿贷成本的增加速度超过收入，迫使人们削减支出，由于一个人的支出是另一个人的收入，收入开始减少，而偿贷成本继续增加，导致支出继续减少，周期开始逆转。</p>
<p>这时到达了长期债务的顶峰，债务负担变得过重。美国和欧洲在 2008 年就发生了这一情况。日本在 1989 年和美国在 1929 年因同样原因发生了这一情况。现在经济进入了去杠杆化时代。</p>
<h2 id="去杠杆化"><a href="#去杠杆化" class="headerlink" title="去杠杆化"></a>去杠杆化</h2><p>在去杠杆化过程中，人们削减支出，收入下降，信贷消失，资产价格下跌，银行发生挤兑，股票市场暴跌，社会紧张加剧，整个过程开始下滑并形成恶性循环，随着收入下降和偿债成本增加，借款人倍感拮据，随着信用消失，信贷枯竭，借款人再也无法借到足够的钱来偿还债务，借款人竭力填补这个窟窿，不得不出售资产，在支出下降的同时，出售热潮使市场充满待售资产。这时股票市场暴跌，不动产市场一蹶不振，银行陷入困境，随着资产价格下跌，借款人的抵押物价值也在降低，进一步降低了借款人的信用，人们觉得自己很穷，信贷迅速消失，支出减少，收入减少，财富减少，信贷减少，借债减少，这是一个恶性循环。看起来和衰退相似，但是无法通过降低利率来改变局面。</p>
<p>在衰退中，可以通过降低利率来刺激借贷。但是在去杠杆化过程中，由于利率已经很低，低至零，从而丧失刺激功能，因此降低利率不起作用。</p>
<p>美国国内的利率在 1930 年代的去杠杆化期间下降到零，在 2008 年也是如此。</p>
<p>衰退与去杠杆化的区别在于，在去杠杆化过程中，借款人的债务负担变得过重，无法通过降低利率来减轻。贷款人意识到债务过于庞大，根本无法足额偿还。借款人失去了偿带能力，其抵押物失去价值，他们觉得受到了债务的极大伤害，不想再介入更多债务。贷款人停止放贷，借款人停止借贷。整个经济体与个人一样都失去了信用度。</p>
<p>那么应该怎样应对去杠杆化？问题在于债务负担太重，必须减轻，为此可以采用四种办法：</p>
<ol>
<li>削减支出（紧缩）</li>
<li>减少债务（债务违约和重组）</li>
<li>财务再分配</li>
<li>发行货币</li>
</ol>
<p>历史上每一个去杠杆化阶段都是如此：</p>
<ul>
<li>美国（1930年代）</li>
<li>英国（1950年代）</li>
<li>日本（1990年代）</li>
<li>西班牙和意大利（2010年代）</li>
</ul>
<p>削减支出会导致收入下降，收入下降速度超过还债的速度，因此债务负担实际上更为沉重。削减支出的办法引起通货紧缩。企业不得不削减成本，这意味着工作机会减少，失业率上升。这导致下一个步骤，即必须减少债务。</p>
<p>借款人不还钱，存款人会担心银行没钱，于是纷纷取出存款，银行受到挤兑，个人、企业、银行出现债务违约，这种严重的经济收缩，就是萧条。萧条的一个主要特征是人们发现原来属于自己的财富中很大一部分其实并不存在。</p>
<p>很多贷款人不希望自己的资产消失，同意债务重组。债务重组意味着贷款人得到的还款减少，或偿还期延长，或利率低于当初商定的水平，无论如何，合约被破坏，结果是债务减少，贷款人希望多少收回一些贷款，这强过血本无归。债务重组让债务消失，但是由于它导致收入和资产价值以更快的速度消失，债务负担继续日趋沉重。削减债务和削减支出一样令人痛苦，并且导致通货紧缩，所有这些都对中央政府产生影响，因为收入降低和就业减少，意味着政府的税收减少，于此同时，由于失业率上升，中央政府需要增加支出，很多失业者储蓄不足，需要政府的财务资助。此外政府制定刺激计划和增加支以弥补经济活动的减少。</p>
<p>在去杠杆化过程中，政府的预算赤字飙升，原因是政府的支出超过税收。政府必须加税或者举债以填补赤字。但是要从哪里拿钱？</p>
<p>从富人手中，通过征税把财富从富人那里转到穷人手中。这样会产生阶级矛盾，如果萧条继续下去，就会爆发社会动荡，不仅国家内部的紧张加剧，而且国家之间也会这样，债务国和债权国之间尤其如此，这种局势会导致政治变革，尤其是极端的变革。</p>
<p>1930 年代，这种情况导致希特勒掌权，欧洲爆发战争，和美国的大萧条。</p>
<p>因为支出的很大一部分是信贷，但是萧条时期信贷消失，所以人们钱不够花，那么怎么办，央行发行更多货币，但这个会引起通货膨胀和刺激经济。</p>
<p>央行通过用这些货币购买金融资产，帮助推升了资产价格，从而提高了人们的信用，但是这仅仅有助于那些拥有金融资产的人。</p>
<p>因此，为了刺激经济，央行和政府必须合作。央行通过购买政府债券，其实是把钱借给政府，使其可以通过刺激计划和失业救济金，来增加购买商品和服务的支出，这增加了人们的收入，也增加了政府的债务，但是这个办法将降低经济中的总债务负担。</p>
<p>这是一个风险很大的时刻，决策者需要平衡考虑降低债务负担的四种办法，必须平衡兼顾通货紧缩和通货膨胀以便保持稳定，如果取成适当的平衡，就可以带来<strong>和谐的去杠杆化</strong>。</p>
<p>所以说去杠杆化可以是痛苦的，也可以是和谐的。怎样才能实现和谐的去杠杆化？需要结合使用这四种办法。</p>
<p>那么发行的货币是否会增加通货膨胀呢，如果增发的货币抵消了信贷的降幅就不会引发通货膨胀，因为影响价格的因素是支出，而支出包括货币和信贷。</p>
<p>央行为了扭转局面，不仅需要推动收入的增长，而且需要让收入的增长率，超过所积累债务的利率。也就是收入一定要比债务增长得快。但发行过多货币会导致恶性通货膨胀。</p>
<p>实际上去杠杆化是一个把高债务水平变化到低债务水平的过程。为了使经济再次恢复正常，这个通货再膨胀的阶段大约要持续 7-10 年，因此有失去的 10 年这个说法。</p>
<h2 id="三条经验法则"><a href="#三条经验法则" class="headerlink" title="三条经验法则"></a>三条经验法则</h2><ol>
<li>不要让债务的增长速度超过收入</li>
<li>不要让收入的增长速度超过生产率</li>
<li>尽一切可能提高生产率</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是视频&lt;a href=&quot;https://www.youtube.com/watch?v=rFV7wdEX-Mo&quot;&gt;『经济机器是怎样运行的』&lt;/a&gt;的自制文字稿，当初花了差不多三个小时才完成。这个视频用简单清晰的方式把经济这个大事儿说清楚了，我感觉还是非常不错的。正好在整理笔记，就分享给大家。&lt;/p&gt;
    
    </summary>
    
      <category term="Reading" scheme="http://wdxtub.com/categories/Reading/"/>
    
    
      <category term="经济" scheme="http://wdxtub.com/tags/%E7%BB%8F%E6%B5%8E/"/>
    
      <category term="机制" scheme="http://wdxtub.com/tags/%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>汇报后的小思考 - 博弈</title>
    <link href="http://wdxtub.com/2016/09/05/report-little-thought-2/"/>
    <id>http://wdxtub.com/2016/09/05/report-little-thought-2/</id>
    <published>2016-09-05T14:27:13.000Z</published>
    <updated>2016-09-05T16:26:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>以前觉得『横看成岭侧成峰』，看到不同的东西，不过是因为角度不同罢了。今天才意识到，想要『一览众山小』，非『凌绝顶』不可。所谓大局观，是掌握更多信息，从更高的层次观察和思考问题。</p>
<a id="more"></a>
<hr>
<p>今天参加了两场『不太一样』的会议，一是跟市场部讨论商业智能相关的数据需求，二是和知识产权部一起向公关的老大汇报。</p>
<p>作为数据平台唯二的开发，面对市场部的各类需求，我的内心一方面是拒绝的，另一方面也深知个中的重要性。技术其实和猫一样，无论黑猫白猫，能抓到老鼠就是好猫。现阶段，我们更需要能创造出价值的技术，而不是摆在博物馆悉心呵护的技术。即使如此，问题依然也很多，比方说时间和人力都远远不够，我和另一个开发人员身上各自背着几个不同项目的设计/开发/维护任务，人力几乎是零冗余的。但是这些其实都不是理由，如果天时地利人和都准备有了，还怎么体现我们的价值呢？</p>
<p>在各项资源都吃紧的条件下，我们的策略一是和市场部相关同事梳理具体的需求逻辑，力求在现有系统的基础上做简单调整即可满足需求，同时也给出临时的权宜之计用作过渡；二是结合数据使用者的相关习惯有针对进行较通用的开发，业务的归业务，架构的归架构。</p>
<p>不可忽视的是不同背景不同专业不同角色思考问题角度的不同，开发人员一定要尽可能设身处地去为用户思考，而不是去过度追求设计及实现的优美，当然这并不意味着可以瞎写代码，而是根据实际需要而不是假想的需求来进行开发。</p>
<p>因为数据平台涉及公司各个业务线，需要对接的系统和数据源非常多，如何能够高效进行沟通协调，如何保证讨论时大家的理念概念模型一致，都是值得探索和研究的问题。我现在的策略很简单，就是以『新参者』的姿态，尽可能快得去学习如何把事情做好。</p>
<p>真正看到用户是如何使用自己做的系统的时候，才更能意识到哪里做好了，哪里没做好。但是这之中出现 bug 确实非常尴尬，虽然是因为经验不足了解不深所犯的错，不过错误的数据会导致错误的判断，我的内心是不安的。这次的尴尬经历也更让我意识到把工作做完和把工作做好之间的巨大差距，还是要以更高的标准来要求自己。</p>
<p>中午睡觉起来没多久就接到电话，五点要开会，会上跟公关老大汇报，虽然汇报内容我已经熟稔于心，但还是花了一个多小时重新准备了一次。和考试前复习的大多不会考一样，准备的内容大概只有百分之十有用，剩下的讨论都在我从来没有想过的层次上进行，有点方，但是也很高兴，因为看到自己的差距就有了目标，有了目标，好好努力就是了。</p>
<p>第一次和知识产权部门与公关部门打交道，让我意识到开发只是做好事情的一环，甚至是最基础的一环，眼界放开，每一环都很重要。在诸如占坑定标准的事儿上，技术能力不是最重要的，敏锐的嗅觉和超前的筹划才是。</p>
<p>班门弄斧不是我的爱好，汇报完我了解的内容，基本就在努力观察和学习前辈们思考和表达的方法，也算有幸一撇更高层次的博弈到底是怎么回事儿。程序员间的博弈其实蛮简单，无非是接口如何设计、工作如何划分、脏活累活哪个团队来接。但是到商业博弈的范畴，那可是真刀真枪，招式不多，但是每一次出击都得切入要害，看似风平浪静其实暗涌澎湃。</p>
<p>从老大的字里行间，能够感受到他对信息广度及获取渠道的在意，只有掌握各方的动向，知己知彼，才能结合不同地区社会运转的规律和组织架构的要点，从更高的层级来判断一件事情的价值。另一个很重要的判断依据，则是历史，前事不忘后事之师绝不只是要吸取教训这么简单，而是努力去利用各方面的经验。这么说来，还真应该再去好好看看历史和兵法，都是财富。</p>
<p>『上将伐谋，其次伐交，再次伐兵，其下攻城』，真正参与到项目中来，才意识到各种公司和组织在标准制定这件事情上的白热化竞争。这之中知识产权相关的工作，就是保护自己的壁垒，没有稳固的根基，哪能广积粮缓称王闷声发大财呢？</p>
<p>有的时候出了事故才是真正体现系统价值的时候，在没有出错的时候，总是会有人不相信到底能多错，不相信系统的能力。所以还是应了那句老话，塞翁失马焉知非福。</p>
<p>至于我为什么参与到这件事儿里来了，其实起因也非常简单，说白了就是一封邮件。可能因为一直以来的写作习惯，会注意上下文的逻辑及读者的感受。因此知识产权部门的同事对我的印象还蛮好的，事情才以我完全没有想到的方式在发展着。回过头来想想，即使我当时知道会因此参与到这个项目中来，能做到的程度也就和当初没啥差别。我不是为了别的什么，只是想把事情做好，只是想让大家知道，从我手中交出去的东西，是靠谱的。</p>
<p>大家身上都有很多值得我学习的地方，以后更要踏踏实实以『新参者』的心态向各行各业的老司机学习，再努力争取机会去实践。</p>
<p>最初了解『新参者』这个词，是在东野圭吾的侦探小说中，新参者就是『新来的人』的意思，主角名叫加贺，他的一句话让我印象特别深刻：</p>
<blockquote>
<p>我工作时经常想，残忍的凶杀案发生后，我们不仅要将凶手抓获，还有必要彻查案件发生的原因，否则同样的事情可能还会发生。真相中有很多值得我们学习的东西。</p>
</blockquote>
<p>为了写博客又晚睡了，但是用双手辅助完成整个思考的过程，才是一天最佳的结束方式吧。前些天才知道原来匹兹堡也举办过 G20，再加上正在杭州举办的这次，总让我感觉冥冥之中。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以前觉得『横看成岭侧成峰』，看到不同的东西，不过是因为角度不同罢了。今天才意识到，想要『一览众山小』，非『凌绝顶』不可。所谓大局观，是掌握更多信息，从更高的层次观察和思考问题。&lt;/p&gt;
    
    </summary>
    
      <category term="Thinking" scheme="http://wdxtub.com/categories/Thinking/"/>
    
    
      <category term="思考" scheme="http://wdxtub.com/tags/%E6%80%9D%E8%80%83/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
      <category term="汇报" scheme="http://wdxtub.com/tags/%E6%B1%87%E6%8A%A5/"/>
    
  </entry>
  
  <entry>
    <title>【聊聊无人机】柒 碰撞规避</title>
    <link href="http://wdxtub.com/2016/09/03/drone-thought-collision-avoidance/"/>
    <id>http://wdxtub.com/2016/09/03/drone-thought-collision-avoidance/</id>
    <published>2016-09-02T23:29:07.000Z</published>
    <updated>2016-09-03T03:51:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>对于消费级无人机来说，安全比性能更重要。撞机是小事，砸伤划伤人可就是大事了。本文就来说说，无人机规避碰撞的一些研究思路，以及个人的一个超有诚意的小想法。</p>
<a id="more"></a>
<hr>
<p>前面的系列文章主要介绍了美国对于无人机及其空域的划分及管控。因为各方面原因，没有办法在博客中介绍中国的管理思路（感兴趣可以自行搜索已公开的资料），不过可以提一下的是，欧洲的思路和中国的比较类似，与美国的方法有比较大的不同。不过这并不是本文的重点，今天还是来聊聊无人机规避碰撞的问题。</p>
<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>为了保证无人机的安全，各大厂商可谓是煞费苦心，从地理围栏、主动避障到各种自动化操作，层出不穷的各种方法主要是通过限制、辅助和自动化来降低无人机发生安全事故的风险。这种风险主要是两方面，一是无人机与物体（飞行器、行人、环境）相撞，二是无人机因为各种复杂状况超出处理能力而导致的坠落。</p>
<p>先说说第二个方面，无人机坠落俗称『炸机』，这里用来代称各种因为无人机硬件条件限制所导致的事故，包括但不限于：</p>
<ul>
<li>飞行中遇到大风失去平衡且在已有的动力条件下无法恢复所导致的坠落</li>
<li>电量不足以返航所以不知道在哪里就掉下来了</li>
<li>突然下雨，电机进水失去动力，于是掉下来</li>
</ul>
<p>为了解决以上这些问题，基本思路就是在飞行控制芯片中集成更加先进的算法，在硬件中塞更多的传感器以增加无人机的环境感知能力。那么问题来了，在电池技术没有什么突破的今天，更强大的算法意味着更多的计算，是要智能还是要续航，这种两难的问题就很尴尬了。</p>
<p>前段时间跟我司相机和飞控部门的同事聊过，因为计算能力限制，很多时候连基本的傅立叶变换都没有办法快速完成，也就意味着市面上大部分高大上的论文是没办法集成到无人机的智能系统中的。</p>
<p>回过头来说第一方面，要避免无人机意外撞机，首先需要解决的是让无人机知道周围有什么东西。不过开始之前，先来参考一下有人驾驶飞机的空中管制系统。从最原始的无线电通信，到之后 ATC（空中交通管制系统）的普及使得各类飞行基本得以『井水不犯河水』，能不能考虑把有人驾驶飞机的这一套用在无人机上呢？</p>
<p>很可惜，答案是否定的。与传统的有人驾驶飞机相比，无人机可以被认为是一个全封闭的自治系统（如果数据链路断了，飞手便无法控制），很难根据不同情况做出合适的反应；另外无人机的飞行路线也比较随意，不像有人驾驶飞机基本按照固定的航线飞行；更重要的，无人机的范围很大，无论是大小还是性能都有巨大差异，很难上一套统一的标准来满足所有类型无人机的需求。</p>
<p>抛开军用和商用无人机不说，民用消费级无人机正处于群雄并起的春秋战国时代，大家都想把自己的技术上升成标准。这个时候首发优势就很重要了，谁能先拿出一整套完整的方案，谁的市场占有率高，大约就可以强势促成标准。</p>
<p>以前我还不明白为什么各行各业都有自己的一套标准，直到自己参与其中才意识到，标准就是规矩，规矩背后就是协商好的『分赃』方法，不仅可以拦住搅局者，还能稳固自己的一亩三分地，这样回过头来看布雷顿森林体系，就觉得『哎呀真特么都是套路』。</p>
<p>好了不扯远了，接下来我们来看看现在比较常用的规避碰撞的技术。</p>
<h2 id="现有技术"><a href="#现有技术" class="headerlink" title="现有技术"></a>现有技术</h2><p>各大厂商的相关研发基本围绕着两个思路以达到『感知与躲避』的目的：</p>
<ol>
<li>依赖于多传感器的环境感知</li>
<li>依赖于智能算法的自主规避机制</li>
</ol>
<p>这里我分别来简单说一下。</p>
<p>依赖传感器的解决方案基本可以被认为是『非合作型』方法，之所以叫『非合作型』，主要是因为整个过程不需要与其他物体进行任何形式的通讯（或者是不具有通讯能力的，比方说鸟类）。基本的方法简单粗暴，五个字就可以概括——『声光电磁气』。从雷达到红外线到摄像头到气压计到指南针，基本可以认为是数字信号处理的过程，虽然可以把大部分计算用硬件实现，不过还是前面的问题，暂且不考虑具体的准确性，电量本身就是跨不过去的门槛。</p>
<p>依赖智能算法的解决方案其实也需要硬件配合，不过更多的是通讯的硬件，比方说 TCAS 和 ADS-B。有通讯基本意味着是『合作型』，通过数据链路共享信息，以达到智能规避碰撞的目的。</p>
<p>TCAS 的介绍可以参考下面摘录自《世界民航杂志120期》的内容：</p>
<blockquote>
<p>TCAS 全称是 Traffic Collision Avoidance System，主要由询问器、应答机、收发机和计算机组成。监视范围一般为前方30海里，上、下方为3000米，在侧面和后方的监视距离较小。（为了减少无线电干扰，管理条例对TCAS的功率有所限制。它把TCAS的前向作用距离限定在45英里左右，侧向和后向作用距离则更小。）</p>
<p>TCAS 的询问器发出脉冲信号，这种无线电信号称为询问信号，与地面发射的空中雷达交通管制信号类似。当其他飞机的应答器接收到询问信号时，会发射应答信号。TCAS 的计算机根据发射信号和应答信号间的时间间隔来计算距离。同时根据方向天线确定方位，为驾驶员提供信息和警告，这些信息显示在驾驶员的导航信息显示器上。</p>
<p>TCAS 可以提供语言建议警告，计算机可以计算出监视区内30架以内飞机的动向和可能的危险接近，使驾驶员有25－40秒的时间采取措施。（TCAS 可跟踪45架飞机，根据选定目标的优先级，最多显示30架飞机。）</p>
</blockquote>
<p>看起来很符合要求嘛！不过这样一套系统要 20 万人民币以上，估计短时间内很难降价到能够装载到民用消费级无人机身上。但是 TCAS 的整体思路是值得借鉴的，在下一节会详细介绍。</p>
<p>另一个比较新但是比较有潜力的技术是 ADS-B，全称是 Automatic Dependent Surveillance - Broadcast，会自动从相关机载设备获取参数向其他飞机或地面站广播飞机的位置、高度、速度、航向、识别号等信息。用百科中的一段原理介绍：</p>
<blockquote>
<p>ADS-B 系统是一个集通信与监视于一体的信息系统，由信息源、信息传输通道和信息处理与显示三部分组成。ADS-B的主要信息是飞机的 4 维位置信息(经度、纬度、高度和时间)和其它可能附加信息(冲突告警信息，飞行员输入信息，航迹角，航线拐点等信息)以及飞机的识别信息和类别信息。此外，还可能包括一些别的附加信息，如航向、空速、风速、风向和飞机外界温度等。这些信息可以由以下航空电子设备得到：1)全球卫星导航系统(GNSS);2)惯性导航系统(INS);3)惯性参考系统(IRS);4)飞行管理器；5)其它机载传感器。ADS-B 的信息传输通道以 ADS-B 报文形式，通过空-空、空-地数据链广播式传播。ADS-B 的信息处理与显示主要包括位置信息和其它附加信息的提取、处理及有效算法，并且形成清晰、直观的背景地图和航迹、交通态势分布、参数窗口以及报文窗口等，最后以伪雷达画面实时地提供给用户。</p>
</blockquote>
<p>ADS-B 其实早在十年前就开始了相关的研究和测试，不过目前还没有搭载 ADS-B 的民用消费级无人机（虽然比 TCAS 便宜，但是也要 1 万人民币以上），这种涉及行业和政府之间的沟通协调，往往需要比较长的时间。另外 NASA/Google/Amazon 也都在开发类似的自动化系统，一个技术最终是否能够落地，就要看哪家拿出来的方案更加靠谱了。不过我个人的观察是，因为空中环境的复杂性，可能会基于不同场景使用不同的方案，具体还需要大量的实地测试。</p>
<p>简单总结一下，目前无人机在安全性相关技术的探索，面临着以下问题</p>
<ul>
<li>计算量：硬件条件不允许高负荷计算</li>
<li>传感器：目前还没有集成多种感知能力且足够便宜的传感器能满足设计和冗余的需求</li>
<li>信息源：不同厂商目前并没有信息共享</li>
<li>政策标准：需要较长时间才能达成标准和政策</li>
<li>成本：基于硬件的解决方案在成本上比较有压力</li>
</ul>
<p>实话说，即使是特别昂贵的 TCAS，其实在设计上也有许多妥协，也因此有很多限制条件。目前不存在一种技术，能够一劳永逸解决无人机的安全性问题，如果有，恐怕就是『不起飞』。</p>
<p>之所以这么说，是因为接下来我的想法仅仅在一定条件下适用，而且基于软件的解决方案在现实的移动设备上稳定性存疑，可以作为某种意义上『感知与躲避』功能的补充和辅助，并不能取代硬件解决方案（虽然这部分我是无能为力的）。</p>
<h2 id="我的想法"><a href="#我的想法" class="headerlink" title="我的想法"></a>我的想法</h2><p>再强调一次，接下来的想法只针对特定的条件和场景，是『感知与躲避』功能的补充和辅助。目前能在我司的支持下做相关的研究探索我很开心，希望这个项目不要因为各种各样奇怪原因被迫中止/终止。如果不幸发生了最坏的情况，虽然目前的研究成果没办法带走（毕竟属于公司），如果有相关公司愿意支持这个项目，我还是很愿意从头再设计开发一次的，因为这对于无人机安全性提高还是有一定帮助的。</p>
<p>这里不会说得特别详细（不然就太没有职业道德了），主要说说思路。</p>
<h3 id="总体思路"><a href="#总体思路" class="headerlink" title="总体思路"></a>总体思路</h3><p>针对前面说的『计算量/传感器/信息源/政策标准/成本』几大问题，以 Cortana/Siri 的方式，通过数据连接转移复杂的计算，利用云的力量为无人机提供计算能力和智能调度能力的支持。</p>
<p>回想一下用 Siri 的方式，我们唯一需要做的就是用正常方式说话，然后通过网络传送到服务器，服务器进行处理之后再把结果返回给我们。这样一来，计算量/信息源/成本 这三个问题在用比较好网络连接的条件下就能够一定程度解决。</p>
<p>举个实际的例子：我在飞一台没有搭在 ADS-B 的无人机，但是无人机会把飞行信息发送到手机，手机会把这些信息上传到服务器上，服务器会据此计算/获取：</p>
<ul>
<li>无人机附近的天气状况（大风、降雨等恶劣天气的概率）</li>
<li>无人机附近的其他飞行器的飞行状况，并根据具体特征进行碰撞预警（比方说有另一架无人机在 200 米外，且相向飞行，那么双方都会在手机上接收到碰撞语境提示）</li>
<li>无人机附近的航线及其他计划飞行信息</li>
</ul>
<p>这样用户就可以在手机上获知周围的环境状况，并据此手动/自动进行对应操作。</p>
<h3 id="适用条件"><a href="#适用条件" class="headerlink" title="适用条件"></a>适用条件</h3><p>从前面的描述中，我们可以知道网络是这个系统中非常重要的一环，于是，适用条件/主要服务的飞行器是：</p>
<ul>
<li>没有搭载诸如 ADS-B 或 TCAS 雷达的飞行设备</li>
<li>较好的网络连接条件</li>
<li>遥控器与飞行器间能够维持比较稳定的通信链路</li>
</ul>
<p>实话说，都满足并不简单。尤其是后两个依赖通信的条件，需要找机会实地测试才能给出比较科学的结论。</p>
<h3 id="技术标准"><a href="#技术标准" class="headerlink" title="技术标准"></a>技术标准</h3><p>这部分涉及的问题很多，最近读了一些航空方面的综述，总结下来，需要回答的问题是：</p>
<ul>
<li>怎么样的空域条件可以认为是安全的</li>
<li>每台无人机所需要的安全空域的范围有多大</li>
<li>预警的级别及条件</li>
<li>数据传输的格式及安全性</li>
<li>针对不同性能和类型的飞行器的最低标准</li>
<li>紧急情况的预警机制</li>
</ul>
<p>这部分其实用现有的技术基本可以实现，就是具体的标准需要大量测试。</p>
<h3 id="算法探索"><a href="#算法探索" class="headerlink" title="算法探索"></a>算法探索</h3><p>目前我正在开发的是一套飞行器模拟器，能够模拟无人机的飞行及相关数据上报，这样在测试各类算法的时候能够有更多的数据依据（真实源数据由公司提供）。模拟器的思路很简单，就是基于用户的真实飞行数据进行仿真数据的生成，然后作为算法的输入，来测试算法的表现。</p>
<p>具体的测试标准也需要进行量化，不然就无从评估不同算法的有效程度。这部分需要更多和美国同事沟通，以及多多了解相关的研究（如果有认识的人在做这个请向我引荐一下谢谢！）</p>
<p>目前我的思路大约是机器学习 + 随机过程优化。这部分可以做得简单粗暴，不过要想更加智能，就得多花点时间研究了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文简单介绍了一下碰撞规避的相关技术，并结合自己的工作提出了一点微小的设想，欢迎对无人机飞行感兴趣的同学来交流探讨，争取在 NASA/Google/Amazon 之前撸出一套靠谱可用的系统。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于消费级无人机来说，安全比性能更重要。撞机是小事，砸伤划伤人可就是大事了。本文就来说说，无人机规避碰撞的一些研究思路，以及个人的一个超有诚意的小想法。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="无人机" scheme="http://wdxtub.com/tags/%E6%97%A0%E4%BA%BA%E6%9C%BA/"/>
    
      <category term="碰撞" scheme="http://wdxtub.com/tags/%E7%A2%B0%E6%92%9E/"/>
    
      <category term="感知" scheme="http://wdxtub.com/tags/%E6%84%9F%E7%9F%A5/"/>
    
  </entry>
  
  <entry>
    <title>第十二周 - 湫兮如风</title>
    <link href="http://wdxtub.com/2016/09/02/qiu-xi-ru-feng/"/>
    <id>http://wdxtub.com/2016/09/02/qiu-xi-ru-feng/</id>
    <published>2016-09-02T14:48:23.000Z</published>
    <updated>2016-09-02T16:28:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>日月星辰悄悄，升了又落，我却不知白天黑夜的存在。</p>
<a id="more"></a>
<hr>
<p>白驹过隙，转瞬间，工作就满三个月了。有时候，觉得自己变了，变得现实，学着妥协，试着在各种压力中找到平衡；有时候，又觉得自己没变，一如既往地举着理想主义的旗帜，虽然做了最坏的打算，但还是用最积极的方式在做事。</p>
<p>在父母眼中我永远是长不大的孩子，不说千里，儿行百里母担忧。在发小眼中我永远是那个能吃的小胖墩，打打闹闹没个正形。长大一些，在同学眼中，可能是考试最稳定又最不稳定的，趁着早读吃早餐踩着铃声上课的『捣蛋』分子。工作之后，在同事眼中我又是怎么样呢？</p>
<p>管它是啥，自己开心最重要。</p>
<p>周末一直在聚会，周五晚上凑齐了四个『小学生』，虽然喝得走路歪歪扭扭，心里却感动得稀里哗啦，二十年的羁绊，真的已经比亲兄弟还要亲了。周六带着几个要好的同事在广州溜了一大圈，疯疯癫癫在优衣库试衣间拍奇葩照片，钻到小巷子里只为癞蛤蟆和天鹅肉。周日和高中同学聚了聚，在大洋彼岸漂泊，难免有各种酸甜苦辣，不过难过的时候想着有这么一帮好朋友，或多或少是些慰藉。</p>
<p>周六路过沙面的时候，不由得想起当年和老爸一起沿着江边一路从猎德骑到沙面白天鹅的日子。真的特别庆幸拥有这么多单车上的回忆，我和老爸大约骑遍了广州的东南西北，路过各种各样的风景，但最美的风景其实就在身边。现在想想，哪怕是骑着车去买馒头和烤鸭，都是特别美好的记忆。等工作稍微稳定一些，一定要再去创造些回忆。</p>
<p>周一五点多起床赶着首班动车回深圳上班。周中买了个折叠床和智能手环，解决了『睡』和『跑』的难题。工作中涉及的四五个项目我需要扮演不同的角色，如何合理安排时间平衡沟通和设计，开发和测试就成了一大难题，尤其是产品经理和开发这两个角色，很容易顾此失彼，具体的方法还在慢慢摸索中，反正之前看的书并没有能够解决我的疑惑。</p>
<p>忙忙碌碌之中，还是灵机一动找到了一个突破点，既有学术价值，也有应用价值，我很感兴趣，也非常值得研究。简单来说是智能防撞系统，详细的会在『聊聊无人机』系列中更新。人的自信就是靠把一件一件事情做好建立起来的，能够在刚工作的时候就独立扛起一个项目，开发一个完整的系统，并且在这个过程中学着跟各个部门打交道，我觉得是特别幸运的事情（其实也正是我选择回国的原因），虽然压力不小，不过我就是要 no zuo no die try one try。</p>
<p>这周新日志没写，而是结合工作的内容把之前的坑给补上了，通过一个假想的项目，把设计上的思考和具体实践中需要注意的地方写下来，是一个很好的学习和积累。不过第一次尝试，还是有些准备不足，以后要继续改进。另外因为字体源访问速度较慢的缘故，有两天博客的访问体验非常糟糕，没办法，折腾到一两点，死扣一点点的优化，用耐心去面对网络的复杂。</p>
<p>当然也有好消息！博客的访问量超过十万了！要不要搞个小活动感谢一下大家的支持呢？写书这周因为事务繁杂进展不多，不过最艰难的部分整理基本完成，之后可以按部就班一点点优化了。</p>
<p>有了主攻方向，感觉整个人都斗志昂扬了起来，是时候多快好省搞一波大新闻了！</p>
<p>每次日落日升，今夕何夕，请跟我乘风而去。有一天，想成为，你们的骄傲。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;日月星辰悄悄，升了又落，我却不知白天黑夜的存在。&lt;/p&gt;
    
    </summary>
    
      <category term="Gossip" scheme="http://wdxtub.com/categories/Gossip/"/>
    
    
      <category term="周记" scheme="http://wdxtub.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
      <category term="里程碑" scheme="http://wdxtub.com/tags/%E9%87%8C%E7%A8%8B%E7%A2%91/"/>
    
  </entry>
  
  <entry>
    <title>第十一周 - 大鱼</title>
    <link href="http://wdxtub.com/2016/08/26/big-fish/"/>
    <id>http://wdxtub.com/2016/08/26/big-fish/</id>
    <published>2016-08-26T10:46:17.000Z</published>
    <updated>2016-08-26T11:46:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>海浪无声将夜幕深深淹没，漫过填空尽头的角落。看海天一色，听风起风落；大鱼的翅膀，已经太辽阔。</p>
<a id="more"></a>
<hr>
<p>伴随着稀里哗啦的大雨，我在从深圳平移到广州的路上写下了这篇周记。不过这周却是晴朗的，之前担心的事情一一顺遂。我不禁好奇，为什么我每周都有走上正轨的感觉，是轨道变太快还是我之前走太歪？</p>
<p>姑且叫做『与时俱进』好了。</p>
<p>上周日一个人在房间里看女排哭成一股泥石流，从决赛看到半决赛再到四分之一决赛，倒着看就能明显感觉到姑娘们的心劲儿上来了，尤其是郎平定海神针般杵在场边，颇有千军万马过，我自巍然不动的感觉。而世界级主攻朱婷就更是没话说了，每次她跳起来我就感觉稳了。最后一局 22 比 22 的那次『爆头』式大力杀球，让我甚至有点心疼对手了。这个故事告诉我们，老大当如郎平，上将当如朱婷（押韵的）。</p>
<p>虽然奥运会已经闭幕，但是奥运精神估计还有几个月的半衰期，经过两周比较系统的俯卧撑及跑步练习，上下肢力量，尤其是稳定性有了极大的提高，具体表现出来就是耐力更持久投篮更准确。这个故事告诉我们，要想把事情做好，不能三天打鱼两天晒网，系统训练虽然枯燥和辛苦，却是提高的唯一途径。</p>
<p>周末终于来了一次团队建设，不过随着开学的临近，不少好朋友要暂时离开了。每次要送别，我都会想到古龙的一句话，大意是走的时候我不送你，但倘若再来，无论大风大雨，我都要去接。我特别记得在北京三元桥地铁站的那次回头，瞬间涌上来的伤感淹没了记忆。好在现在我回来了，距离也不再是问题了。</p>
<p>聊聊工作，当救火队员完成支援工作之后（虽然主要是老司机扛着），我终于能够把大部分精力投入到自己负责的项目中去了。写写客户端写写服务端再当当 PM，日子就在设计接口、对接项目、隔空开会以及看论文写文档中度过了。虽然任务很多，不过这也逼着自己提高效率，也更能学到不同岗位不同角色所需要的能力，最重要的，心劲儿要满，心气儿不能高，这样才能沉下心做事，把方方面面的细节都考虑到。</p>
<p>虽然客户端和服务端都是我来写，不过对于 API 的设计、计算的优化、业务场景的理解以及基于用户体验出发的构思，都需要更多向身边有丰富经验的老司机们求教。从写教材这事儿我学到的一点就是，一旦意识到自己交付的是不太可能改变的东西，就更要仔细，代码和白纸黑字一样，第一次没做好，有弥补的机会是非常幸运的，要抱着一次就做好的心态，做到自己的最好。生活也是这样，每一分每一秒过去之后便无法改变，虽然人人都知道，但并不是每个人都能真正理解个中含义。自己擅长的要做好，自己不擅长的，要么找擅长的人，要么努力让自己成为擅长的人。</p>
<p>和同事一起看了谍影重重5，也买了一张大鱼海棠的原声带，生活与工作的平衡很难找到，但是也要尽力去试试。现在想想大鱼海棠的主题，其实任何选择都无可厚非，无论爱或者是被爱，都可以是幸福的，多一点真诚，少一些套路。和伯恩一样，我们都在不断的冲突和挣扎中拨开迷雾寻找自我，即使真相不一定是自己想要的，但也许真实的残酷要比海市蜃楼的美好要让人踏实。</p>
<p>大鱼在梦境的缝隙里游过，我松开时间的绳索，执子手吹散苍茫茫烟波。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;海浪无声将夜幕深深淹没，漫过填空尽头的角落。看海天一色，听风起风落；大鱼的翅膀，已经太辽阔。&lt;/p&gt;
    
    </summary>
    
      <category term="Gossip" scheme="http://wdxtub.com/categories/Gossip/"/>
    
    
      <category term="生活" scheme="http://wdxtub.com/tags/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="周记" scheme="http://wdxtub.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【Go 之旅】V 项目实战：一起看飞机 - 客户端</title>
    <link href="http://wdxtub.com/2016/08/25/go-plane-3/"/>
    <id>http://wdxtub.com/2016/08/25/go-plane-3/</id>
    <published>2016-08-25T13:40:20.000Z</published>
    <updated>2016-08-31T14:18:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们搭建好了后台（虽然实际开发中我是客户端后台一起写的），这次来看看如何做一个配套的客户端（虽然从各种角度看都非常『简约』），小归小，还是有一点意思的。</p>
<a id="more"></a>
<hr>
<p>还记得系列第一篇中的设计稿吗？我把它捣鼓出来了：</p>
<p><img src="/images/14721336651764.jpg" alt=""><br>（感谢上镜的各位名人名言）</p>
<h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>因为早早确定了接口，其实 iOS 部分的实现比后端还要简单粗暴，即使如此，具体开发过程中的坑有很多，比如（大家看一下源代码应该就知道了）</p>
<ul>
<li>时隔太久我忘了 Objective-C 要咋写，花了十分钟才摆脱了 C++ 写法</li>
<li>iOS9 新增的强制 Https 需要改一下描述文件才能走 Http（毕竟只是一个测试）</li>
<li>为了调试方便所有的网络都走的是同步请求（理论上都得用异步来着）</li>
<li>为了测试方便，所有的信息会统一输出到中间的文本框中，我干脆直接叫它 <code>consoleTextView</code></li>
<li>为了测试方便，采用类似终端输出，快速对接接口</li>
<li>为了每次生成不一样的数据，随机生成用户 id，并利用手机本身的定位来获取位置</li>
<li>没有做缓存（服务器端也没有），所以速度还是比较慢。上线不到 1ms，但是涉及数据库操作的『位置』和『附近』功能，基本就需要 200ms 了，这部分还有很大的优化空间</li>
<li>天气部分使用 Yahoo 的 API，弄清楚格式即可</li>
<li>航线部分没找到免费的，暂时先不考虑（也没时间自己写爬虫嘛）</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本来想着把这个项目撸完整一点的，不过因为工作压力和个人事务缠身，不得不仓促结尾。在可以预见的将来应该也不会继续填坑了，只有开源的代码可能能给大家一点帮助（至少服务端是可以的，客户端用了很多过分省事儿的方法，不太好）。</p>
<p>但是，但是。</p>
<p>这里预告一下，之后会用 Go 把我之前的毕业设计重新开发一次，并以开源服务的形式共享给大家。相信对于喜欢做笔记的同学来说，是一个非常好用的工具。<a href="https://github.com/wdxtub/wkk/wiki" target="_blank" rel="external">这里</a>是项目简介。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://github.com/wdxtub/watch-plane-together-ios" target="_blank" rel="external">项目代码</a></li>
<li><a href="https://github.com/wdxtub/wkk/wiki" target="_blank" rel="external">下一个要开发的项目 Wdxtub’s Knowledge Kit</a></li>
<li><a href="http://blog.csdn.net/mylizh/article/details/44838065" target="_blank" rel="external">iOS定位服务系列之一：获取当前位置信息</a></li>
<li><a href="http://www.sollyu.com/objective-c-synchronization-requests-requests-for-asynchronous-requests-get-and-post-requests/" target="_blank" rel="external">Objective-C 之同步请求、异步请求、GET请求、POST请求</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面我们搭建好了后台（虽然实际开发中我是客户端后台一起写的），这次来看看如何做一个配套的客户端（虽然从各种角度看都非常『简约』），小归小，还是有一点意思的。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="Go" scheme="http://wdxtub.com/tags/Go/"/>
    
      <category term="项目" scheme="http://wdxtub.com/tags/%E9%A1%B9%E7%9B%AE/"/>
    
      <category term="飞机" scheme="http://wdxtub.com/tags/%E9%A3%9E%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>【Go 之旅】IV 项目实战：一起看飞机 - 后台</title>
    <link href="http://wdxtub.com/2016/08/24/go-plane-2/"/>
    <id>http://wdxtub.com/2016/08/24/go-plane-2/</id>
    <published>2016-08-24T14:57:38.000Z</published>
    <updated>2016-08-31T14:05:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面已经了解了『一起看飞机』的基本需求，这次我们来搭建一个完整的基于 beego 框架的后台。同时也会涉及调试测试部署等一系列配套工作，我觉得这些反而是工作中很重要的能力，但不知道为啥大部分书都略过了。</p>
<a id="more"></a>
<hr>
<h2 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h2><p>按照 <a href="http://beego.me/docs/install" target="_blank" rel="external">beego 的安装</a> 和 <a href="http://beego.me/docs/install/bee.md" target="_blank" rel="external">bee 工具简介</a> 中的介绍把 <code>beego</code> 和 <code>bee</code> 都安装好，然后在 <code>$GOPATH</code> 中我们能看到 <code>bin</code>, <code>pkg</code>, <code>src</code> 三个文件夹，进入 <code>src</code> 文件夹，之后的项目代码都会在这里。接着用以下命令来创建项目</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># dawang @ dawang in ~/Documents/GO/src [16-08-24 14:36:44] C:2</span></div><div class="line">$ bee new watch-plane-together</div><div class="line">[INFO] Creating application...</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/conf/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/controllers/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/models/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/routers/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/tests/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/static/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/static/js/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/static/css/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/static/img/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/views/</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/conf/app.conf</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/controllers/default.go</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/views/index.tpl</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/routers/router.go</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/tests/default_test.go</div><div class="line">/Users/dawang/Documents/GO/src/watch-plane-together/main.go</div><div class="line">2016/08/24 14:36:51 [SUCC] New application successfully created!</div></pre></td></tr></table></figure>
<p>进入 <code>watch-plane-together</code> 文件夹，然后先跑起来试试看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># dawang @ dawang in ~/Documents/GO/src/watch-plane-together on git:master x [16-08-24 15:56:09] C:130</span></div><div class="line">$ bee run</div><div class="line">bee   :1.4.1</div><div class="line">beego :1.6.1</div><div class="line">Go    :go version go1.7 darwin/amd64</div><div class="line"></div><div class="line">2016/08/24 15:56:11 [INFO] Uses <span class="string">'watch-plane-together'</span> as <span class="string">'appname'</span></div><div class="line">2016/08/24 15:56:11 [INFO] Initializing watcher...</div><div class="line">2016/08/24 15:56:11 [TRAC] Directory(/Users/dawang/Documents/GO/src/watch-plane-together/controllers)</div><div class="line">2016/08/24 15:56:11 [TRAC] Directory(/Users/dawang/Documents/GO/src/watch-plane-together)</div><div class="line">2016/08/24 15:56:11 [TRAC] Directory(/Users/dawang/Documents/GO/src/watch-plane-together/routers)</div><div class="line">2016/08/24 15:56:11 [TRAC] Directory(/Users/dawang/Documents/GO/src/watch-plane-together/tests)</div><div class="line">2016/08/24 15:56:11 [INFO] Start building...</div><div class="line">2016/08/24 15:56:13 [SUCC] Build was successful</div><div class="line">2016/08/24 15:56:13 [INFO] Restarting watch-plane-together ...</div><div class="line">2016/08/24 15:56:13 [INFO] ./watch-plane-together is running...</div><div class="line">2016/08/24 15:56:13 [asm_amd64.s:2086][I] http server Running on :8080</div></pre></td></tr></table></figure>
<p>一切正常的话，访问 <code>localhost:8080</code> 就可以见到：</p>
<p><img src="/images/14720513974639.jpg" alt=""></p>
<p>到底发生了什么？结合具体的目录结构，我们来看看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># dawang @ dawang in ~/Documents/GO/src/watch-plane-together on git:master x [16-08-24 16:02:16]</span></div><div class="line">$ tree ./</div><div class="line">./</div><div class="line">├── README.md</div><div class="line">├── conf</div><div class="line">│   └── app.conf</div><div class="line">├── controllers</div><div class="line">│   └── default.go</div><div class="line">├── main.go</div><div class="line">├── models</div><div class="line">├── routers</div><div class="line">│   └── router.go</div><div class="line">├── static</div><div class="line">│   ├── css</div><div class="line">│   ├── favicon.ico</div><div class="line">│   ├── img</div><div class="line">│   └── js</div><div class="line">├── tests</div><div class="line">│   └── default_test.go</div><div class="line">├── views</div><div class="line">│   └── index.tpl</div><div class="line">└── watch-plane-together</div><div class="line"></div><div class="line">10 directories, 9 files</div></pre></td></tr></table></figure>
<p>我们运行 <code>bee run</code> 之后，程序从 <code>main.go</code> 中开始执行，具体做的工作是把 <code>routers</code> 文件夹中的对应路由规则与具体的控制器进行绑定，比方说 <code>routers/router.go</code> 中有一句为 </p>
<p><code>beego.Router(&quot;/&quot;, &amp;controllers.MainController{})</code></p>
<p>然后对应于 <code>controllers/default.go</code> 中的</p>
<figure class="highlight go"><table><tr><td class="code"><pre><div class="line"><span class="keyword">type</span> MainController <span class="keyword">struct</span> &#123;</div><div class="line">	beego.Controller</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *MainController)</span> <span class="title">Get</span><span class="params">()</span></span> &#123;</div><div class="line">	c.Data[<span class="string">"Website"</span>] = <span class="string">"wdxtub.com"</span></div><div class="line">	c.Data[<span class="string">"Email"</span>] = <span class="string">"dacrocodilee@gmail.com"</span></div><div class="line">	c.TplName = <span class="string">"index.tpl"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里我们给 MainController 添加了 Get() 方法，用于处理基本的 get 请求。而在具体的模板中，使用两个大括号加点号（这里直接输入会导致 hexo 解析异常，所以就用文字描述了）进行引用。</p>
<p>最后，我们可以在 <code>conf/app.conf</code> 中添加一行 <code>EnableAdmin = true</code>，我们就可以在 <code>localhost:8088</code> 中见到一个监控页面，像这样：</p>
<p><img src="/images/14720514130726.jpg" alt=""></p>
<p>基本的套路就是这么简单！然后我们来简单设计下 API，方便之后的开发。</p>
<h2 id="API-规划"><a href="#API-规划" class="headerlink" title="API 规划"></a>API 规划</h2><p>服务 API 的设计其实不算特别难，类似于起函数名。不过随着项目的不断开发，随意的命名会导致项目复杂度指数爆炸。所以我们要采用更为现代化的套路 - RESTful API。</p>
<h3 id="RESTful"><a href="#RESTful" class="headerlink" title="RESTful"></a>RESTful</h3><p>在我看来 RESTful API 和成语差不多，通过一定的规约和沉淀，把互联网连接的低语境拔高的高语境，所谓高语境的意思是大家已经了解一组通用的条件和原则，在此基础上沟通可以极大降低沟通成本。</p>
<p>先来看几个基本概念：</p>
<ul>
<li>资源(Resources)：我们平常上网访问的一张图片、一个文档、一个视频等。这些资源我们通过URI来定位，也就是一个URI表示一个资源</li>
<li>表现(Representation)：资源是做一个具体的实体信息，他可以有多种的展现方式。而把实体展现出来就是表现层，例如一个 txt 文本信息，他可以输出成 html、json、xml 等格式，一个图片他可以 jpg、png 等方式展现，这个就是表现层的意思。 URI 确定一个资源，而在 HTTP 请求的头信息中用 Accept 和 Content-Type 字段指定，这两个字段才是对”表现层”的描述</li>
<li>状态转化(State Transfer)：访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，肯定涉及到数据和状态的变化。而 HTTP 协议是无状态的，那么这些状态肯定保存在服务器端，所以如果客户端想要通知服务器端改变数据和状态的变化，肯定要通过某种方式来通知它。客户端能通知服务器端的手段，只能是 HTTP 协议。具体来说，就是 HTTP 协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET 用来获取资源，POST 用来新建资源（也可以用于更新资源），PUT 用来更新资源，DELETE 用来删除资源。</li>
</ul>
<p>简单点来说，RESTful 的核心思想类似于面向对象，把原先的过程导向转变为资源导向，然后围绕着资源做文章。</p>
<p>这里需要保证请求是无状态的，有以下几个好处：</p>
<ul>
<li>客户端可以缓存数据来改进性能</li>
<li>在接口层帮助系统分层解耦，限制复杂度，底层可以更加独立</li>
<li>易于扩展，没有机器间关联</li>
</ul>
<p><img src="/images/14720514271343.jpg" alt=""></p>
<h3 id="API-总览"><a href="#API-总览" class="headerlink" title="API 总览"></a>API 总览</h3><p>根据 RESTful 的风格，接口中不应该出现动词，而利用 GET/PUT/POST/DELETE 来进行具体的动作，对应到『一起看飞机』这个项目，API 总览：</p>
<ul>
<li><code>/api/position</code> 客户端每隔一段调用一次该接口<ul>
<li><code>POST</code> 上传当前位置到服务器（状态变为上线）</li>
<li><code>DELETE</code> 结束上传位置（状态变为下线，或通过超时判断）</li>
</ul>
</li>
<li><code>/api/weather</code><ul>
<li><code>GET</code> 获取用户附近的天气状况</li>
</ul>
</li>
<li><code>/api/flight</code><ul>
<li><code>GET</code> 获取用户附近的航班情况</li>
</ul>
</li>
<li><code>/api/near</code><ul>
<li><code>GET</code> 获取用户附近的人</li>
</ul>
</li>
</ul>
<p>调试用接口</p>
<ul>
<li><code>/debug/position</code><ul>
<li><code>POST</code> 获取指定用户的当前位置</li>
</ul>
</li>
<li><code>/debug/weather</code><ul>
<li><code>GET</code> 获取指定位置附近的天气状况</li>
</ul>
</li>
<li><code>/debug/flight</code><ul>
<li><code>GET</code> 获取指定位置附近的航班</li>
</ul>
</li>
<li><code>/debug/near</code><ul>
<li><code>POST</code> 获取用户或指定位置附近的人</li>
</ul>
</li>
<li><code>/debug/upload</code><ul>
<li><code>POST</code> 测试 json 数据上传</li>
</ul>
</li>
</ul>
<p>这里因为时间关系，只用非常简单粗暴的方式实现了 debug 接口。所有的数据处理都在 controller 层完成，并未涉及任何 model（暂时还不需要）</p>
<h3 id="Debug-接口"><a href="#Debug-接口" class="headerlink" title="Debug 接口"></a>Debug 接口</h3><p>坐标采用 WGS84 标准，不同的标准有很多，<a href="http://blog.csdn.net/ma969070578/article/details/41013547" target="_blank" rel="external">参考来源</a>：</p>
<ul>
<li>WGS84 坐标系：即地球坐标系，国际上通用的坐标系。设备一般包含GPS芯片或者北斗芯片获取的经纬度为 WGS84 地理坐标系, 谷歌地图采用的是 WGS84 地理坐标系（中国范围除外）</li>
<li>GCJ02 坐标系：即火星坐标系，是由中国国家测绘局制订的地理信息系统的坐标系统。由 WGS84 坐标系经加密后的坐标系。谷歌中国地图和搜搜中国地图采用的是 GCJ02 地理坐标系</li>
<li>BD09坐标系：即百度坐标系，GCJ02 坐标系经加密后的坐标系; 搜狗坐标系、图吧坐标系等，估计也是在 GCJ02 基础上加密而成的</li>
</ul>
<p>另一个坐标转换的<a href="https://github.com/wandergis/coordtransform" target="_blank" rel="external">项目</a></p>
<p>这里因为时间关系，只用非常简单粗暴的方式实现了 debug 接口。所有的数据处理都在 controller 层完成，并未涉及任何 model（暂时还不需要）</p>
<p>Debug 接口主要用于内部测试，对于参数的传递要求比较灵活，主要是基于地理位置进行测试，输入除 position 外都是经纬度坐标</p>
<ul>
<li><code>/debug</code> (GET) 连接测试函数</li>
<li><code>/debug/position</code> (POST) 获取指定用户的当前位置</li>
<li><code>/debug/weather</code> (GET) 获取指定位置附近的天气状况</li>
<li><code>/debug/flight</code> (GET) 获取指定位置附近的航班</li>
<li><code>/debug/near</code> (POST) 获取指定位置附近的航班</li>
<li><code>/debug/upload</code> (POST) 测试复杂数据上传</li>
</ul>
<h2 id="数据库设计"><a href="#数据库设计" class="headerlink" title="数据库设计"></a>数据库设计</h2><p>设计数据库之前，我们先要做一些准备工作：</p>
<ul>
<li>安装 Go 的数据库驱动 <code>go get github.com/go-sql-driver/mysql</code></li>
<li>连接到测试用数据库 <code>mysql -h[host ip] -P[host port] -u[user name] -p[password]</code></li>
<li>查看已有的数据库 <code>show databases;</code></li>
<li>新建数据库 <code>create database wptdb</code></li>
<li>使用该数据库 <code>use wptdb</code></li>
<li>清空某表 <code>truncate table history</code></li>
</ul>
<p>因为我们需要记录在线的人，所以得要一个表，叫做 <code>current</code>；另外我们需要记录历史纪录，所以需要另外一个表，叫做 <code>history</code>，这两个表的差别在于其中一个会对数据进行修改和删除，另一个则不会。</p>
<p>因为功能不同，这两个表的设计也不尽相同（主要是主键的选择）。<code>current</code> 表需要经常更新，<code>history</code> 表更多是记录轨迹和时间。</p>
<p>Current 表的初步设计，这里在基本信息中加入了一个 <code>region</code> 字段，用来标识所在分区，这样在检索的时候可以极大提高效率。为什么要用 BIGINT 这里说一下。经度范围 0-360，维度为 0-180，如果精确到 0.001 的话，就是<br><code>36000*18000=648,000,000</code>，加上为高度预留的 3 位，就是 12 位，是超过 INT 所能表示的大小的。时间戳为 UNIX 时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`current`</span> (</div><div class="line">    <span class="string">`uid`</span> <span class="built_in">INT</span>(<span class="number">10</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    <span class="string">`latitude`</span> <span class="keyword">DOUBLE</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    <span class="string">`longitude`</span> <span class="keyword">DOUBLE</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    <span class="string">`altitude`</span> <span class="keyword">DOUBLE</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    <span class="string">`timestamp`</span> <span class="built_in">INT</span>(<span class="number">10</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    <span class="string">`region`</span> <span class="built_in">BIGINT</span>(<span class="number">12</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    <span class="string">`online`</span> TINYINT <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    PRIMARY <span class="keyword">KEY</span> (<span class="string">`uid`</span>)</div><div class="line">);</div></pre></td></tr></table></figure>
<p>History 表的初步设计主要是保证每条数据的唯一性（也就是主键），以及同一个 id 的数据要尽可能放在一起方便检索。这里的 <code>actionid</code> 是由 <code>uid</code> 和 <code>timestamp</code> 拼接而成的。而 <code>location</code> 是由 经纬度和高度拼接而成的。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`history`</span>(</div><div class="line">    <span class="string">`actionid`</span> <span class="built_in">CHAR</span>(<span class="number">32</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    <span class="string">`location`</span> <span class="built_in">VARCHAR</span>(<span class="number">64</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    <span class="string">`timestamp`</span> <span class="built_in">INT</span>(<span class="number">10</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">    PRIMARY <span class="keyword">KEY</span> (<span class="string">`actionid`</span>)</div><div class="line">);</div></pre></td></tr></table></figure>
<h2 id="框架搭建"><a href="#框架搭建" class="headerlink" title="框架搭建"></a>框架搭建</h2><p>借助 beego，对于框架部分我们需要做的不多，主要就是配置好路由和对应的 Controller，具体可以参见代码。简单来说，就是新建一个 <code>struct</code> 包含 <code>beego.Controller</code>，然后对应写函数，并在路由中注册，比如：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><div class="line"><span class="comment">// router.go</span></div><div class="line"><span class="keyword">package</span> routers</div><div class="line"></div><div class="line"><span class="keyword">import</span> (</div><div class="line">	<span class="string">"watch-plane-together/controllers"</span></div><div class="line">	<span class="string">"github.com/astaxie/beego"</span></div><div class="line">)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</div><div class="line">    beego.Router(<span class="string">"/"</span>, &amp;controllers.MainController&#123;&#125;)</div><div class="line">    beego.Router(<span class="string">"/debug/"</span>, &amp;controllers.DebugController&#123;&#125;)</div><div class="line">    beego.Router(<span class="string">"/debug/position"</span>, &amp;controllers.DebugController&#123;&#125;, <span class="string">"post:DebugPosition"</span>)</div><div class="line">    beego.Router(<span class="string">"/debug/weather"</span>, &amp;controllers.DebugController&#123;&#125;, <span class="string">"get:DebugWeather"</span>)</div><div class="line">    beego.Router(<span class="string">"/debug/flight"</span>, &amp;controllers.DebugController&#123;&#125;, <span class="string">"get:DebugFlight"</span>)</div><div class="line">    beego.Router(<span class="string">"/debug/near"</span>, &amp;controllers.DebugController&#123;&#125;, <span class="string">"post:DebugNear"</span>)</div><div class="line">    beego.Router(<span class="string">"/debug/upload"</span>, &amp;controllers.DebugController&#123;&#125;, <span class="string">"post:DebugUpload"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们只需要在 <code>debug.go</code> 中对应编写 <code>DebugPosition</code>, <code>DebugWeather</code>, <code>DebugFlight</code> 和 <code>DebugUpload</code> 方法即可。这里需要注意，附近的人具体的算法没有实现，会在之后专门介绍。</p>
<h2 id="测试部署"><a href="#测试部署" class="headerlink" title="测试部署"></a>测试部署</h2><p>接口测试可以使用 Chrome 插件 Postman，部署的话，因为 Go 直接静态编译，扔到服务器上运行即可，或者参考<a href="http://beego.me/docs/deploy/" target="_blank" rel="external">这里</a></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>因为 beego 的缘故，其实只需要不到 200 行代码就可以完成基本的 demo 后台搭建，后面的 mvc 封装等等可以随着项目进行具体调整，作为一个简单的展示大约是足够的，进一步的学习就需要多多阅读源码，真正用 Go 的思路来写代码了。 </p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://github.com/wdxtub/Watch-Plane-Together/tree/master" target="_blank" rel="external">项目源代码</a></li>
<li><a href="https://astaxie.gitbooks.io/build-web-application-with-golang/content/zh/08.3.html" target="_blank" rel="external">8.3 REST</a></li>
<li><a href="https://github.com/go-sql-driver/mysql" target="_blank" rel="external">go-sql-driver/mysql</a></li>
<li><a href="http://www.cnblogs.com/good_hans/archive/2010/03/29/1700046.html" target="_blank" rel="external">Mysql 远程登录及常用命令</a></li>
<li><a href="http://blog.csdn.net/hanxuemin12345/article/details/7818662" target="_blank" rel="external">数据库中删除语句Drop、Delete、Truncate的相同点和不同点的比较（举例说明）</a></li>
<li><a href="http://blog.csdn.net/pandajava/article/details/45667001" target="_blank" rel="external">MySQL 插入条件判断</a></li>
<li><a href="http://blog.rpplusplus.me/blog/2014/03/18/yahoo-weather-api/" target="_blank" rel="external">Yahoo Weather API</a></li>
<li><a href="https://github.com/bitly/go-simplejson" target="_blank" rel="external">bitly/go-simplejson</a></li>
<li><a href="https://developer.yahoo.com/weather/archive.html" target="_blank" rel="external">Yahoo! Weather RSS Feed</a></li>
<li><a href="https://developer.yahoo.com/yql/console/" target="_blank" rel="external">Yahoo YQL Console</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面已经了解了『一起看飞机』的基本需求，这次我们来搭建一个完整的基于 beego 框架的后台。同时也会涉及调试测试部署等一系列配套工作，我觉得这些反而是工作中很重要的能力，但不知道为啥大部分书都略过了。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="Go" scheme="http://wdxtub.com/tags/Go/"/>
    
      <category term="项目" scheme="http://wdxtub.com/tags/%E9%A1%B9%E7%9B%AE/"/>
    
      <category term="飞机" scheme="http://wdxtub.com/tags/%E9%A3%9E%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>【Go 之旅】III 项目实战：一起看飞机 - 项目介绍</title>
    <link href="http://wdxtub.com/2016/08/24/go-plane-1/"/>
    <id>http://wdxtub.com/2016/08/24/go-plane-1/</id>
    <published>2016-08-24T14:57:34.000Z</published>
    <updated>2016-08-24T15:06:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>经过之前的两篇文章，应该已经对 Go 有了一定的了解，不过想要真正掌握，还是得做项目，这里我结合工作的内容，设计了『一起看飞机』这么一个无厘头项目，将通过三篇文章来介绍具体的设计和实现。</p>
<a id="more"></a>
<hr>
<p>实话实说，因为我对博客的热爱，本来我是打算做一个博客系统的，简单搜索了一下，已经有几百个轮子了，所以我只好开了这么一个奇怪的脑洞。</p>
<h2 id="总体介绍"><a href="#总体介绍" class="headerlink" title="总体介绍"></a>总体介绍</h2><p>假设有很多飞机的狂热爱好者，假设这些爱好者都喜欢看飞机，假设这些看飞机的爱好者想要一起看飞机，在这么多假设时候，产品经理感觉找到了一个刚需，所以决定开发一个『一起看飞机』的 APP，帮助飞机爱好者凑到一起看飞机。</p>
<p>为了早日上线，产品经理决定第一个版本只需要做很少的功能，甚至都不需要一个图形界面，用文字先凑合就行。我花了五分钟花了一个草图，产品经理表示可以，如下：</p>
<p><img src="/images/14720510281192.jpg" alt=""></p>
<p>只能说，简约不简单！</p>
<h2 id="基本功能"><a href="#基本功能" class="headerlink" title="基本功能"></a>基本功能</h2><p>好吧，其实简约也简单，具体需要的功能是</p>
<ul>
<li>告知用户附近的人，包括距离和昵称</li>
<li>获取位置告知用户当前的天气、温度、风向</li>
<li>获取用户附近的航线</li>
</ul>
<p>如果不考虑具体展示的话，其实就是维护一个数据库，然后在用户上报位置的时候做一些查询并返回。</p>
<p>虽然看起来很简单，但是也已经足够我们通过这个无厘头项目来了解 Go 了，将会涉及的后台相关技术和原理有：</p>
<ul>
<li>Web 基本工作方式</li>
<li>连接数据库</li>
<li>处理 JSON 格式的文件</li>
<li>Socket 与 WebSocket</li>
<li>RESTful API 的设计和实现</li>
<li>数据的简单加密和解密</li>
<li>错误处理、调试和测试</li>
<li>部署与维护</li>
</ul>
<p>当然我们也会简单涉及一些客户端（Web 和 iOS）的编程，以及创建一些自己用的调试用接口。</p>
<p>接下来的文章中，我会从后台开始搭建起，尽量用简单且清晰的方式把 Web 开发的方方面面展现给大家。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://github.com/wdxtub/Watch-Plane-Together/tree/master" target="_blank" rel="external">项目源代码</a></li>
<li><a href="https://astaxie.gitbooks.io/build-web-application-with-golang/content/zh/index.html" target="_blank" rel="external">Go Web 编程</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经过之前的两篇文章，应该已经对 Go 有了一定的了解，不过想要真正掌握，还是得做项目，这里我结合工作的内容，设计了『一起看飞机』这么一个无厘头项目，将通过三篇文章来介绍具体的设计和实现。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="Go" scheme="http://wdxtub.com/tags/Go/"/>
    
      <category term="项目" scheme="http://wdxtub.com/tags/%E9%A1%B9%E7%9B%AE/"/>
    
      <category term="飞机" scheme="http://wdxtub.com/tags/%E9%A3%9E%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>Git 指南</title>
    <link href="http://wdxtub.com/2016/08/23/git-guide/"/>
    <id>http://wdxtub.com/2016/08/23/git-guide/</id>
    <published>2016-08-23T14:37:08.000Z</published>
    <updated>2016-08-23T14:39:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>虽然网上 Git 相关指南已经够多了，不过没有自己整理过一次，用得时候还是经常忘，所以这里按照自己的思路来写一篇，跟大家分享一下。</p>
<a id="more"></a>
<hr>
<p>本指南会按照自己工作的经验来进行相关命令的选择，力求贴近实战。慢慢更新中</p>
<h2 id="检出仓库"><a href="#检出仓库" class="headerlink" title="检出仓库"></a>检出仓库</h2><p>一般来说，刚到公司，参与某个项目，第一件事情就是看代码，在开了项目的 Git 权限之后，我们要做的第一步就是把代码克隆到本地，命令为：</p>
<p><code>git clone username@host:/path/to/repository</code></p>
<h2 id="本地仓库结构"><a href="#本地仓库结构" class="headerlink" title="本地仓库结构"></a>本地仓库结构</h2><p>本地仓库由 git 维护的三棵“树”组成。第一个是你的<strong>工作目录</strong>，它持有实际文件；第二个是<strong>暂存区（Index）</strong>，它像个缓存区域，临时保存你的改动；最后是 <strong>HEAD</strong>，它指向你最后一次提交的结果。</p>
<p><img src="/images/14719631395607.jpg" alt=""></p>
<h2 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h2><p>在熟悉了代码之后，组长交给你一个任务。因为是多人共同开发，所以用分支来进行隔离。</p>
<ul>
<li>创建分支 <code>git checkout -b new_branch_name</code></li>
<li>如果主分支经常改动，用 <code>git pull</code> 拉取最新的代码</li>
</ul>
<p>因为我们都是武艺高强的程序员，所以很快就完成了开发和测试，需要提交代码</p>
<ul>
<li>添加文件 <code>git add *</code></li>
<li>提交到 HEAD <code>git commit -m &quot;message&quot;</code></li>
<li>推送改动 <code>git push origin new_branch_name</code></li>
<li>然后可以在 Gitlab 网页中发起合并请求，等组长 review 通过后便可以进行代码合并</li>
<li>删除分支 <code>git branch -d new_branch_name</code></li>
</ul>
<h2 id="意外处理"><a href="#意外处理" class="headerlink" title="意外处理"></a>意外处理</h2><p>如果操作失误，可以使用 <code>git checkout -- filename</code> 来替换最后一次提交的文件。</p>
<p>如果想丢弃本地所有改动并与服务器同步，可以使用 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">git fetch origin</div><div class="line">git reset --hard origin/master</div></pre></td></tr></table></figure>
<h2 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h2><ul>
<li><code>git init</code> 用来创建新仓库，不过在公司里一般有内部的 Gitlab，很少需要用到</li>
<li><code>git clone /path/to/repository</code> 克隆本地仓库，一般比较少用到</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://rogerdudler.github.io/git-guide/index.zh.html" target="_blank" rel="external">git - 简明指南</a></li>
<li><a href="https://github.com/xirong/my-git/blob/master/git-workflow-tutorial.md" target="_blank" rel="external">Git Workflows and Tutorials</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然网上 Git 相关指南已经够多了，不过没有自己整理过一次，用得时候还是经常忘，所以这里按照自己的思路来写一篇，跟大家分享一下。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="Git" scheme="http://wdxtub.com/tags/Git/"/>
    
      <category term="版本控制" scheme="http://wdxtub.com/tags/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>跑步与技术债</title>
    <link href="http://wdxtub.com/2016/08/22/running-and-tech-debt/"/>
    <id>http://wdxtub.com/2016/08/22/running-and-tech-debt/</id>
    <published>2016-08-22T13:49:48.000Z</published>
    <updated>2016-08-22T15:39:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>单纯跑步是很快乐的，但是有一个配速，就要努力跑。单纯做技术也是快乐的，但是有一堆技术债，就不只是努力可以搞得定的了。今天我们来聊聊技术债的问题。</p>
<a id="more"></a>
<hr>
<p>关于技术债的文章大多以非常规范的口吻一板一眼在写，读起来有些膈应，所以我决定写一篇接地气的，正好最近把公司里软件相关的项目接触了个遍，在对接的时候还是深深能感受到短平快带来的技术债。一家之言，随便聊聊，以下所有内容都属于梦话，我就这么一写，您就这么一看，关了网页就忘掉吧。</p>
<p>每天我跑步回家总路程大约是 4.3 公里，需要过四个十字路口和一个天桥。四个十字路口有两个在第一公里，第二和第四公里各有一个，而天桥也是在第四公里。</p>
<p>那么问题来了，如果我想要把全程的配速稳定在五分钟以内，具体需要怎么分配时间？</p>
<p>理想情况下当然是匀速，但是因为马路并不是像跑步机这样的恒定的理想环境，就需要处理各种各样的意外情况了。比如：</p>
<ul>
<li>在第一公里因为两个十字路口的原因花了六分钟</li>
<li>在第四公里的天桥处为了节省体力走路上台阶和爬坡</li>
<li>因为没有午休在第三公里后体力迅速枯竭</li>
</ul>
<p>要怎么办？</p>
<p>先说说我的策略</p>
<ul>
<li>第一公里：因为等待十字路口红绿灯的时间不可控也没有办法跳过，所以得尽可能在不受红绿灯制约的路上『要回来一点时间』，包括预估信号灯的变化并据此调整速度（比如说离路口还有一百米，看到是红灯，其实就可以稍微放慢点速度以节省体力）只要能在 5 分 30 秒之内完成，后面就不会太辛苦（后面的平均时间需要 4 分 50 秒以下）</li>
<li>第二公里：只有一个十字路口，后面将是路况较好的路段，所以在十字路口前可以适当保留体力，然后进入较高速的匀速阶段（大约 4 分 30 秒左右）</li>
<li>第三公里：几乎全程无阻碍（除了下班的人群），是为未来保留余量的最佳时机（大约 4 分 30 秒左右）</li>
<li>第四公里：体力需要精打细算，一般来说经过天桥的折磨，后面得依靠呼吸和发力部位来调整（能在 5 分 20 秒左右完成就很好）</li>
</ul>
<p>幸运的是一般来说还是能勉强达标的，可是做技术就没有跑步这么简单粗暴了。</p>
<ul>
<li>老板催得急，要的就是短平快，粗糙就粗糙吧，质量差点没关系</li>
<li>人力不够，不停被打断，很多应完成的计划都处于做了一点但是又没做完的尴尬境地</li>
<li>没有测试，祈祷式编程</li>
<li>没有文档或者文档除了原来的开发团队谁都看不懂，没办法拓展</li>
<li>架构设计随意或者过分依赖框架，最终一大堆代码耦合在一起成了烂摊子谁都不愿意管</li>
<li>重复工作没有自动化，不得不耗费大量时间在零碎小事上</li>
<li>团队沟通不畅，下面的人忙的要死上面的人却感觉大家都很空闲</li>
<li>朝令夕改，各种临时的紧急需求</li>
<li>步调不一，有种无头苍蝇般瞎忙</li>
<li>流程混乱，同一个事情绕来绕去，当然一定要说为了『安全』，只能说从一开始就不信任员工吧</li>
</ul>
<p>就好像第一公里跑了十五分钟，后面跑得再快，也没办法追回来了。但是老板却又觉得肯定是能追回来的，那么结果倒是蛮清晰的，就看哪一天弦绷断了呗。</p>
<p>以上除了跑步部分都是道听途说和胡拼八凑，没有任何一点对号入座的意思。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;单纯跑步是很快乐的，但是有一个配速，就要努力跑。单纯做技术也是快乐的，但是有一堆技术债，就不只是努力可以搞得定的了。今天我们来聊聊技术债的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="Thinking" scheme="http://wdxtub.com/categories/Thinking/"/>
    
    
      <category term="跑步" scheme="http://wdxtub.com/tags/%E8%B7%91%E6%AD%A5/"/>
    
      <category term="技术债" scheme="http://wdxtub.com/tags/%E6%8A%80%E6%9C%AF%E5%80%BA/"/>
    
      <category term="人才" scheme="http://wdxtub.com/tags/%E4%BA%BA%E6%89%8D/"/>
    
  </entry>
  
  <entry>
    <title>主流程序员的自我修养</title>
    <link href="http://wdxtub.com/2016/08/21/im-a-programmer-too/"/>
    <id>http://wdxtub.com/2016/08/21/im-a-programmer-too/</id>
    <published>2016-08-21T14:10:16.000Z</published>
    <updated>2016-08-21T15:05:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>工作也有一段时间了，无论是亲身实践还是观察他人，或多或少能意识到怎么样才是一个『好』的工程师。本文是我的一点思考。</p>
<a id="more"></a>
<hr>
<p>在美国的时候，教授常常跟我说，engineering 和 engineer 在大家看来是特别神圣的词，因为这意味着他们做的事情和他们本身，都是为了解决问题而存在的。而在开源运动风风火火的今天，写代码其实只是工程师应该具备的能力的一小部分。还有很多代码之外的东西，需要去学习和实践。</p>
<p>调试是最基本的技能之一，IDE 是如此强大，搞得很多人以为调试就是设个断点或者输出一下值，但其实并不是这样的，或者说，这不是问题的核心。那么问题的核心是什么？</p>
<p><strong>掌控</strong></p>
<p>通过断点或输出来分离代码确定错误点，然后进行修复。这里一定要弄清楚是因为程序员的无心之失导致的错误，还是因为系统设计的模糊性导致的实现不一致。前者只牵一发，后者要动全身。而对于需要 7x24 运行的服务来说，对于服务质量的掌控，很多时候是通过日志实现的。日志这个是个大话题，通过汇总线上的各类数据，可以确定重点代码进行重点优化，尤其是在计算资源或者带宽资源吃紧的时候，从数据出发才是有的放矢。一般来说需要注意两个地方，一是 IO，二是资源共享。</p>
<p>IO 部分的优化通常是要么是利用缓存，尽量一次多做一点事情，要么是减少数据的传输量，不过这个无形之中增加了设计的复杂度，具体使用的时候需要权衡。资源共享部分涉及某种确定资源使用的机制，比方说锁或者信号量之类的，这部分本身就是很难的，比较好的办法是设计时就尽量隔离，不要过分依赖于资源共享。</p>
<p>另一个增加掌控的方法是写文档，文档可以看作是在时间尺度上的掌控，因为认真写文档的代码，即使过几年再看，也很容易跟上当时的思路，否则就容易迷失在茫茫跳转中。而面对糟糕的代码，文档其实也是很好的重构工具，尤其是在测试紧缺的情况下，通过文字辅助理解可能是最为保险的选择。</p>
<p>关于团队其实是另一个话题，不过说到底原则其实很简单：把自己手头上的事情用心做好，并以此带动同事，成为值得信任的人，才有可能赢得信任。今天看女排赛后采访，听郎平说了两个词『承担』和『包容』，的的确确是成为顶梁柱所必备的素质。</p>
<p>工程师应该把时间花在关键问题上，无聊且机械化的事情，哪怕第一次可能会比较麻烦，必须要交给机器来做。所谓关键问题，就是那些能真正创造价值，或者能让创造价值的过程更好的问题。更重要的是，要让提供资源的人能够意识到解决这些问题的价值。</p>
<p>不过话说回来，掌控也需要一个度，也要容忍一定的黑盒，总不能啥都重新发明一次轮子吧。所谓『掌控』，不是啥都亲力亲为，最后累得死去活来还不一定能达到效果，而是找到关键点，用最少的成本来达到自己想要的效果。</p>
<p>没错，我说的就是《永恒的终结》。观测师和程序员其实挺像的，而我也一定会像小说中那样，愿意为自己珍惜的东西，拼尽一切。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;工作也有一段时间了，无论是亲身实践还是观察他人，或多或少能意识到怎么样才是一个『好』的工程师。本文是我的一点思考。&lt;/p&gt;
    
    </summary>
    
      <category term="Thinking" scheme="http://wdxtub.com/categories/Thinking/"/>
    
    
      <category term="程序员" scheme="http://wdxtub.com/tags/%E7%A8%8B%E5%BA%8F%E5%91%98/"/>
    
      <category term="能力" scheme="http://wdxtub.com/tags/%E8%83%BD%E5%8A%9B/"/>
    
  </entry>
  
  <entry>
    <title>第十周 - 茶酒伴</title>
    <link href="http://wdxtub.com/2016/08/19/tea-wine-friend/"/>
    <id>http://wdxtub.com/2016/08/19/tea-wine-friend/</id>
    <published>2016-08-19T14:44:32.000Z</published>
    <updated>2016-08-19T16:04:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>青山在，杨柳湾，无人等风来。杏花开，绿水盘，新芽莫摘采。知是一年春又来，冰雪也消散。山路若陡缓步迈，斗酒洒羁绊。</p>
<a id="more"></a>
<hr>
<p>这周平静却暗藏汹涌。周一从广州直接赶回深圳上班，周二到周五跑了四天，终于借着奥运会的东风成绩有所突破。因为台风的缘故，白天总是下雨，所以打着伞穿着拖鞋赶车上班，下班的时候总不能把第二天要用的伞和拖鞋留在公司，所以只好左手拖鞋右手伞往家里跑，路人一定很诧异为啥有个人疯疯癫癫气喘吁吁手里拿着这么奇怪的组合，没想到以这种奇葩的形式不走寻常路。</p>
<p>这两周和另一个老司机一起把公司的数据和日志平台搭建好并正式启用。因为各种历史遗留原因，我们一边填之前别人挖的坑以保证服务质量，一边着手搭建新的更加通用和规范的平台。写文档、设规范流程、重构代码居然能在两周之内搞完，我都被这样的效率震惊了。现在后台的开发和维护已经慢慢走上正轨，总算是有了一个好的地基了。</p>
<p>不过做这些填坑的活简单来说就是没有太多汇报的东西，领导并不会在意代码质量，很多时候他们在意的是能汇报些什么，不同决策之间也没有同步好，总体来说还有成吨地方可以改进。现在慢慢可以理解为什么跨部门合作一定要抄送对方的领导了，不然人家完全不鸟你好嘛。所以这样的后果就是大家都只做可以汇报的工作，于是越来越追求短平快，能用但摇摇欲坠，最终积重难返。</p>
<p>只能说我做事的风格不是这样的，我也不想改变，还是要做真正有价值对得起自己的事情，从我手上交出去的，一定是达到我自己的标准的，而不是简单做完就好，希望能让我看到事情在往好的方面发展的迹象吧。</p>
<p>关于写书，最近有比较大的思路调整（感谢我的编辑及时给我寄来的相关新书），也正式开始正文部分的写作和润色，真的是『鬼知道我经历了什么』。不过目前来看我还是比较满意自己的写作质量的，希望这两三个月能够持续输出，交出一份漂亮的答卷。另外因为自己写了一系列关于 ELK 的文章，（又）有出版社的编辑来约稿，不过因为时间精力能力的原因只能往后排，相信经过更多时间的磨练，写出来的东西会更好吧。这也再次说明了写博客的好处，做的哪怕是微小的工作，也要努力让大家知道，不然就成了小透明了（在公司里我也要想办法展示自己，但绝不要踩在别人头上的那种）。</p>
<p>最后插播两条新闻，一是分别针对国内外的读者部署的两套博客，通过 DNS 解析来定位，应该是极大提高了国内读者的访问体验。二是 Dota2 Ti6 Wings 战队夺冠，真乃『护国神翼』，我们太需要一个这样的故事，专注于本职工作的团队才最有资格获得最高荣誉，而不是那些直播和卖饼的。</p>
<p>这周完美执行了减肥和健身计划（吃了五天沙拉），下周要继续努力。</p>
<p>雨生哀，镜尘满，韶颜虽易改。弃脂彩，肠何断，拂手去青苔。衷心诉了春过半，平生光影短。儿女情长愁摩愁，不如茶相伴。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;青山在，杨柳湾，无人等风来。杏花开，绿水盘，新芽莫摘采。知是一年春又来，冰雪也消散。山路若陡缓步迈，斗酒洒羁绊。&lt;/p&gt;
    
    </summary>
    
      <category term="Gossip" scheme="http://wdxtub.com/categories/Gossip/"/>
    
    
      <category term="生活" scheme="http://wdxtub.com/tags/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="周记" scheme="http://wdxtub.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
</feed>
