<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小土刀</title>
  <subtitle>Agony is my triumph</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wdxtub.com/"/>
  <updated>2016-12-03T06:27:20.000Z</updated>
  <id>http://wdxtub.com/</id>
  
  <author>
    <name>wdxtub</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【不周山之操作系统】Linux 概念指南</title>
    <link href="http://wdxtub.com/2016/12/03/linux-concept-guide/"/>
    <id>http://wdxtub.com/2016/12/03/linux-concept-guide/</id>
    <published>2016-12-03T00:19:09.000Z</published>
    <updated>2016-12-03T06:27:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>如果说计算机科学的三大浪漫是操作系统、编译原理和计算机图形学的话，谈及操作系统，Linux 就一定是那个程序员对它又爱又恨的存在。本文带大家了解 Linux 中的基本概念和原理，正所谓知其然也要知其所以然。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.12.03: 初稿完成</li>
</ul>
<h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><ul>
<li><a href="http://wdxtub.com/2016/12/03/linux-concept-guide/">Linux 概念指南</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>了解 Linux 的基本概念</li>
<li>理解 Linux 的架构和背后的设计思考</li>
<li>初步掌握文件系统的操作和原理</li>
<li>对管道、进程和进程间通信有简单的感性认识</li>
</ol>
<h2 id="当我们谈论-Linux-时我们在谈论什么"><a href="#当我们谈论-Linux-时我们在谈论什么" class="headerlink" title="当我们谈论 Linux 时我们在谈论什么"></a>当我们谈论 Linux 时我们在谈论什么</h2><p>Linux 的出现其实是一位大学生的心血来潮，Linus Torvalds（就是 Linux 之父）不满意当年学习操作系统时所使用的 Minix 系统，在其代码的基础上参考 Unix 的设计，写出了第一版 Linux 内核。之后 Linus 开源了代码，随着网络时代的大幕逐渐揭开，Linux 和 GNU 金风玉露一相逢，在开源协议下迅速发展成熟。</p>
<p>正所谓『不懂 Unix 的人注定最终还要重复发明一个蹩脚的 Unix』，通过学习 Linux 来掌握 Linux/Unix 的核心思想其实是非常有意义的。作为诸多天才的智慧结晶，能够从中偷师一星半点，也能受益匪浅。（新闻插播：2016.12.03 Solaris 操作系统将终止开发）</p>
<p>准确来说，Linux 其实只是一个内核，负责管理硬件和为上层应用提供接口。不过随着 Linux 概念的不断外延，现在提到 Linux，更多是指以 Linux 内核为基础配上各种应用的 Linux 发行版本（比如 Ubuntu, Debian 等等）。这个系列的文章不会过多着眼于各个发行版，而是专注于 Linux 内核和系统基本概念本身，比如操作系统中重要的抽象：文件系统、输入输出操作、进程、线程和进程间通信。</p>
<h2 id="常见-Linux-发行版简介"><a href="#常见-Linux-发行版简介" class="headerlink" title="常见 Linux 发行版简介"></a>常见 Linux 发行版简介</h2><p>因为 Linux 开源的特性，各种不同的发行版层出不穷，感兴趣的同学可以在 <a href="https://zh.wikipedia.org/wiki/Linux%E5%8F%91%E8%A1%8C%E7%89%88%E5%88%97%E8%A1%A8" target="_blank" rel="external">维基百科 - Linux 发行版列表</a> 这个条目中看到各式各样的发行版及简介，也可以在 <a href="http://distrowatch.com/" target="_blank" rel="external">distrowatch.com</a> 查看更加详细的排名，这之中比较流行的发行版有：</p>
<ul>
<li>ArchLinux，一个基于 KISS(Keep It Simple and Stupid) 的滚动更新的操作系统。</li>
<li>CentOS，从 Red Hat 发展而来的发行版，由志愿者维护，旨在提供开源的，并与 Red Hat 100%兼容的系统。比较稳定，不用动不动就升级。</li>
<li>Debian，一个强烈信奉自由软件，并由志愿者维护的系统。</li>
<li>Elementary OS：基于 Ubuntu，接口酷似 Mac OS X。</li>
<li>Fedora，是 Red Hat 的社区版，会经常引入新特性进行测试。</li>
<li>Gentoo，一个面向高级用户的发行版，所有软件的源代码需要自行编译。</li>
<li>Linux Mint，从 Ubuntu 派生并与 Ubuntu 兼容的系统。</li>
<li>openSUSE，最初由 Slackware 分离出来，现在由 Novell 维护。用起来还是比较生涩。</li>
<li>Red Hat Enterprise Linux，Fedora 的商业版，由 Red Hat 维护和提供技术支持。</li>
<li>Ubuntu，一个非常流行的桌面发行版，由 Canonical 维护。基本上日常常用的就是它了。</li>
</ul>
<p>因为手头上只有基于 Ubuntu 的虚拟机（包括 Win10 中自带的 Linux），所以接下来的示例都是基于 Ubuntu 14.04 LTS 的。</p>
<h2 id="按下开机键之后"><a href="#按下开机键之后" class="headerlink" title="按下开机键之后"></a>按下开机键之后</h2><p>虽然现在我们使用的云主机基本都已经预装好了 Linux，也不需要自己去操心开机，但是操作系统毕竟不是凭空出现的，了解从按下开机键到操作系统启动之间的过程有助于我们深入理解计算机系统。整个过程的步骤如下：</p>
<ol>
<li>按下开机键</li>
<li><strong>BIOS 步骤</strong>：计算机从主板的 BIOS(Basic Input/Output System) 中读取存储的程序</li>
<li><strong>MBR 步骤</strong>：该程序从存储设备中读取起始的 512 字节数据（称为主引导记录 Master Boot Record, MBR）</li>
<li><strong>Boot Loader 步骤</strong>：MBR 告诉计算机从哪个分区(Partition)来载入引导加载程序(Boot Loader)，Boot Loader 保存了操作系统的相关信息</li>
<li><strong>Kernel 步骤</strong>：Boot Loader 根据所存储的信息加载内核(Kernel)，内核主要的任务是管理计算机的硬件资源</li>
<li><strong>Init 步骤</strong>：内核会为自己预留内存空间，然后进行硬件检测，之后启动 init 进程（1 号进程），之后的操作会由 init 进程来接管</li>
<li><strong>初始化脚本步骤</strong>：如果没有进入单用户模式，就会为操作系统启动做各种初始化工作，包括计算机基本信息、文件系统、硬盘、清理临时文件、设置网络等等</li>
<li><strong>登录步骤</strong>：操作系统准备好之后，我们就可以用用户名和密码登录到计算机中，我们成为了一个用户，属于某个用户组</li>
</ol>
<h2 id="Linux-的架构"><a href="#Linux-的架构" class="headerlink" title="Linux 的架构"></a>Linux 的架构</h2><p>现在我们有了一个可以运行的 Linux 操作系统，具体它是怎么工作的呢？这就要从架构说起了。</p>
<p><img src="/images/14807342792897.jpg" alt=""></p>
<p>最底层是硬件，硬件之上是内核，前面说内核负责管理所有的硬件资源的意思是，所有的计算机操作都需要通过内核传递给硬件。如果接触过硬件的同学一定知道，硬件本身是颇为复杂的，即使有了内核代为管理，仍旧非常繁琐，所以在内核之上我们有了系统调用。我们不需要了解内核和硬件的细节，就可以通过系统调用来操作它们，系统调用是操作系统的最小组成单位，也就是说，计算机能做的所有操作，最终能且仅能分解成已有的系统调用。</p>
<p>我们可以看到，内核实际上是硬件的抽象，而系统调用是内核的抽象，在这之上的 shell 和 library 甚至应用程序其实是更高层次的抽象，正是通过这样一层一层的抽象，计算机才得以发展成为如今这么庞大却简洁的系统。</p>
<p>我们在命令行中输入 <code>man 2 syscalls</code> 就可以浏览系统调用的说明了，顺着列表往下滑，就可以看到一些我们常常使用的命令了，比如 <code>chmod</code>, <code>fork</code>, <code>kill</code> 等等。反应快的同学应该已经意识到了，这些命令不就是我们在 shell 中常常使用的嘛，原来它们就是系统调用！</p>
<p>现在最常用的 shell 叫做 bash，其他诸如 zsh, fish 等也各有各的拥趸。这里要具体说一下 shell 和终端(Terminal)的不同，在大型机时代，终端是一个硬件设备，用来进行输入输出，而随着计算机硬件的发展，终端已经慢慢从实体变成了一个概念。我们打开 Gnome Terminal 的 About 页面，就可以发现下面的介绍是这样写的：</p>
<p><img src="/images/14807358566249.jpg" alt=""></p>
<p>注意这个说法 “A terminal emulator for the GNOME desktop”，什么是 emulator 呢？中文翻译叫做仿真器，等于是说，这个程序是一个仿真终端的程序。与 emulator 相关的一个非常容易混淆的概念是 simulator（模拟器），他们的差别在于：</p>
<ul>
<li>仿真器。通过软件方式，精确地在一种处理器上仿真另一种处理器或者硬件的运行方式。其目的是完全仿真被仿真硬件在接收到各种外界信息的时候的反应。</li>
<li>模拟器。通过某种手段，来模拟某些东西。不一定要完全正确的原理，追求的只是尽可能的相像。</li>
</ul>
<p>我们找一个 Mac OS 上最流行的终端的介绍来看看，同样会发现，这是一个仿真器：</p>
<p><img src="/images/14807362345090.jpg" alt=""></p>
<p>所以可以这样理解，现代计算中的终端是一个用软件仿真的终端，我们在这上面输入输出的命令会传给具体执行这些命令的 shell 程序，再由 shell 程序执行对应的系统调用。重要的事情说三遍：终端不是 shell，终端不是 shell，终端不是 shell。</p>
<p>因为系统调用是操作系统的最小功能单位，所以一般来说提供的功能是非常零碎的，我们完成一个操作一般需要多个系统调用进行配合，于是 Linux 定义了一些 library（库），将常见的系统调用组合打包成各种功能。如果说系统调用是笔画的话，那么库函数大概就是偏旁部首了。一般来说 Linux/Unix 系统都会有 ISO C 标准库和 POSIX 标准库，用来保证不同平台的兼容性。</p>
<p>在 shell 和 library 的基础上，我们就可以构造各式各样强大的应用了，当然除了这两种方式外，也可以根据需要自己进行系统调用。</p>
<p>至此，我们就简单介绍了 Linux 架构中的各个层级：</p>
<ul>
<li>内核是软硬件的桥梁</li>
<li>系统调用是应用与内核的桥梁，一方面隐藏了内核的复杂性，另一方面提高了应用的可移植性</li>
<li>库实际上是系统调用组成的模块化功能</li>
<li>shell 实际上是一种方便我们操作计算机的机制</li>
</ul>
<p>在图形化界面出现之前，在命令行中输入命令是跟电脑交互的主要方式。而在图形化界面出现这么多年之后，命令行依然扮演者举足轻重的角色，一是因为简单粗暴，二是因为可以方便地自动化流程化。</p>
<h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>简单来说，文件系统是 0 与 1 的逻辑组织形式，常见的抽象是文件和目录。在 Linux 中，文件系统是一个树结构，树的根就是我们常常能看到的根目录 <code>/</code>，每一个分叉表示一个文件夹，如下图所示：</p>
<p><img src="/images/14807377801394.jpg" alt=""></p>
<p>文件名加上从根目录到该文件所在目录的目录名就构成了一个路径。对于目录来说，里面至少会包含两个条目：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">.       指向当前目录</div><div class="line">..      指向父目录</div></pre></td></tr></table></figure>
<p>当一个文件被放入到目录中，实际上就是建立了一个到该文件的硬链接(hard link)，当对这个文件的硬链接数目为零的时候，文件实际上就被删除了。不过现在基本都使用软链接(soft link)，类似于 windows 中的快捷方式，不会影响链接数目。</p>
<p>我们能对文件进行三种操作：</p>
<ul>
<li>读取 Read: 获取数据</li>
<li>写入 Write: 创建新文件或在旧文件中写入数据</li>
<li>运行 Execute: 文件是可执行的二进制代码，那么会被载入内存进行执行</li>
</ul>
<p>但是三种操作都有各自的权限，我们使用 <code>ls -l filename</code> 就可以看到详情，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">wdxtub@ubuntu:~/GO/bin$ ls -l -rwxrwxr-x 1 wdxtub wdxtub 11277064 Sep 14 10:35 bee</div><div class="line">wdxtub@ubuntu:~/GO$ ls -l bin</div><div class="line">drwxrwxr-x 2 wdxtub wdxtub 4096 Sep 14 10:35 bin</div></pre></td></tr></table></figure>
<p>这里简单介绍下各个字段的含义：</p>
<ul>
<li>第一个字符，如果是 <code>-</code> 表示常规文件，如果是 <code>d</code> 表示目录</li>
<li>后面的九个字符表示 owner, owner group 和 other 的权限，rwx 分别代表读取、写入和执行，如果是 <code>-</code> 则表示没有对应的权限</li>
<li>第二列的数字是 hard link 的数目</li>
<li>第三、四列是所属的用户和用户所在的用户组</li>
<li>第五列是文件大小，单位是字节 byte</li>
<li>最后的是上一次写入的时间</li>
</ul>
<p>文件系统的使用基本上就是这些内容，但是这样的一个文件系统到底是怎么实现的呢？这又要从存储设备说起了。前面提到，存储设备的前 512 字节是 MBR，用于开机启动，剩余的空间可能会被分为多个分区(partition)，每个分区有对应的分区表(partition table)来记录分区的相关信息（比如起始位置和分区大小）。需要注意的是，分区表并不保存在该分区中，不然万一分区挂了，连最关键的分区表都找不到了。</p>
<p>每个分区大概的样子是这样的：</p>
<p><img src="/images/14590056997684.jpg" alt=""></p>
<ul>
<li>Boot block 是为计算机启动而准备的，在 MBR 指定启动分区之后，就会把 Boot block 部分的程序读入内存执行。为了方便管理，即使该分区没有操作系统，仍然会预留 Boot block</li>
<li>Super block 存储文件系统的信息，比如类型、inode 数目和数据块的数目</li>
<li>inodes 是文件存储的关键，每个文件对应一个 inode，inode 中包含指向具体数据的指针，读取的时候根据这些指针进行数据读取即可</li>
<li>Data blocks 就是具体的数据了，我们通过 inode 中的指针来进行访问</li>
</ul>
<p>关于 inode 的具体实现细节这里因为篇幅所限就不展开了，会在系列后面的文章中进行介绍。</p>
<h2 id="管道与流"><a href="#管道与流" class="headerlink" title="管道与流"></a>管道与流</h2><p>在 Linux 中 “Everything is a stream of bytes”，用设计模式的话说其实这就是一个数据流导向的设计，信息在不同的应用之间流动，最终成为我们所需要的信息。Linux 在执行程序的时候，会自动打开三个流：</p>
<ul>
<li>标准输入(Standard Input)</li>
<li>标准输出(Standard Output)</li>
<li>标准错误(Standard Error)</li>
</ul>
<p>我们可以按需进行使用。而如果我们想把一个程序的标准输出作为另一个程序的标准输入，就需要使用管道(pipeline)了。而正是因为这样的机制，我们可以把诸多小功能组合成强大的应用，一个简单的例子是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">wdxtub@ubuntu:~$ cat hello.txt welcome to wdxtub.comwdxtub@ubuntu:~$ cat hello.txt | wc -w3</div></pre></td></tr></table></figure>
<h2 id="进程与进程组"><a href="#进程与进程组" class="headerlink" title="进程与进程组"></a>进程与进程组</h2><p>最基础的操作是指令，一堆指令在一起就是程序，而进程就是程序的具体实现，也就是把程序载入到内存中并执行的过程。操作系统的重要功能之一便是对进程进行从摇篮（分配内存空间）到坟墓（回收）的管理。我们先执行如下命令看看 <code>ps -eo pid,comm,cmd</code>（列出全部进程并展示 pid, command 和 cmd 信息）</p>
<p><img src="/images/14807437718754.jpg" alt=""></p>
<p>这里每一行都是一个进程，第一列是 pid，相当于身份证号；第二列是进程的简称；第三列是进程启动时候的命令。如果我们往上滚动，就会找到这样的一行 <code>1 init /sbin/init</code>，这个就是内核建立的唯一一个进程了，剩下的进程都是 init 通过 fork 方式创建的，也就是说，所有的其他进程都是 init 的子进程。</p>
<p>子进程终结的时候会通知父进程进行内存空间的回收，而如果父进程比子进程还早终结，那么这个子进程就会被过继给 init 进程，并由 init 进程通过调用 <code>wait</code> 函数进行回收。如果无法正确回收，那么这个子进程就成为了僵尸进程，所占据的内存空间就无法被访问了。</p>
<p>除了父子进程的关系外，还有一个进程组(process group)的概念：每个进程组中有多个进程，进程组的 pid 由进程组 leader 的 pid 决定。而多个进程组还可以组成一个会话(session)，会话使得前台和后台程序得以展示出来。当我们创建了多个终端窗口，实际上就创建了多个会话，每个会话都有其前台和后台进程。</p>
<h2 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h2><p>前面介绍了进程，但是进程之前如果想要交互怎么办？除了管道之外，有没有其他方法？当然有也必须要有。其中最简单的一种就是信号，所谓信号就是一个整数，一个由进程 A 发送给进程 B 的整数。因为一个整数所能携带的信息量有限，所以一般用于系统管理。</p>
<p>信号的传递机制也很简单，由内核，或者由其他进程经由内核往目标进程发送信号，实际上是在该进程对应的表中写入信号。当进程执行完系统调用退出内核的时候，就会查看这个信号，然后根据信号的不同执行不同的操作。</p>
<p>具体什么整数表示什么意思可以通过 <code>man 7 signal</code> 来查看，常见的有：</p>
<ul>
<li><code>SIGINT</code>: 当键盘按下 CTRL+C 从 shell 中发出信号，信号被传递给 shell 中前台运行的进程，对应该信号的默认操作是中断(INTERRUPT)该进程</li>
<li><code>SIGQUIT</code>: 当键盘按下 CTRL+\ 从 shell 中发出信号，信号被传递给 shell 中前台运行的进程，对应该信号的默认操作是退出(QUIT)该进程</li>
<li><code>SIGTSTP</code>: 当键盘按下 CTRL+Z 从 shell 中发出信号，信号被传递给 shell 中前台运行的进程，对应该信号的默认操作是暂停(STOP)该进程</li>
<li><code>SIGCONT</code>: 用于通知暂停的进程继续</li>
<li><code>SIGALRM</code>: 起到定时器的作用，通常是程序在一定的时间之后才生成该信号</li>
</ul>
<p>上面的介绍说『默认』操作，那么也就意味着我们是可以采取其他操作的，比方说直接无视掉，或者执行我们自定义的操作。</p>
<p>除了信号，消息队列(message queue)和共享内存(shared memory)也可以在进程间进行信息共享。不过因为这种机制比较复杂，尤其是涉及到同步的问题，所以在使用的时候需要多加注意。</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>试着自己安装一个 Linux 系统，尝试只使用终端来完成基本的文件夹查看操作</li>
<li>查看系统当前正在运行的进程</li>
<li>试着给某个进程发送一个信号</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这一讲中我们简单介绍了 Linux 系统中几个比较重要的概念，部分内容可能会比较难理解，这时候就要实际在电脑上试一试，配合关键词进行搜索咯。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://www.catb.org/~esr/writings/cathedral-bazaar/introduction/" target="_blank" rel="external">大教堂和市集(The Cathedral and the Bazaar)</a></li>
<li><a href="http://coolshell.cn/articles/2322.html" target="_blank" rel="external">Unix 传奇(上篇)</a></li>
<li><a href="http://coolshell.cn/articles/2324.html" target="_blank" rel="external">Unix 传奇(下篇)</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果说计算机科学的三大浪漫是操作系统、编译原理和计算机图形学的话，谈及操作系统，Linux 就一定是那个程序员对它又爱又恨的存在。本文带大家了解 Linux 中的基本概念和原理，正所谓知其然也要知其所以然。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="Linux" scheme="http://wdxtub.com/tags/Linux/"/>
    
      <category term="概念" scheme="http://wdxtub.com/tags/%E6%A6%82%E5%BF%B5/"/>
    
  </entry>
  
  <entry>
    <title>第二十五周 - 沧海一声笑</title>
    <link href="http://wdxtub.com/2016/12/02/roar-of-the-ocean/"/>
    <id>http://wdxtub.com/2016/12/02/roar-of-the-ocean/</id>
    <published>2016-12-02T13:32:57.000Z</published>
    <updated>2016-12-02T15:14:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>沧海一声笑，滔滔两岸潮，浮沉随浪只记今朝。苍天笑，纷纷世上潮，谁负谁胜出天知晓。</p>
<a id="more"></a>
<hr>
<p>为了给周记编链接地址，不得不求助词典来翻译『沧海一声笑』这个歌名，出来的结果也是颇让我哭笑不得，有 “See the world indifferently” - 冷眼看世界，有 “Laughter in the sea” - 海中的笑声，甚至还有 “The sea on voice laugh” - 绝对是机器翻译，最后只好选了一个稍微靠谱点的 “Roar of the Ocean”。</p>
<p>冬季运动计划进行中，因为天气变暖所以加上了跑步，于是继续浑身酸痛。跑了一段时间上坡，慢慢也已经习惯了，虽说是朝抵抗力最大的路径走，但蹬地的反作用力其实要比平地来得大，跑起来反而有另一种轻松。考虑到春节临近又要预备着每逢佳节胖三公斤，所以还是得先减一点，算是给自己多一些放纵的空间。</p>
<p>赶在十二月之前发布了博客的 Beta 版，基本上把自己之前所有的文章和资料进行了统一的梳理，把地基打牢了才好在这之上盖更宏伟的建筑。正像编辑所说的那样，编写著作要趁早，在这个过程中可以理清思路进而升华。不过在此之前还是要有一定的阅读量的，所以 kindle 这个东西嘛，早买早悟道呀。</p>
<p>自上个周末起沉浸在自己营造的牢笼中体验了好几天的矛盾与冲突，从前刻意去避免的情感波动在短短几天内似乎全都释放了出来。虽然这个释放的过程非常痛苦，但是起起伏伏反而激荡出了变化，也很高兴能在迷茫中找回自己。很多事情虽然没有那么好，但也没有那么糟。负能量这个东西大抵虚幻，不要做任何判断与决定，少些到处宣泄，其实该走的自然就会走。或者说，其实这样的体验才是最好的认识自己的机会。</p>
<p>睡前抽空看了一本书，叫《系统之美》，说的是输入、存量、输出、反馈这样的『系统』思维模式。换不同的角度去看待世界是蛮有意思的事情，如果用系统的思维来复盘自己的一天，那么早起对应于系统的预防机制，晚睡对应于系统的容错机制。这俩机制都很重要，预防的思路是事先把事情做好，尽量少出错；容错的思路是即使错了没关系，能有办法补救。很多时候在资源受限的情况下必须做出选择，不过我总是倾向于预防机制（看我的代码就更能体现出来了），但转过头来想想，容错也非常重要，以后还是要多多综合考虑。当然，一次就把事情做好是最好的。</p>
<p>临近年底，也开始在思考 2017 年的计划了，我本人是更倾向于去做人工智能相关应用的，毕竟做工程的东西虽然可以磨练技艺，但是说白了还就是几门编程语言几个框架罢了，真正能够对社会产生巨大价值的是激发智能本身。当然在这个过程中还要继续为计算机基础教育添砖加瓦，让更多的同学能找到最适合自己的学习道路。</p>
<p>我一直觉得，把自己想做的事情说出来，告诉身边的人，让更多的人知道，除了可以更好的鞭策自己外，还可能吸引到更多志同道合的人，也许别人的小小建议和不同角度的思考，就能发挥巨大的作用。随着项目的逐渐深入，也慢慢能够体会到高效团队的重要性，或者说，和脑子转得快的人共事其实是很幸福的事情，毕竟以肉眼可见的速度在提高完成度，是颇有成就感的。</p>
<p>很多东西，越是在乎，越会放大快乐和痛苦，这真是一个令人烦恼的问题呀，希望能慢慢找到解答。</p>
<p>江山笑，烟雨遥，涛浪淘尽红尘俗世几多娇。苍生笑，不再寂寥，豪情仍在痴痴笑笑。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;沧海一声笑，滔滔两岸潮，浮沉随浪只记今朝。苍天笑，纷纷世上潮，谁负谁胜出天知晓。&lt;/p&gt;
    
    </summary>
    
      <category term="Gossip" scheme="http://wdxtub.com/categories/Gossip/"/>
    
    
      <category term="周记" scheme="http://wdxtub.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>wdxtub.com Beta 发布说明</title>
    <link href="http://wdxtub.com/2016/11/30/wdxtub-beta-release-notes/"/>
    <id>http://wdxtub.com/2016/11/30/wdxtub-beta-release-notes/</id>
    <published>2016-11-29T23:47:31.000Z</published>
    <updated>2016-12-01T13:00:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>经过一年多时间的筹备，终于能在今天发布 wdxtub.com 的首个 Beta 版本，包含三项我非常满意的重大改动！</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.30: wdxtub.com Beta 版正式发布</li>
</ul>
<h2 id="统计数字"><a href="#统计数字" class="headerlink" title="统计数字"></a>统计数字</h2><p>至北京时间 2016 年 11 月 30 日 21 时 15 分，关于我的博客 wdxtub.com 的统计数字大约有这些：</p>
<ul>
<li>博客数据<ul>
<li>书籍数量: 366</li>
<li>日志数量: 746</li>
<li>评论数量: 887</li>
<li>打赏数量: 78</li>
</ul>
</li>
<li>访问数据<ul>
<li>访问量: 241126</li>
<li>访问人次: 92714</li>
<li>访客来自中国 33 个省级行政区域（唯一少了一个省）</li>
<li>7 个访客中只有 1 个是使用移动设备的</li>
<li>2 个访客中就有 1 个是使用 Mac OS 的</li>
<li>访客中 Chrome 浏览器的使用率高达 68%</li>
<li>有 35% 的访客使用 1080p 的显示器</li>
<li>新老访客比例大约为 1 比 1</li>
<li>有将近一半的访客年龄在 18 到 24 岁之间</li>
<li>访问来源前五名为：广东省、美国、北京市、上海市、浙江省</li>
</ul>
</li>
<li>代码数据<ul>
<li>Github Repo Star 数量: 79</li>
<li>提交数量: 151 commits / 6,459,262 ++ / 3,439,925 –</li>
<li>最常提交时间: 周三/周四晚十点</li>
</ul>
</li>
</ul>
<h2 id="发布摘要"><a href="#发布摘要" class="headerlink" title="发布摘要"></a>发布摘要</h2><p>从半年多写书的过程中我真切理解了写作的几个关键技巧，即『分层』、『聚类』和『主题先行』。从这样的产品思路出发，博客被划分为三大板块，分别代表不同的主题。把目光放在导航栏，就可以看到『不周山』、『通天塔』和『好望角』三个全新的板块。</p>
<p><img src="/images/14805022102586.jpg" alt=""></p>
<p>名字的含义及主题分别是：</p>
<ol>
<li><strong>不周山</strong>：偏理论，用具体的例子来深入理解概念。学习知识就像不周山，永远不会有『周全』的一天，是为活到老，学到老。</li>
<li><strong>通天塔</strong>：偏实战，用具体的实践来打造完整产品。工程实践就像通天塔，需要不断添砖加瓦才能越盖越高。</li>
<li><strong>好望角</strong>：生活、思考、兴趣、观察、回忆、创作。好望角是寻找通往『黄金乐土』的海上通道，终年大风大浪，所谓生活，就是要乘风破浪冲向新大陆。</li>
</ol>
<p>另外两个值得提及的重大更新是：</p>
<ol>
<li>博客进入版本化时代，无论文章还是板块更新，均会有更新记录，方便大家查阅最新信息</li>
<li>正式采用 <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="external">署名(BY)-非商业性(NC)-禁止演绎(ND)</a> 协议</li>
</ol>
<p>当然细小的更新还有很多，具体可以参见下面的版本注释。感谢大家一直以来的支持，我会努力越做越好。</p>
<h2 id="浏览指南"><a href="#浏览指南" class="headerlink" title="浏览指南"></a>浏览指南</h2><p>使用页面左上方的导航栏可以在首页以及其他板块之间切换。</p>
<p><img src="/images/14805022102586.jpg" alt=""></p>
<p>进入每个板块后，页面右边会出现目录，可以方便地进行导航：</p>
<p><img src="/images/14805100643249.jpg" alt=""></p>
<p>如果一篇文章属于某个系列，在文章的开头会有该系列的目录，以及更新时间：</p>
<p><img src="/images/14805102469086.jpg" alt=""></p>
<p>在书影音页面能看到改版后的展示效果：</p>
<p><img src="/images/14805104198419.jpg" alt=""></p>
<p>在关于页面中新增博客更新文档，后续的版本更新会在这里展现：</p>
<p><img src="/images/14805109563356.jpg" alt=""></p>
<p>还有很多比较小的改动，就留待大家慢慢挖掘啦！</p>
<h2 id="版本注释"><a href="#版本注释" class="headerlink" title="版本注释"></a>版本注释</h2><h3 id="Beta-2016-11-30"><a href="#Beta-2016-11-30" class="headerlink" title="Beta 2016.11.30"></a>Beta 2016.11.30</h3><ul>
<li>导航栏更新，用『不周山』『通天塔』和『好望角』取代了原来的『作品』『技术』和『生活』板块</li>
<li>导航栏更新，原来的『关于我』改为『关于』，增加了关于博客本身内容的信息</li>
<li>导航栏更新，原来的『书单』改为『书影音』，增加了关于电影、音乐和游戏部分的内容</li>
<li>每篇文章除了创建时间外，增加了『更新时间』，方便大家了解文章更新状况</li>
<li>为了配合全新的板块设计，对大部分文章内容进行了调整，之后所有的文章都会隶属于某一板块的某一系列，更加清晰</li>
<li>书单部分由原来的表格切换成了无序列表，并根据内容进行了更加详细的分类</li>
<li>感谢网友 keli 的建议，导航栏图标进行了更换（虽然没有找到山和塔的图标）</li>
</ul>
<p>Beta2 预计更新内容</p>
<ul>
<li>全新的原创网站图标</li>
<li>新系列逐步上线</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经过一年多时间的筹备，终于能在今天发布 wdxtub.com 的首个 Beta 版本，包含三项我非常满意的重大改动！&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="wdxtub" scheme="http://wdxtub.com/tags/wdxtub/"/>
    
      <category term="beta" scheme="http://wdxtub.com/tags/beta/"/>
    
      <category term="release" scheme="http://wdxtub.com/tags/release/"/>
    
  </entry>
  
  <entry>
    <title>【不周山之数据挖掘】壹 概率与统计基础知识</title>
    <link href="http://wdxtub.com/2016/11/27/bzs-dm-basis/"/>
    <id>http://wdxtub.com/2016/11/27/bzs-dm-basis/</id>
    <published>2016-11-26T23:42:13.000Z</published>
    <updated>2016-11-27T13:31:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎走进数据挖掘的世界！『不周山之数据挖掘』系列会结合原理与实践，在弄懂数据挖掘理论的前提下，用实例和分析应用数据挖掘。这一讲是系列正文的开端，主要介绍开始学习数据挖掘的预备知识和相关学习资料。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.27: 完成初稿</li>
</ul>
<h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><p>数据挖掘入门指南，理论为主，配合<a href="http://wdxtub.com/2016/09/11/work-page/#通天塔之-W-I-S-E">『通天塔之 W.I.S.E』</a>有更好的理解</p>
<ul>
<li><a href="http://wdxtub.com/2016/11/27/bzs-dm-basis/">壹 概率与统计基础知识</a></li>
<li><a href="http://wdxtub.com/2016/11/27/bzs-dm-internet/">贰 互联网数据挖掘导论</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>复习概率和统计的基本知识</li>
<li>熟悉并掌握各种分布</li>
<li>意识到统计分析的数字是具有欺骗性</li>
<li>阅读推荐书籍</li>
</ol>
<h2 id="两类专家"><a href="#两类专家" class="headerlink" title="两类专家"></a>两类专家</h2><p>开始复习之前，我们先来看看两类专家的对比（出自《信号与噪声》）：</p>
<table>
<thead>
<tr>
<th style="text-align:center">狐狸型专家的想法</th>
<th style="text-align:center">刺猬型专家的想法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">千伎百俩：汇聚不同学科的思想，忽略最初的政治派别</td>
<td style="text-align:center">一技之长：把大部分精力投入到一两个重大问题上，以怀疑的眼光看待『局外人』的观点</td>
</tr>
<tr>
<td style="text-align:center">适应力强：最初的方法失效后，试图找到新的方法，或同时寻求多种方法</td>
<td style="text-align:center">坚持力强：坚持『总揽一切』的方法，新数据只能用来改善原始模式</td>
</tr>
<tr>
<td style="text-align:center">严于律己：有时会愿意（或是欣于）承认预测中的错误并接受谴责</td>
<td style="text-align:center">固执己见：错误归咎到坏运气或特殊情况上——好模式没有赶上好时机</td>
</tr>
<tr>
<td style="text-align:center">承认复杂性：承认宇宙的复杂性，认为许多基本问题不可解决或本身就是不可预测的</td>
<td style="text-align:center">寻找秩序：一旦从噪声中找到信号，便期望世界遵循某种相对简单的支配关系</td>
</tr>
<tr>
<td style="text-align:center">谨慎：用概率术语表达预测结论，并且证明自己的观点是正确的</td>
<td style="text-align:center">自信：很少对自己的预测进行正面回复，并且不愿改变自己的预测</td>
</tr>
<tr>
<td style="text-align:center">经验主义：更多地依赖观察而非理论</td>
<td style="text-align:center">意识形态：期待日常的问题正是宏伟理论或斗争的体现</td>
</tr>
<tr>
<td style="text-align:center">较好的预测家</td>
<td style="text-align:center">较差的预测家</td>
</tr>
</tbody>
</table>
<p>我们学习数据挖掘，自然是希望自己能够成为『好』的预测家！而数据挖掘中很重要的一个意识就是：统计分析中陷阱重重。</p>
<h2 id="统计分析中的陷阱"><a href="#统计分析中的陷阱" class="headerlink" title="统计分析中的陷阱"></a>统计分析中的陷阱</h2><p>概率和统计是一种观察世界的角度，统计思维从某种程度上来说和读写能力一样重要。回顾自己常犯的错误，很多时候不是因为那些我不知道的事情，而是我知道的是错误答案的事情，这种错误的『正确感』最终导致了错误发生。而概率和统计其实是利用数据对我们已有的偏见进行纠正，这个过程中会有很多看似『反常』的结论出现。至于是相信感觉还是相信数字，啊这真是一个令人头疼的问题。下面是摘录自《统计陷阱》中的只言片语，大家可以先感受一下：</p>
<ul>
<li>有三种谎言：谎言，糟糕透顶的谎言和统计资料</li>
<li>整数总是不完善的</li>
<li>单凭某一数据很难反应实情</li>
<li>一条河永远不可能高于它的源头，同理，对样本研究后得到的结论不会好于样本本身</li>
<li>一个以抽样为基础的报告如果要有价值，就必须使用具有代表性的样本，这种样本排除了各种误差</li>
<li>无形的误差与有形的误差一样容易破坏样本的可信度</li>
<li>普查工作者一般都具有足够的统计知识、技术以及调查费用以确保抽样的精确度。他们并非居心叵测之徒。但并不是所有能见到的数据都产生于这样良好的环境，也并不是所有的数据都会有附有类似的精确度说明</li>
<li>如果某条信息提供了显著性程度，你将对它有更深的了解。显著程度通常用概率表示</li>
<li>将『正常的』与『期望的』混为一谈导致事情变得更糟</li>
<li>这些没有透露的数据其欺骗性在于人们经常忽略了它们的不存在，这当然也是使用这些数据的人获取成功的奥秘</li>
<li>当一个平均数、一张图表或者某种趋势遗漏了这些重要的数据，请对它们保留一些怀疑</li>
<li>你的样本以多大的精度代表总体是可以用数据来衡量的，那就是：可能误差和标准误差</li>
<li>只有当差别有意义时才能称之为差别</li>
<li>注意比例尺和起始标尺，这可能会产生极大的误导性</li>
<li>利用一维图形的信息不对称，可以营造出非常夸张的视觉效果</li>
<li>如果你想证明某事，却发现没有能力办到，那么试着解释其他事情并假装它们是同一回事</li>
<li>相关并不等于因果，一定要注意这里的区别</li>
<li>扭曲统计数据的最巧妙方法是利用地图</li>
<li>百分数也给误解提供了肥沃的土壤。和小数一样，它也能为不确切的食物蒙上精确的面纱</li>
<li>将一些看似能直接相加却不能这样操作的事情加在一起会产生大量的欺骗和隐瞒</li>
<li>对统计资料提出的五个问题<ol>
<li>谁说的？有意识的偏差和无意识的偏差</li>
<li>他是如何知道的？注意样本的有偏，数值是否足够大</li>
<li>遗漏了什么？</li>
<li>是否有人偷换了概念？</li>
<li>这个资料有意义吗？</li>
</ol>
</li>
<li>我们以为自己可以控制很多风险，但结果并非如此，也许这才是更大的威胁</li>
<li>贪婪和恐惧是两个非常不稳定的因素，只有两者保持平衡，经济才能顺利发展。若贪婪在经济体系中占上风，就会产生经济泡沫；若恐惧因素压过贪婪，经济又会陷入恐慌</li>
<li>狐狸型预测方法<ul>
<li>用概率的方法思考问题</li>
<li>今天的预测是你以后人生的第一个预测</li>
<li>寻求共识 </li>
</ul>
</li>
<li>信息是决定预测成败的关键，并不是信息越多，预测就越成功</li>
<li>经济是一个动态系统，不是一个方程式</li>
<li>运气和技能通常被视为两个极端，但两者之间的关系其实更复杂一些</li>
<li>若想做出更准确的预测，就必须承认我们的判断是不可靠的</li>
</ul>
<p>是不是有点感觉了？那么现在复习课开始！</p>
<h2 id="概率与统计基础知识"><a href="#概率与统计基础知识" class="headerlink" title="概率与统计基础知识"></a>概率与统计基础知识</h2><p>概率论主要研究随机事件，既然是研究，我们就不能用『可能/很可能/不太可能』这样的字眼了，要学会如何去量化这种可能性。</p>
<p>统计学主要是根据样本去推测总体情况，大部分的统计分集都是基于概率的，所以概率与统计经常会被放在一起。</p>
<p>那么概率与统计要如何复习呢？最简单粗暴的方法就是把一份我见过最完整的 Cheat Sheet 过一次（在<a href="https://datastories.quora.com/The-Only-Probability-Cheatsheet-Youll-Ever-Need" target="_blank" rel="external">这里</a>），当然，这一讲也不会这么水，还是会把重要概念串讲一次的（部分内容来自《统计思维：程序员数学之概率统计》）。</p>
<p>假设我们要统计之前美国大选各个州各个城市的投票倾向，使用专业的统计学手段应该有如下几个步骤，才更可能绕过各种陷阱，得到更可能正确的结论（注意这里我一直在用可能，因为统计分析中，很少有东西是确定的）：</p>
<ol>
<li>收集数据 - 使用可靠来源的数据</li>
<li>描述性统计 - 计算能总结数据的统计量，并评测各种数据可视化的方法</li>
<li>探索性数据分析 - 寻找模式、差异和其他能解答我们问题的特征。同时，我们会检查不一致性，并确认其局限性</li>
<li>假设检验 - 在发现明显的影响时，我们需要评判这种影响是否真实，也就是说是否是因为随机因素造成的</li>
<li>估计 - 我们会用样本数据推断全部人口的特征</li>
</ol>
<h3 id="描述性统计量"><a href="#描述性统计量" class="headerlink" title="描述性统计量"></a>描述性统计量</h3><p>当我们拿到一组数据，在别人问起『这组数据怎么样』这样的问题的时候，我们肯定不能把这组数据一个一个读出来，而是需要用一些数值来简单描述这组数据的特征，是为『描述性统计量』。</p>
<p>那么描述性统计量都有啥呢？常用的有：均值、平均值、方差、分布、直方图、概率质量函数、条件概率…之所以有这么多各种各样的描述性统计量，就是因为数据本身可以描述的维度太多了：</p>
<ul>
<li>平均值没办法了解数据之间的差异，于是有了方差</li>
<li>方差只能描述数据之间的差异，但无法描述数据的分布，于是有了分布和直方图</li>
<li>直方图没有办法套入数学公式体系进行快速推导，于是有了概率质量函数</li>
<li>概率质量函数没办法处理附加条件的情况，于是有了条件概率</li>
</ul>
<p>这么一说，是不是各种统计量就有了意义了？</p>
<h3 id="连续分布"><a href="#连续分布" class="headerlink" title="连续分布"></a>连续分布</h3><p>比较常用的连续分布是指数分布、帕累托分布、正态分布和对数正态分布，之所以称为『连续』，是因为它们的  CDF 是一个连续函数，而很多实际现象都近似于连续分布。</p>
<blockquote>
<p>跟所有模型一样，连续分布也是一种抽象。换言之，就是会舍弃一些无关紧要的细节。例如，真实观察到的分布中可能会有测量误差或是对样本来说很奇怪的数据，而连续模型会消除这些无关紧要的细节。 连续模型也是一种数据压缩。如果模型能很好地拟合数据集，那么少量参数就可以描述大量数据。 有时候，我们会惊讶地发现某种自然现象服从某个连续分布，观察这些现象可以让我们深入理解真实的系统。</p>
</blockquote>
<ul>
<li>指数分布：事件在每个时间点发生的概率相同，间隔时间的分布就是指数分布</li>
<li>帕累托分布：最初用来描述财富分布状况，后来广泛用于描述自然界和社会科学中的各种现象，包括城镇大小、砂粒和陨石、森林火灾和地震等</li>
<li>正态分布：也称为高斯分布，因其可以近似描述很多现象而成为最常用的分布</li>
</ul>
<h3 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h3><p>概率可以认为是一个零到一之间的值，用于定量描述一件事情发生的可能性大小。如果 E 表示一个事件，那么 P(E) 就表示该事件发生的概率。检测 E 发生情况的过程就叫做试验(trial)。</p>
<p>虽然说关于概率本身的哲学意义还有一些争议，不过我们不妨暂时不去想它，来看看可能是历史上最富争议的概率问题 —— 蒙提霍尔问题(The Monty Hall problem)。</p>
<blockquote>
<p>蒙提·霍尔原本是美国电视游戏节目 Let’s Make a Deal 的主持人，蒙提霍尔问题就是源自该节目中的一个游戏。如果你是参赛者，以下是节目现场的情况。 你会看到三扇关闭的门，蒙提会告诉你每扇门后的奖励：其中有一扇门后面是一辆车，而另外两扇门后面则是诸如花生酱或假指甲之类不太值钱的东西。奖品的摆放是随机的。 你的目标就是要猜出哪扇门后是汽车。如果猜对，汽车就归你了。 我们把你选择的门称为 A 门，其他两扇门分别是 B 门和 C 门。 在打开你所选择的 A 门之前，蒙提往往会打开 B 门或 C 门扰乱你的选择。（如果汽车确实是在 A 门后面，那蒙提随机打开 B 门或 C 门都没有问题。） 接下来，蒙提会给你一个机会：你是坚持原来的选择，还是选择另一扇未打开的门。 问题是，坚持原来的选择或选择另一扇门，会有什么不同吗？</p>
</blockquote>
<p>这个看起来很简单的问题因为与人们的直觉完全不同而变得复杂了起来，一个靠谱的解释是：</p>
<blockquote>
<p>大部分人凭直觉觉得这没有区别。因为，还剩下两扇门，所以汽车在A门后面的概率是50%。但这就错了。实际上，坚持选择A门，获胜的机会就只有 1/3；而如果选择另一扇门，获胜的机会就是 2/3。<br>其中的关键在于要明白，这里有三种可能的情况：汽车可能会在 A 门后，也可能在 B 门或 C 门后面。因为奖品是随机摆放的，所以每种情况的概率都是 1/3。 如果坚持选择 A 门，那么就只有在一开始汽车就在 A 门后面的情况下才能获胜，获胜的概率是 1/3。 但如果选择另一扇没打开的门，那么在 B 或 C 后面有车这两种情况下都会获胜，总体的获胜概率就是 2/3。</p>
</blockquote>
<p>最后提一下聚类错觉(clustering illusion)，指看上去好像有某种特点的聚类实际上是随机的。如何去验证呢？可以看看在随机情况下是否会产生类似聚类的概率，如果是的话，则认为聚类结果无意义，这个过程叫做蒙特卡罗模拟(Monte Carlo simulation)。</p>
<h3 id="进阶知识"><a href="#进阶知识" class="headerlink" title="进阶知识"></a>进阶知识</h3><p>接下来的部分涉及到更加多的数学知识，因为篇幅所限，这里不再一一介绍，但是感兴趣的同学可以根据这些关键词按图索骥，一定会有不一样的收获。</p>
<ul>
<li>分布的运算：偏度、随机变量、概率密度函数、卷积、正态分布性质、中心极限定理</li>
<li>假设检验：交叉验证、卡方验证</li>
<li>估计：方差估计、误差、置信区间、贝叶斯估计</li>
<li>相关性：协方差、最小二乘拟合、因果关系</li>
</ul>
<h2 id="术语表"><a href="#术语表" class="headerlink" title="术语表"></a>术语表</h2><p>术语表是很好的自查列表（内容来自《统计思维：程序员数学之概率统计》），如果有不明白的，可以去维基百科看看，或者一劳永逸找一本教材过一遍即可：</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li>经验之谈(anecdotal evidence): 个人随意收集的证据，而不是通过精心设计并经过研究得到的</li>
<li>直观效应(apparent effect): 表示发生了某种有意思的事情的度量或汇总统计量</li>
<li>人为(artifact): 由于偏差、测量错误或其他错误导致的直观效应</li>
<li>队列(cohort): 一组被调查者</li>
<li>横断面研究(cross-sectional study): 收集群体在特定时间点的数据的研究</li>
<li>字段(field): 数据库中组成记录的变量名称</li>
<li>纵贯研究(longitudinal study): 跟踪群体，随着时间推移对同一组人反复采集数据的研究</li>
<li>过采样(oversampling): 为了避免样本量过少，而增加某个子群体代表的数量</li>
<li>总体(population): 要研究的一组事物，通常是一群人，但这个术语也可用于动物、蔬菜和矿产</li>
<li>原始数据(raw data): 未经或只经过很少的检查、计算或解读而采集和重编码的值</li>
<li>重编码(recode): 通过对原始数据进行计算或是其他逻辑处理得到的值</li>
<li>记录(record): 数据库中关于一个人或其他对象的信息的集合</li>
<li>代表性(representative): 如果人群中的每个成员都有同等的机会进入样本，那么这个样本就具有代表性</li>
<li>被调查者(respondent): 参与调查的人</li>
<li>样本(sample): 总体的一个子集，用于收集数据。 </li>
<li>统计显著(statistically significant): 若一个直观效应不太可能是由随机因素引起的，就是统计显著的</li>
<li>汇总统计量(summary statistic): 通过计算将一个数据集归结到一个数字（或者是少量的几个数字），而这个数字能表示数据的某些特点</li>
<li>表(table): 数据库中若干记录的集合</li>
</ul>
<h3 id="描述性统计量-1"><a href="#描述性统计量-1" class="headerlink" title="描述性统计量"></a>描述性统计量</h3><ul>
<li>区间(bin): 将相近数值进行分组的范围</li>
<li>集中趋势(central tendency): 样本或总体的一种特征，直观来说就是最能代表平均水平的值</li>
<li>临床上有重要意义(clinically significant): 分组间差异等跟实践操作有关的结果</li>
<li>条件概率(conditional probability): 某些条件成立的情况下计算出的概率</li>
<li>分布(distribution): 对样本中的各个值及其频数或概率的总结</li>
<li>频数(frequency): 样本中某个值的出现次数</li>
<li>直方图(histogram): 从值到频数的映射，或者表示这种映射关系的图形</li>
<li>众数(mode): 样本中频数最高的值</li>
<li>归一化(normalization): 将频数除以样本大小得到概率的过程</li>
<li>异常值(outlier): 远离集中趋势的值</li>
<li>概率(probability): 频数除以样本大小即得到概率</li>
<li>概率质量函数(Probability Mass Function，PMF): 以函数的形式表示分布，该函数将值映射到概率</li>
<li>相对风险(relative risk): 两个概率的比值，通常用于衡量两个分布的差异</li>
<li>分散(spread): 样本或总体的特征，直观来说就是数据的变动有多大</li>
<li>标准差(standard deviation): 方差的平方根，也是分散的一种度量</li>
<li>修剪(trim): 删除数据集中的异常值</li>
<li>方差(variance): 用于量化分散程度的汇总统计量</li>
</ul>
<h3 id="累积分布函数"><a href="#累积分布函数" class="headerlink" title="累积分布函数"></a>累积分布函数</h3><ul>
<li>条件分布(conditional distribution): 在满足一定前提条件下计算出的分布</li>
<li>累积分布函数(Cumulative Distribution Function，CDF): 将值映射到其百分等级的函数</li>
<li>四分差(interquartile range): 表示总体分散情况的值，等于75和25百分等级之间的差</li>
<li>百分位(percentile): 与百分等级相关联的数值</li>
<li>百分等级(percentile rank): 分布中小于或等于给定值的值在全部值中所占的百分比</li>
<li>放回(replacement): 在抽样过程中，“有放回”表示对于每次抽样，总体都是不变的。“无放回”表示每个元素只能选择一次</li>
<li>再抽样(resampling): 根据由样本计算得到的分布重新生成新的随机样本的过程</li>
</ul>
<h3 id="连续分布-1"><a href="#连续分布-1" class="headerlink" title="连续分布"></a>连续分布</h3><ul>
<li>连续分布(continuous distribution): 由连续函数描述的分布</li>
<li>语料库(corpus): 特定语言中用做样本的正文文本</li>
<li>经验分布(empirical distribution): 样本中值的分布</li>
<li>误差函数(error function): 一种特殊的数学函数，因源自误差度量研究而得名</li>
<li>一次频词(hapaxlegomenon): 表示语料库中只出现一次的词。这个单词在本书中迄今出现了两次</li>
<li>间隔时间(interarrival time): 两个事件的时间间隔</li>
<li>模型(model): 一种有效的简化。对于很多复杂的经验分布，连续分布是不错的模型</li>
<li>正态概率图(normal probability plot): 一种统计图形，用于表示样本中排序后的值与其服从正态分布时的期望值之间的关系</li>
<li>秩变换(rankit): 元素的期望值，该元素位于服从正态分布的已排序列表中。</li>
</ul>
<h3 id="概率-1"><a href="#概率-1" class="headerlink" title="概率"></a>概率</h3><ul>
<li>贝叶斯认识论(Bayesianism): 一种对概率更泛化的解释，用概率表示可信的程度</li>
<li>变异系数(coefficient of variation): 度量数据分散程度的统计量，按集中趋势归一化，用于比较不同均值的分布</li>
<li>事件(event): 按一定概率发生的事情</li>
<li>失败(failure): 事件没有发生的试验</li>
<li>频率论(frequentism): 对概率的一种严格解读，认为概率只能用于一系列完全相同的试验</li>
<li>独立(independent): 若两个事件之间相互没有影响，就称这两个事件是独立的</li>
<li>证据的似然值(likelihood of the evidence): 贝叶斯定理中的一个概念，表示假设成立的情况下看到该证据的概率</li>
<li>蒙特卡罗模拟(Monte Carlo simulation): 通过模拟随机过程计算概率的方法</li>
<li>归一化常量(normalizing constant): 贝叶斯定理中的分母，用于将计算结果归一化为概率</li>
<li>后验(posterior): 贝叶斯更新后计算出的概率</li>
<li>先验(prior): 贝叶斯更新前计算出的概率</li>
<li>成功(success): 事件发生了的试验</li>
<li>试验(trial): 对一系列事件是否可能发生的尝试</li>
<li>更新(update): 用数据修改概率的过程。</li>
</ul>
<h3 id="分布的运算"><a href="#分布的运算" class="headerlink" title="分布的运算"></a>分布的运算</h3><ul>
<li>中心极限定理(Central Limit Theorem): 早期的统计学家弗朗西斯·高尔顿爵士认为中心极限定理是“The supreme law of Unreason”</li>
<li>卷积(convolution): 一种运算，用于计算两个随机变量的和的分布</li>
<li>虚幻的优越性(illusory superiority): 心理学概念，是指人们普遍存在的将自己高估的一种心理</li>
<li>概率密度函数(probability density function): 连续型累积分布函数的导数</li>
<li>随机变量(random variable): 一个能代表一种随机过程的客体</li>
<li>随机数(random variate): 随机变量的实现</li>
<li>鲁棒性(robust): 如果一个统计量不容易受到异常值的影响，我们说它是鲁棒的</li>
<li>偏度(skewness): 分布函数的一种特征，它度量的是分布函数的不对称程度</li>
</ul>
<h3 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h3><ul>
<li>单元格(cell): 在卡方检验中，将观测按一定的标准分到各个单元格里，每个单元格代表一种分类</li>
<li>卡方检验(chi-square test): 用卡方统计量做统计量的统计检验</li>
<li>交叉验证(cross-validation): 交叉验证使用一个数据集进行探索性数据分析，然后用另一个数据集进行测试</li>
<li>假阴性(false negative): 在效应真实存在的情况下，我们认为这个效应是由偶然因素引起的</li>
<li>假阳性(false positive): 在原假设为真的情况下，我们拒绝了原假设的结论</li>
<li>假设检验(hypothesis testing): 判定出现的效应是否具有统计显著性的过程</li>
<li>似然比(likelihood ratio): 一种概率的比值， P(E|A)/P(E|B)，这里A和B是两种假设。似然比不依赖于先验概率，可以用来报道贝叶斯统计推断的结果</li>
<li>原假设(null hypothesis): 一种基于以下假设的模型系统：我们观测到的效应只是由偶然因素引起的</li>
<li>单边检验(one-sided test): 一种检验类型，关注的是出现比观测到的效应更大（或小）的效应的概率</li>
<li>p值(p-value): 在原假设成立的情况下，出现我们观测到的效应的概率</li>
<li>功效(power): 在原假设为假的情况下，检验推翻原假设的概率</li>
<li>显著性(significant): 我们说某个效应具有统计显著性指的是这种情况不大可能是由偶然因素引起的</li>
<li>检验统计量(test statistic): 衡量观测到的效应与原假设下期望的结果之间偏差的统计量</li>
<li>测试集(testing set): 用做测试的数据集</li>
<li>训练集(training set): 用做训练的数据集</li>
<li>双边检验(two-sided test): 一种检验类型，关注的是出现比观测到的效应更大的效应的概率，不考虑正负</li>
</ul>
<h3 id="估计"><a href="#估计" class="headerlink" title="估计"></a>估计</h3><ul>
<li>有偏性(bias): 在平均多次试验的结果后，一个估计量倾向于高估或者低估真实的参数值</li>
<li>删失数据(censored data): 一种数据集，数据来源于某种采集方式，但是这种采集方式会系统性地排除某些数据</li>
<li>置信区间(confidence interval): 一种参数的区间估计，以一定的概率包含待估计的参数</li>
<li>可信区间(credible interval): 贝叶斯统计理论中的置信区间</li>
<li>估计(estimation): 用样本信息估计分布中未知参数的过程</li>
<li>估计量(estimator): 用于估计参数的统计量</li>
<li>极大似然估计量(maximum likelihood estimator): 使得似然函数最大化的估计</li>
<li>均方误差(mean squared error): 一种衡量估计误差的值</li>
<li>点估计(point estimate): 用单一的值估计某个参数</li>
</ul>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><ul>
<li>确定系数(coefficient of determination): 衡量模型拟合结果好坏的指标</li>
<li>对照组(control group): 对照试验中没有接受处理的组，或受到已知效应处理的组</li>
<li>相关性(correlation): 对两个变量关系的一种描述</li>
<li>协方差(covariance): 衡量两个变量变化方向是否一致的统计量</li>
<li>因变量(dependent variable): 我们想要解释或者预测的变量</li>
<li>自变量(independent variable): 用于预测因变量的变量，也称解释变量</li>
<li>最小二乘拟合(least squares fit): 最小化残差平方和的数据拟合方法</li>
<li>自然试验(natural experiment): 一种试验设计的方法，就是利用自然形成的界限将受试者分成几个分组，并且大体上使得分组结果接近随机分组</li>
<li>归一化(normalize): 将一组数据进行转换，使其均值为 0，方差为 1</li>
<li>随机对照试验(randomized controlled trial): 一种试验设计的方法，将受试者随机分成几个分组，并对不同的分组实施不同的处理</li>
<li>秩(rank): 将一个序列按大小排序后，序列中的某个元素所处的位置</li>
<li>残差(residual): 衡量模型预测结果与真实值离差的值</li>
<li>标准分数(standard score): 归一化后的值</li>
<li>处理(treatment): 对照试验中对一个分组所做的干预或改变</li>
</ul>
<h2 id="推荐书籍"><a href="#推荐书籍" class="headerlink" title="推荐书籍"></a>推荐书籍</h2><ul>
<li>Statistics for business and economics</li>
<li>Data Mining: concepts and technologies</li>
<li>数据挖掘导论</li>
<li>Python for Data Analysis</li>
<li>Web Analysis</li>
<li>深入浅出数据分析</li>
<li>增长黑客</li>
<li>网站分析实战</li>
<li>精益数据分析</li>
<li>统计陷阱</li>
<li>统计思维：程序员数学之概率统计</li>
</ul>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>找个朋友问一下蒙提霍尔问题，看看最后你能解释清楚没有</li>
<li>试着用概率统计的思维去观察一下，有没有什么新发现？</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这一讲我们简单了解了数据挖掘的基础知识，下一讲会简要介绍数据挖掘的基本流程和各个组件的功能。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;欢迎走进数据挖掘的世界！『不周山之数据挖掘』系列会结合原理与实践，在弄懂数据挖掘理论的前提下，用实例和分析应用数据挖掘。这一讲是系列正文的开端，主要介绍开始学习数据挖掘的预备知识和相关学习资料。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="数据挖掘" scheme="http://wdxtub.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>【不周山之数据挖掘】贰 互联网数据挖掘导论</title>
    <link href="http://wdxtub.com/2016/11/27/bzs-dm-internet/"/>
    <id>http://wdxtub.com/2016/11/27/bzs-dm-internet/</id>
    <published>2016-11-26T23:42:13.000Z</published>
    <updated>2016-11-27T07:08:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>上一讲中我们简单复习的概率和统计相关知识，现在我们就可以正式开始接触数据挖掘了。本文是接下来内容的大纲，让大家对互联网搜索与挖掘有一个宏观的了解，即知道要做什么和怎么做。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.27: 完成初稿</li>
</ul>
<p>注：本文的框架来源于北京大学<a href="http://www.icst.pku.edu.cn/lcwm/wanxj/" target="_blank" rel="external">万小军</a>开设的<a href="http://www.icst.pku.edu.cn/lcwm/course/WebDataMining2016/?page_id=560" target="_blank" rel="external">互联网数据挖掘 Web Data Mining</a> 课程，我对内容进行了筛选和编排，用来作为『不周山之数据挖掘』系列的导论部分。</p>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><p>数据挖掘入门指南，理论为主，配合<a href="http://wdxtub.com/2016/09/11/work-page/#通天塔之-W-I-S-E">『通天塔之 W.I.S.E』</a>有更好的理解</p>
<ul>
<li><a href="http://wdxtub.com/2016/11/27/bzs-dm-basis/">壹 概率与统计基础知识</a></li>
<li><a href="http://wdxtub.com/2016/11/27/bzs-dm-internet/">贰 互联网数据挖掘导论</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>了解搜索和自然语言处理的基本知识</li>
<li>熟悉数据挖掘的流程与各个步骤所用的技术</li>
<li>对数据挖掘的应用场景有基本的认识</li>
</ol>
<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>随着互联网的日益蓬勃发展，如何从广袤的信息海洋中提取出有价值的信息、模式和关系，逐渐成为了一门新的领域 —— 数据挖掘。作为一门交叉学科，数据挖掘融合了信息检索、互联网、数据库、机器学习、自然语言处理等不同的学科，用多样技术完成具体的数据挖掘应用。常见的应用有：垂直搜索、推荐系统、智能问答、机器翻译、舆情监测、情报收集等等，可谓是深入到了我们日常生活的方方面面</p>
<p>接下来我们会从基础技术说起，从以下三个方面来了解数据挖掘：</p>
<ul>
<li>搜索技术</li>
<li>数据挖掘技术</li>
<li>具体应用</li>
</ul>
<h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>搜索其实是一个很大的主题，但是核心问题其实并不复杂，一是如何去表示文档，二是在这样的基础上如何去检索文档。具体的评价标准是『效果』和『效率』。效果指的是如何准确匹配查询与文档，一般来说会基于检索模型进行。效率值得是如何快速返回检索结果，一般来说是基于索引进行的。</p>
<h3 id="文档表示"><a href="#文档表示" class="headerlink" title="文档表示"></a>文档表示</h3><p>文档表示一般有两种方法：手动或自动。</p>
<p>手动方法主要依靠人工标注，结果比较可靠，而且标注的词汇是预先设定好的关键词，检索起来效率比较高。但是人工标注无论是时间成本还是人力成本都较高，一般来说难以大批量使用。这类人工标注的信息一般称为文档的元描述(Meta-descriptions)，除了包含域信息(author, title, date)外，还回包含关键词和分类。</p>
<p>自动方法最有代表性的是词袋(Bag of Words)技术，即使用文档中出现的词的集合来表示一篇文档。但是这种方法也有很多不足之处，因为是词语的无序集合，句法信息首先已经丢失了，另外针对不同的语言会有不同的难点。</p>
<p>对于中文来说，如何进行分词（即把句子分成词）就是一个很大的难点，尤其是层出不穷的网络热梗，如何保证准确和实时就是非常大的挑战。对于英文来说，虽然没有分词的问题，但是大小写、单复数、时态、词根等等同样让人头疼。这也导致了大部分搜索引擎都不会考虑词根问题，一是因为文档太多，进行二次处理得不偿失，二是因为对于搜索结果来说影响没有那么大，自然就没有太大的动力去做。</p>
<p>但是无论是中文还是英文，有一个操作是一定会做的，就是去掉停用词(Stop Words)，也就是去掉那些不具有内容信息的词，比如对于中文来说『的地得』，对于英文来说的『a, an, the』。但需要注意的是这样一个简单的操作虽然可以大幅减少索引的大小，缩短检索时间，但实际上不能提高检索效果，具体挺用词表的确定也需要根据不同的文档集合和应用具体对待，没有一个一概而论的方案。</p>
<h3 id="文档索引"><a href="#文档索引" class="headerlink" title="文档索引"></a>文档索引</h3><p>表示了文档之后，我们需要对其进行索引，不然每次检索如果需要用户等太久，体验就很糟糕了。而具体到用什么进行检索，最终人们选择了用词而不是短语来作为索引，这里一个比较有代表性的工具就是 Lucene，现在互联网上广为应用的 Elasticsearch 和 Solr 都是基于 Lucene 的。</p>
<p>Lucene 最重要的技术就是倒排索引(inverted index)，可看做链表数组，每个链表的表头包含关键词，其后序单元则包括所有包括这个关键词的文档标号，以及一些其他信息，如该词的频率和位置等。这里关键词查询一般采用 B-Tree 或哈希表，文档列表组织一般采用二叉搜索树。</p>
<h3 id="文档检索"><a href="#文档检索" class="headerlink" title="文档检索"></a>文档检索</h3><p>文档检索的思路也很简单：如果一篇文档与一个查询相似，那么该文档与查询相关。相似性一般根据字符串匹配来判定，比方说相似的词汇或相同的语义。</p>
<p>最初人们常用的是基于布尔代数的匹配，虽然比较简单，但是对查询的要求很高；并且匹配标准过于严格，容易导致过少或过多的检索结果。尽管布尔模型不再用作主流文档检索模型，但其思想常用于实现高级(综合)检索功能。</p>
<p>现在最常用的是向量空间模型(Vector Space Model)，其思路是文档与查询都是高维空间中的一个向量，用户自由输入文本也是一个向量，利用向量空间的相似性进行查询。具体的相似性同样可以用两种方法来确定：内积或者夹角。因为是空间，所以度量距离的时候会采用不同的描述距离的方式，有 Minkowski metric, Euclidian distance, Jacquard measure 和 Dice’s coefficient 等等。</p>
<p>同一篇文档中不同词语其实也会有不同的权重，这里我们比较常用的是 TF-IDF 算法，其中 TF 表示词语出现的频率，而 IDF 则能区别不同词语的重要性。</p>
<h3 id="文档收集"><a href="#文档收集" class="headerlink" title="文档收集"></a>文档收集</h3><p>前面介绍了文档检索的各种概念，但是现在问题来了，文档从哪里来呢？这就要提到我们最常听见的爬虫(Web Crawler)了，它能够快速有效地收集尽可能多的有用 Web 页面，包括页面之间的链接结构。</p>
<p>随着 Web 2.0 的兴起，脚本语言生成的动态内容和各类多媒体内容给爬虫增加了许多难度，但基本的页面爬取策略没有太大的改变，一般以以广度优先为主，深度优先为辅，需要具体的特性主要有：</p>
<ul>
<li>健壮 Robustness, 避免进入死循环</li>
<li>友好 Politeness, 遵守服务器的采集协议</li>
<li>分布式 Distributed, 多台机器分布式采集</li>
<li>可扩展 Scalable, 爬虫架构方便扩展</li>
<li>性能与效率，有效利用系统资源</li>
<li>质量 Quality, 倾向于采集有用的页面</li>
<li>新颖 Freshness, 获取网页的最新版本</li>
<li>可扩充 Extensible, 能够处理新数据类型、新的采集协议等</li>
</ul>
<h3 id="链接分析"><a href="#链接分析" class="headerlink" title="链接分析"></a>链接分析</h3><p>除了页面的内容本身，超链接其实也能提供非常多有价值的信息。一条从页面 A 指向页面 B 的链接表明 A 与 B 相关且 A 推荐/引用/投票/赞成 B。Google 当年最重要的 PageRank 算法，其实就是这个问题的最初且最成功的解决方案。</p>
<p>这里有一个很有趣的现象叫做排序沉入(Rank Sink)，页面 A 引用了页面 B，页面 B 也引用了页面 A，就形成了一个闭环，不再向外传播分数了。这是我们在实际运用中需要避免的情况。</p>
<h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><p>数据挖掘根据应用的不同，分为不同的子领域，这些子领域又和机器学习、概率统计、模式识别等有着千丝万缕的关系。接下来先介绍基本概念，然后聊聊一些常见的应用。</p>
<h3 id="主要任务"><a href="#主要任务" class="headerlink" title="主要任务"></a>主要任务</h3><p>数据挖掘的任务主要包括两类，一类是基于一些变量预测其他变量的未知值或未来值，称为预测型任务，常用的技术是分类(Classification)，回归(Regression)和偏差分析(Deviation Detection)。另一类是发现描述数据的人们可解释的模式，称为描述型任务，常用的技术是聚类(Clustering)，关联规则挖掘(Association Rule Discovery)和摘要(Summarization)。</p>
<p>为了完成上述任务，整个数据挖掘的流程为：获取数据 -&gt; 选择数据 -&gt; 预处理数据 -&gt; 数据规整 -&gt; 数据挖掘 -&gt; 模式识别。不同阶段会使用不同的技术，但一定要把整个流程走通，数据挖掘才有意义。</p>
<p>随着数据量的增大，如何让数据挖掘更加容易拓展效率更高，如何去挖掘有上下文关系的数据，如何从复杂、异构、网络化数据中挖掘复杂知识，如何挖掘低质量数据，如何保证安全性和隐私，都是未来数据挖掘需要努力的方向。</p>
<h3 id="常用工具"><a href="#常用工具" class="headerlink" title="常用工具"></a>常用工具</h3><p>开源的工具有：</p>
<ul>
<li>Weka</li>
<li>GATE</li>
<li>Carrot2</li>
<li>NLTK</li>
<li>Orange</li>
<li>RapidMiner</li>
<li>KNIME</li>
</ul>
<p>商用的应用主要有：</p>
<ul>
<li>IBM InfoSphere Warehouse</li>
<li>Microsoft Analysis Services</li>
<li>SAS Enterprise Miner</li>
<li>STATISTICA Data Miner</li>
<li>Oracle Data Mining</li>
</ul>
<h3 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h3><p>自然语言处理是人工智能和语言学领域的分支学科指的是利用计算机对人类特有的书面形式和口头形式的自然语言进行各种类型处理和加工的技术。其中最关键的任务有：自动分词、命名实体识别、词性标注、句法分析、语义分析和篇章分析。主要应用在：机器翻译、<strong>文本分类</strong>、情感分析、信息检索与过滤、自动问答、<strong>信息抽取</strong>、<strong>自动文摘</strong>和人机对话等领域。</p>
<p>推荐教材</p>
<ul>
<li>Foundations of Statistical Natrual Language Processing</li>
<li>Speech and Language Processing</li>
<li>统计自然语言处理</li>
</ul>
<p>这里主要以汉语为例子说说分词。一般认为词是最小的、能够独立运用的、有意义的语言单位。但是汉语分词有许多挑战，比如</p>
<ul>
<li>词和词组的边界模糊</li>
<li>新词(未登陆词)</li>
<li>切分歧义<ul>
<li>汉字串 AJB 被称作<strong>交集型切分歧义</strong>，如果满足 AJ, JB 同时为词，此时汉字串 J 被称作交集串</li>
<li>汉字串 AB 被称作<strong>组合型切分歧义</strong>，如果满足条件 A, B, AB 同时为词</li>
<li>真歧义：存在两种或两种以上的真实存在的切分形式</li>
</ul>
</li>
</ul>
<p>具体的分词方法目前主要有以下几种，前两天也有一个利用深度学习的解决方案开源了，可以关注一下</p>
<ul>
<li>简单的模式匹配<ul>
<li>正向最大匹配(FMM)、逆向最大匹配(BMM, 比正向更有效)、双向匹配(BM, 比较两种方法的结果，大颗粒词越多越好，非词典词和单子词越少越好，可以识别出交叉歧义)</li>
</ul>
</li>
<li>基于规则的方法<ul>
<li>最少分词算法</li>
</ul>
</li>
<li>基于统计的方法<ul>
<li>统计语言模型分词、串频统计和词形匹配相结合的汉语自动分词、无词典分词</li>
<li>第一步是候选网格构造：利用词典匹配，列举输入句子所有可能的切分词语，并以词网格形式保存</li>
<li>第二步计算词网格中的每一条路径的权值，权值通过计算图中的每一个节点(每一个词)的一元统计概率和节点之间的二元统计概率的相关信息</li>
<li>最后根据图搜索算法在图中找到一条权值最大的路径，作为最后的分词结果</li>
<li>优缺点：可利用不同的统计语言模型计算最优路径，具有比较高的分词正确率；但算法时间、空间复杂度较高</li>
</ul>
</li>
</ul>
<h2 id="常见应用"><a href="#常见应用" class="headerlink" title="常见应用"></a>常见应用</h2><p>接下来介绍数据挖掘的积累常见应用</p>
<h3 id="智能问答技术"><a href="#智能问答技术" class="headerlink" title="智能问答技术"></a>智能问答技术</h3><p>智能问答技术起源于信息检索社区，简单来说就是根据用户的提问给出简短的答案或提供答案的证据。根据不同的划分标准，我们可以总结出如下的几类问题类型</p>
<ul>
<li>根据答案类型划分<ul>
<li>事实型问题(Factual questions)</li>
<li>观点型问题(Opinions)</li>
<li>摘要型问题(Summaries)</li>
</ul>
</li>
<li>根据问题言语行为(question speech act)划分<ul>
<li>是否型问题(Yes/NO questions)</li>
<li>WH 问题(WH questions)</li>
<li>间接请求(Indirect Requests)</li>
<li>命令(Commands)</li>
</ul>
</li>
<li>复杂/困难问题<ul>
<li>为什么/怎么样(Why, How questions)</li>
<li>什么(What questions)</li>
</ul>
</li>
</ul>
<p>遗憾的是，目前大部分理解问题的技术都是基于正则表达式的，毕竟在自然语言理解这块，暂时还没有突破性进展。</p>
<p>传统自动问答技术主要是基于语料库的自动问答或基于知识库的自动问答，基本包括三个步骤：</p>
<ol>
<li>问题分析(分类、模板匹配、语义分析)</li>
<li>段落检测(段落抽取、排序)</li>
<li>答案抽取(实体识别、模板匹配、排序)</li>
</ol>
<p>社区问答主要是应用与诸如知乎和 Quora 这类网站，目前主要的方向是问题分类、问题推荐、信誉评估和知识抽取等等。</p>
<h3 id="情感分析与观点挖掘"><a href="#情感分析与观点挖掘" class="headerlink" title="情感分析与观点挖掘"></a>情感分析与观点挖掘</h3><p>情感分析与观点挖掘主要应用于产品比较与推荐、个人与机构声誉分析、电视节目满意度分析、互联网舆情分析和反恐与维稳。目前很多互联网平台（如淘宝、大众点评）都已经利用这种技术帮助提取用户评价中的关键词以提供更好的用户体验。</p>
<p>基本的框架如下所示</p>
<ul>
<li>应用层：情感检索，情感摘要，情感问答</li>
<li>核心层：情感要素抽取，情感倾向性分析，主客观分析/观点文本识别</li>
<li>基础层：NLP 基本模块，情感资源收集与标注</li>
<li>来源：产品评论，电影评论，新闻评论，博客，微博</li>
</ul>
<p>而具体应用中，会将文本按照所表达的总体情感进行分类，可能的分类主要有如下三种，一般会从词、句子、文档三中粒度来进行分析：</p>
<ul>
<li>主客观分析/观点文本识别<ul>
<li>客观：反映关于世界的事实信息</li>
<li>主观：反映个人情感、信念等</li>
</ul>
</li>
<li>倾向性分析(可看作主客观分析的细粒度处理)<ul>
<li>对包含观点的文本进行倾向性判断</li>
<li>一般分为三类：褒义、贬义、中性(在一些问题不考虑中性)</li>
</ul>
</li>
<li>情绪分析<ul>
<li>愤怒、高兴、喜好、悲哀、吃惊等等</li>
</ul>
</li>
</ul>
<p>而对于观点挖掘来说，一个观点表示为一个五元组：目标对象, 目标对象特征, 观点的情感值, 观点持有者, 观点表达时间。实际上，观点抽取任务是很困难的，我们<strong>重点关注两个子任务</strong></p>
<ul>
<li>特征抽取与聚类(aspect extraction and grouping)<ul>
<li>抽取对象的所有特征表达，并将同义特征表达聚类。每个特征类表示了关于该对象的独一无二的某个特征</li>
</ul>
</li>
<li>特征情感分类(aspect sentiment classification)<ul>
<li>确定观点针对每个特征的情感倾向：正面、负面、中性</li>
</ul>
</li>
</ul>
<h3 id="信息摘要"><a href="#信息摘要" class="headerlink" title="信息摘要"></a>信息摘要</h3><p>信息摘要指的是对海量数据内容进行<strong>提炼与总结</strong>，以<strong>简洁、直观</strong>的摘要来概括用户所关注的主要内容，方便用户快速了解与浏览海量内容。遗憾的是，研究 50 多年，有一定进展，但仍不能令人满意。一般来说实现思路有两种</p>
<ol>
<li><strong>抽取式</strong>：从文档中抽取已有句子形成摘要。这种方法实现简单，能保证句子的可读性</li>
<li><strong>生成式/混合式</strong>：生成新的句子，或者对已有句子进行压缩、重构与融合。这种方法难度更大，但更接近摘要的本质</li>
</ol>
<p>抽取式文档摘要的典型工作流程是：文档集 -&gt; 文档理解 -&gt; <strong>句子重要性计算与排名(利用词语句子的各类特征，基于机器学习)</strong> -&gt; 句子选择 -&gt; 摘要句子排序 -&gt; 摘要</p>
<p>目前摘要总体性能不高，需要方法上的突破。</p>
<h3 id="社交网络分析"><a href="#社交网络分析" class="headerlink" title="社交网络分析"></a>社交网络分析</h3><p>社交网络作为 Web 2.0 的典型代表，用户生成的内容相当多，可以看作是某种程度上的群体智慧和在强交互性基础上构造的异构网络。</p>
<p>社交网络分析主要是基于社交关系、结构进行挖掘，比如社区检测、连接预测、影响力分析。而社交内容挖掘则是基于文本等内容数据进行挖掘，比如摘要、关键词、情感分析。因为每个人在社交网络上可以抽象为一个元素，于是他们之间的关系可以用矩阵表示。另一种表示的方式是使用图，其中节点 = 成员，边 = 关系。</p>
<p>比较常见的任务有：</p>
<ul>
<li>社交网络抽取(Social Network Extraction)：从数据源中抽取、构建社交网络</li>
<li><strong>网络中心性分析(Network Centrality Analysis)</strong>：识别社交网络上最重要的节点(重要性的定义由目的、环境所定)<ul>
<li>输入为一个社交网络，输出为最重要的节点列表，一般方法是为节点计算分数或排序，反映节点的重要性/专业性/影响力</li>
<li>对于点重要性的评估可以采用<strong>网络中心性测度(Centrality measures)</strong>方法，具体中心性的定义可能是度数中心性（朋友最多）、中介中心性（处在信息流动关键节点）或亲近中心性（离所有节点平均距离最短）</li>
</ul>
</li>
<li>用户画像：根据用户特点给用户群体分类</li>
<li>链接预测(Link Prediction)：给定一个社交网络，预测哪些节点相互连接。例如: facebook 中的好友推荐</li>
<li>病毒式营销(Viral Marketing)：找出若干用户，为其提供优惠或折扣，从而影响网络上的其他用户，使得收益最大化</li>
</ul>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>尝试在网络寻找应用了数据挖掘的产品，并思考不同公司是如何使用的</li>
<li>对于大数据时代的个人隐私问题，你怎么看？</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这一讲我们简单了解了数据挖掘及应用的方方面面，接下来我们就会具体深入介绍其中一些非常有意思的部分。当然，如果有很多不明白的概念，建议简单看看维基百科了解一下，不过实在不明白也没关系，随着之后的实践，应该会有恍然大悟的一天。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一讲中我们简单复习的概率和统计相关知识，现在我们就可以正式开始接触数据挖掘了。本文是接下来内容的大纲，让大家对互联网搜索与挖掘有一个宏观的了解，即知道要做什么和怎么做。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="数据挖掘" scheme="http://wdxtub.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>第二十四周 - 缘分一道桥</title>
    <link href="http://wdxtub.com/2016/11/25/bridge-of-fate/"/>
    <id>http://wdxtub.com/2016/11/25/bridge-of-fate/</id>
    <published>2016-11-25T15:00:00.000Z</published>
    <updated>2016-11-25T16:40:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>秦时明月汉时关，万里长征人未还，但使龙城飞将在，不教胡马度阴山。</p>
<a id="more"></a>
<hr>
<p>一场大雨之后，深圳也入冬了，虽然温度依然在两位数徘徊，但的确是最冷的两天了。周末在广州看了一场天鹅湖改编的灯光舞台秀，得到的结论是果然我是艺术木头疙瘩，基本处于难以欣赏的状态，唯一能够确定的是中间混杂的农业重金属部分我是非常不喜欢的。</p>
<p>天气变冷之后还在外面跑步就有些艰难了，所以开始了在室内的冬季锻炼计划，这两天是浑身酸疼，不过坚持过去就好了。从朋友圈里得知匹兹堡也迎来了今年冬天的第一场雪，虽然我已经离开，但是却仿佛依然在，这种神奇的感觉，估计明年就不会剩下多少了。</p>
<p>周日(11.20)参加了执信九十五周年校庆，执信之于我像是一个魔法盒子，开启了新世界的大门。走在校园里，看着似曾相识的建筑和教室，踏在操场上的瞬间，感觉大家都回来了。看着优秀校友们做出的成绩，也在心底默默鞭策自己要好好努力。</p>
<p>这周开始了新系列的写作，从最初零散的技术文章，到整理成系列，最后再到作品集，也看到了自己的成长。尤其是冥思苦想出来的『通天塔』和『不周山』系列，还是有中西药结合疗效好的感觉的。随着思考方式的升级，也开始逐渐谋划自己的职业生涯，如何打造自己的核心竞争力和技术门槛，就成为接下来的工作重点了。</p>
<p>雨天上班，到车站还是有一段路的，走在路上意识到：因为有伞，除非是大暴雨，不然其实很难把鞋子淋湿，相比之下，对鞋子更大的威胁是积水，走在积水的路面，除非非常小心，不然湿鞋在所难免。但积水背后是有很多问题的，比方说排水管道，比方说地形地貌。更重要的是，如果能把雨水利用好，不但能解决积水问题，更是把资源利用了起来，但这里的门道，就需要好好想一想了。</p>
<p>最近愈发觉得自己心理素质要加强，很多时候是患得患失让我们最终没有迈开步子，最后既没有得，也没有失，但其实这就是最大的失去吧。不由得想起大鱼海棠里的一段话：</p>
<blockquote>
<p>我们这一生很短，我们终将会失去它，所以不妨大胆一点。爱一个人，攀一座山，追一次梦。有很多事都没有答案，但我相信，上天给我们生命，一定是让我们创造奇迹的。</p>
</blockquote>
<p>谈爱恨，不能潦草。用信任，立下誓言我来熬。这缘分，像一道桥。走天涯，你我卸下战袍，梦回长城谣。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;秦时明月汉时关，万里长征人未还，但使龙城飞将在，不教胡马度阴山。&lt;/p&gt;
    
    </summary>
    
      <category term="Gossip" scheme="http://wdxtub.com/categories/Gossip/"/>
    
    
      <category term="周记" scheme="http://wdxtub.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【不周山之机器学习】壹 基础知识与环境搭建</title>
    <link href="http://wdxtub.com/2016/11/25/bzs-ml-basis/"/>
    <id>http://wdxtub.com/2016/11/25/bzs-ml-basis/</id>
    <published>2016-11-25T01:09:09.000Z</published>
    <updated>2016-11-25T23:21:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎走进机器学习的世界！『不周山之机器学习』系列会结合原理与实践，在弄懂机器学习理论的前提下，把抽象的机器学习理论给用起来。这一讲是系列正文的开端，主要介绍开始学习机器学习的预备知识和相关学习资料，以及搭建好我们用于实践算法的环境。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.25: 完成初稿</li>
</ul>
<h2 id="系列目录"><a href="#系列目录" class="headerlink" title="系列目录"></a>系列目录</h2><ul>
<li><a href="http://wdxtub.com/2016/11/25/bzs-ml-basis/">壹 基础知识与环境搭建</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>复习机器学习所需的数学基础，即线性代数、概率和统计</li>
<li>了解机器学习中的基础概念</li>
<li>完成实验环境的搭建</li>
<li>熟悉 SQL 的基本语法，能够处理比较复杂的查询（可选）</li>
<li>阅读推荐的书籍和文章（可选）</li>
</ol>
<p>注：很多概念对于刚入门的同学来说可能难以理解，不要担心，能明白多少是多少，随着课程的深入，回过头来看看就可以明白了。</p>
<h2 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h2><p>机器学习是一门理论结合实践的学科，甚至可以认为是更加偏重于理论的学科，因为没有对理论的深入了解，可能连现成的机器学习开源工具都学不好。这里列出一些必备基础知识与对应的书籍、课程推荐，方便大家查漏补缺（不会涉及特别艰深的数学）</p>
<p>熟悉机器学习的朋友会发现，线性代数和统计学实际上是机器学习中两种攀登路线的代表。</p>
<ul>
<li>代数方法研究函数和变换，如降维、特征提取、核工程</li>
<li>统计方法研究模型和分布，如图模型、熵模型</li>
</ul>
<p>作为非数学专业科班出身的程序员（比如我），虽然很多时候对于底层的数学理论只有基本的概念，但是了解不同方法背后的核心思想仍然是非常重要的。我建议是先从高层的理论和应用做起，需要深入的时候，再从最底层进行数学的补全，会稍微轻松一些（这句话的意思是涉及到数学的话无论怎么样都不会太轻松）</p>
<h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><p>推荐书籍</p>
<ul>
<li>Linear Algebra and Its Applications (3rd Ed.) by David C. Lay</li>
<li>Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang<ul>
<li><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm" target="_blank" rel="external">课程视频</a></li>
</ul>
</li>
</ul>
<p>线性代数在机器学习中最重要的不是矩阵运算和解方程的方法，而是理解矩阵及其背后的基础概念，这些概念会在之后的学习中不断出现，它们是：</p>
<ul>
<li>子空间 Subsapce</li>
<li>正交 Orthogonality</li>
<li>特征值和特征向量 Eigenvalues and Eigenvectors</li>
<li>线性变换 Linear transform</li>
</ul>
<p>详细内容会在『习题课』子系列中进行介绍。</p>
<h3 id="概率和统计"><a href="#概率和统计" class="headerlink" title="概率和统计"></a>概率和统计</h3><ul>
<li>《普林斯顿微积分读本》</li>
<li>《统计学习理论》李航</li>
<li>A First Course in Probability (9th Ed.) by Sheldon M. Ross</li>
<li>Probability and Statistics (5th Ed.) by Jay L. Devore</li>
</ul>
<p>概率和统计最重要的同样是对基本概念的理解，部分内容需要高等数学基础，《普林斯顿微积分读本》是挺好的复习，其他重要的概念有：</p>
<ul>
<li>概率论公理与随机变量</li>
<li>离散与连续</li>
<li>条件概率与联合分布</li>
<li>期望与极限定理</li>
<li>抽样、估计与模拟</li>
<li>回归与数据分析</li>
</ul>
<p>详细内容会在『习题课』子系列中进行介绍。</p>
<h2 id="机器学习基本概念"><a href="#机器学习基本概念" class="headerlink" title="机器学习基本概念"></a>机器学习基本概念</h2><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><ul>
<li>代表算法：KNN, NB, SVM, DT, BP, RF, GBRT（全称在文末列出）</li>
</ul>
<p>需要标注好的数据，必须知道目标变量的分类信息。对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。</p>
<p>举个例子，现在我有两百张猫和狗的图片，每张图片都已经标记好了是猫还是狗，这样机器可以从这两百张图片中『学习』到猫和狗的特征，这样再来一张猫或者狗的图片，机器能够根据之前学习的特征来判断这是猫还是狗。这里我们首先得知道有两种分类（猫、狗），且要分类的图片也必须在这两类之中。</p>
<h3 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h3><ul>
<li>代表算法：KMEANS, DL（全称在文末列出）</li>
</ul>
<p>数据没有类别信息，也没有目标变量，直接对未标记的样本进行训练学习，来发现我们之前不知道的结构知识。将数据集合分成由类似的对象组成的多个类的过程被称为<strong>聚类</strong>。</p>
<p>举个例子，我们收集一百篇新闻，把里面的词汇拆开，在同一篇文章里的词汇我们认为关联程度更高，这样把所有的词汇都统计一次，就会发现不同的词汇会根据大家习惯的用法而汇聚成不同的类别，比方说褒义词/贬义词。但这里我们没有事先进行标记（即指定哪个词是褒义），而是根据输入的数据自动聚集而成的类别。</p>
<h3 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h3><ul>
<li>代表算法：Graph Inference, Laplacian SVM</li>
</ul>
<p>简单来说就是一小部分数据有类别信息，一大部分数据没有，我们先利用一小部分有类别信息的数据为一大部分没有类别信息的数据进行分析和预测，然后用所有的数据来进行类别预测。一般用于数据量庞大且没办法完全标注的情况（图像识别）。</p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>代表算法：Q-Learning, Temporal difference learning</li>
</ul>
<p>主要用于机器人控制和系统控制领域，输入数据直接会模型的反馈，而不仅仅只是一个判断对错的标准。反馈之后，模型会对此进行调整。简单来说，和我们驯化动物的方式类似，做得好就给奖励，做得不好就给惩罚，这样模型就会根据这样的反馈进行调整。</p>
<h3 id="离散数据"><a href="#离散数据" class="headerlink" title="离散数据"></a>离散数据</h3><p>离散数据（标称型）的目标变量结果只在有限目标集中取值，一般用于<strong>分类</strong>。举个例子，袋子里有红、白、黑三种颜色的小球，每次摸出来一个，记下每次摸出来的颜色，那么这个数据就是一个离散数据，因为每次只可能是红、白、黑三种之一，不可能是其他的（即有限的目标集）。</p>
<h3 id="连续数据"><a href="#连续数据" class="headerlink" title="连续数据"></a>连续数据</h3><p>连续数据（数值型）目标变量主要用于<strong>回归</strong>分析，通过给定数据点的最优拟合曲线。举个例子，我们收集每天的最高温度数据，这个数据是一个连续数据，因为每天的最高温度的值可能是任何的数值（变化范围取决于精度），我们没有办法找到一个可能的值的集合。</p>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>一种概率论在统计学中的应用，主要用于参数评估。理论介绍可以参考<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="external">维基-最大似然估计</a>，这里我用一个简单的例子来进行说明。</p>
<p>假如我们拿到了一个骰子，但掷出每一面的概率不一定是相同的，在没有办法得到真实数值的情况下，我们如何进行估计呢，其中一个方法就是多掷几次看看。比方说我们掷了 600 次，其中：</p>
<ul>
<li>点数 1：100 次</li>
<li>点数 2：150 次</li>
<li>点数 3：150 次</li>
<li>点数 4：50 次</li>
<li>点数 5：50 次</li>
<li>点数 6：100 次</li>
</ul>
<p>从这样的分布来看，每一面的概率是 2:3:3:1:1:2，或者说，如果这个骰子每一面的概率是 2:3:3:1:1:2，那么更可能会掷出我们刚才的结果，所以就干脆把这个概率当做我们已知的最可能的概率，即最大似然估计。按照这个思路，如果我们要预测下一次可能掷出的点数，那么就更可能是点数 2 或 3。</p>
<h3 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h3><p>先验概率是利用过去历史资料计算得到的，一般来说比较准确。而后验概率是在先验概率的基础上增加了信息之后得到的概率，具体的理论介绍可以参考 <a href="https://en.wikipedia.org/wiki/Posterior_probability" target="_blank" rel="external">Wiki - Posterior probability</a>，这里同样使用一个简单的例子进行说明。</p>
<p>假设操场上有两个班各 50 人上体育课，每个班都有 30 个女生和 20 个男生，所有的男生今天都穿了短裤，而一个班的女生穿了短裤，另一个班的女生穿了长裤。忽然来了一个记者在远处抓拍了一张穿短裤但看不清男女的照片，问照片里的是女生的概率是多少。</p>
<p>那么这个概率，就是我们所说的后验概率了，是在先验概率的基础上（即女生占 60%）增加了穿短裤这个信息后需要计算的概率。那么现在来具体计算一下：</p>
<ul>
<li>设事件 A 是看到女生，那么 $P(A)=60\%$，看到男生的概率是 $P(A’)=40\%$</li>
<li>$P(B\;| \;A)$ 是女生穿短裤的概率，这里是 50%</li>
<li>$P(B\;| \;A’)$ 是男生穿短裤的概率，这里是 100%</li>
<li>设事件 B 是看到穿短裤的同学，那么 $P(B)=70\%$，看到穿长裤的同学的概率 $P(B’)=30\%$，这个 70% 是这么算出来的：$P(B)=P(B\;|\;A)P(A)+P(B\;|\;A’)P(A’)$，即『女生穿短裤的概率乘以女生的概率』+『男生穿短裤的概率乘以男生的概率』</li>
</ul>
<p>根据贝叶斯定理，我们可以计算出照片里是女生的概率（即穿短裤的这个人是女生的概率）为</p>
<p>$$P(A\;|\;B)= \frac{P(B\;|\;A)P(A)}{P(B)}= \frac{0.5 \times 0.6}{0.7}=0.4286$$</p>
<p>即照片里是女生的概率是 42.86%</p>
<h3 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h3><ul>
<li>代表算法：KNN, DT, SVM, NN, CRF, LDA(线性判别分析), LR, Boosting</li>
</ul>
<p>由数据直接学习决策函数 $Y=f(X)$ 或条件概率分布 $P(Y\;|\; X)$ 作为预测的模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。</p>
<ul>
<li>优点<ul>
<li>分类边界更灵活，比使用纯概率方法或生产模型得到的更高级</li>
<li>能清晰的分辨出多类或某一类与其他类之间的差异特征</li>
<li>适用于较多类别的识别</li>
<li>可以对数据进行各种程度上的抽象、定义特征并使用特征，简化学习问题</li>
</ul>
</li>
<li>缺点<ul>
<li>不能反映训练数据本身的特性</li>
<li>类似于黑盒，变量之间的关系不明确</li>
<li>无法处理包含隐变量的情况</li>
</ul>
</li>
</ul>
<h3 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h3><ul>
<li>代表算法：NB, GDA, HMM, BN, GMM, LDA(潜在狄利克雷分配)</li>
</ul>
<p>先来复习一下条件概率的公式：</p>
<p>$$P(Y\;|\; X)= \frac{P(X, Y)}{P(X)}$$</p>
<p>所谓生成模型，意思是我们不是直接得到公式左边的部分，而是通过求出公式右边分子分母的两项来得到后验概率 $P(Y\;|\; X)$ 并用它来进行分类。</p>
<ul>
<li>优点<ul>
<li>实际上带的信息要比判别模型丰富，有更强的解释力</li>
<li>研究单类问题比判别模型灵活性强</li>
<li>模型可以通过增量学习得到</li>
<li>能用于数据不完整的情况</li>
<li>可以处理存在隐变量的情况</li>
</ul>
</li>
<li>缺点：<ul>
<li>学习和计算过程比较复杂</li>
</ul>
</li>
</ul>
<p><strong>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</strong></p>
<h3 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h3><ul>
<li>常见线性分类器：LR, 贝叶斯分类，单层感知机，线性回归，SVM(线性核)</li>
</ul>
<p>模型是参数的线性函数，并且存在线性分类面。SVM 是比较特殊的一类，会根据核函数的不同而有不同。线性分类器速度快、编程方便，但是可能拟合效果不会很好。</p>
<ul>
<li>如果特征比数据量还大，那么选择线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</li>
<li>对于维度很高的特征，选择线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</li>
</ul>
<h3 id="非线性分类器"><a href="#非线性分类器" class="headerlink" title="非线性分类器"></a>非线性分类器</h3><ul>
<li>常见非线性分类器：DT, RF, GBDT, 多层感知机，SVM(高斯核)</li>
</ul>
<p>不属于线性分类器的就是非线性分类器，非线性分类器编程复杂，但是效果拟合能力强。</p>
<ul>
<li>对于维度极低的特征，选择非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分</li>
</ul>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。</p>
<p>产生原因</p>
<ul>
<li>因为参数太多，导致模型复杂度上升，容易过拟合</li>
<li>权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征</li>
<li>解决方法：交叉验证法、减少特征、正则化、权值衰减、验证数据</li>
</ul>
<p><strong>泛化能力是指模型对未知数据的预测能力</strong></p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>实际上是通过为要优化的函数增加一个模型复杂度的项目来权衡训练出来模型本身的结构化经验风险，可以防止训练出来的模型过度复杂，降低过拟合的风险。</p>
<p>比方说现在训练出两个模型，一个模型的准确率是 75%，一个是 85%，那么按照原来的选择标准，肯定选 85% 的那个。但是这个 85% 是针对训练数据得出来的模型，可能这个模型只对这些数据有用（也就是过拟合，而这些数据可能不具有很高的代表性），那么在实际使用的过程中，可以表现得比 75% 的那个还要糟糕。但是加入正则化之后，可能原先 75% 的模型会更加优秀，在一定程度上可以降低过拟合的概率。</p>
<p>正所谓奥卡姆剃刀原理所说的那样：<strong>能够很好的解释已知数据并且十分简单才是最好的模型</strong>。</p>
<p>作用是：</p>
<ol>
<li>数值上更容易求解；</li>
<li>特征数目太大时更稳定；</li>
<li>控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。</li>
<li>减小参数空间；参数空间越小，复杂度越低。</li>
<li>系数越小，模型越简单，而模型越简单则泛化能力越强。</li>
<li>可以看成是权值的高斯先验。</li>
</ol>
<p>一般常用的是 L1 和 L2 正则，用来降低模型复杂度：</p>
<ul>
<li>L1 是在损失函数后面加上模型参数的 1 范数（也就是  $|x_i|$ ）</li>
<li>L2 是在损失函数后面加上模型参数的 2 范数（也就是  $\Sigma(x_i^2)$ ），注意L2范数的定义是  $\sqrt{( \Sigma(x_i^2)}$ )，在正则项上没有添加根号是为了更加容易优化</li>
<li>L1 会产生稀疏的特征</li>
<li>L2 会产生更多地特征但是都会接近于 0</li>
</ul>
<p>L1 会趋向于产生少量的特征，而其他的特征都是 0，而 L2 会选择更多的特征，这些特征都会接近于 0。L1 在特征选择时候非常有用，而 L2 就只是一种规则化而已。</p>
<h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3><p>很多机器学习的问题到最后都会变成一类数学问题，而且是一类特定的数学问题，就是凸函数的优化问题。为什么呢？因为对于凸函数，我们已经拥有有效求解出全局最优值的方法，也就是我们常说的凸优化。不过在介绍凸优化之前，我们先了解一些前置基础概念：</p>
<p><strong>凸集</strong></p>
<p>一个集合 C 是凸集，当前仅当 $\forall x,y \in C$ 且 $0 \le \theta \le 1$ 时，都有 $\theta x+(1-\theta)y \in C$</p>
<p>在二维空间举例的话就是，C 是这样一个集合，集合中任意两点所组成的线段，也在集合 C 中，那么 C 是一个凸集（大家可以自己画画图感受一下）。</p>
<p><strong>凸函数</strong></p>
<p>有了凸集的定义，我们可以继续了解凸函数了。一个凸函数 f 其定义域 $D(f)$是凸集，并且对任意 $x,y \in D(f)$ 且 $0 \le \theta \le 1$ 都有<br>$f(\theta x+(1-\theta)y)&lt;=\theta f(x)+(1-\theta)f(y)$（称为 jensen 不等式）</p>
<p>还是用二维空间举例，就是函数曲线上任意两点的割线都在曲线的上方。那么为什么这个性质这么重要呢？我们画一个 U 形，是有一个最低点的，并且任意找两点组成的割线都在曲线的上方，随着我们缩小两点间的距离，我们是能够找到这个最低点的，也就是我们要的全局最优解！</p>
<p>常见的凸函数有</p>
<ul>
<li>指数函数 $f(x)=a^x \; when \;a&gt;1$</li>
<li>负对数函数  $-log_a x \; when \; a>1,x>0$ </li>
<li>开口向上的二次函数</li>
</ul>
<p>凸函数的判定方法为</p>
<ol>
<li>如果 f 是一阶可导的，且 $f’(x)$ 是递增函数，那么是凸函数</li>
<li>如果 f 是一阶可导的，对于任意数据域内的 x,y 满足 $f(y)\ge f(x)+f’(x)(y-x)$，那么是凸函数</li>
<li>如果 f 是二阶可导的，且在区间上非负，那么是凸函数</li>
</ol>
<p>凸函数的进阶——凸优化部分会专门介绍，这里暂时略过</p>
<h2 id="实验环境搭建"><a href="#实验环境搭建" class="headerlink" title="实验环境搭建"></a>实验环境搭建</h2><h3 id="Octave"><a href="#Octave" class="headerlink" title="Octave"></a><a href="https://www.gnu.org/software/octave/" target="_blank" rel="external">Octave</a></h3><p>Octave 可以看作是开源版的 Matlab，具体的安装教程可以参考 <a href="http://wiki.octave.org/Octave_for_MacOS_X" target="_blank" rel="external">Octave Wiki</a>，这里简单说一下使用 homebrew 进行安装的教程（比较简单的方式）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">brew tap homebrew/science</div><div class="line">brew update &amp;&amp; brew upgrade</div><div class="line">brew cask install xquartz</div><div class="line">brew install octave</div></pre></td></tr></table></figure>
<h3 id="Scikit-Learn"><a href="#Scikit-Learn" class="headerlink" title="Scikit-Learn"></a><a href="http://scikit-learn.org/stable/" target="_blank" rel="external">Scikit-Learn</a></h3><p>简单粗暴的方式是直接安装 <a href="https://www.enthought.com/products/canopy" target="_blank" rel="external">Canopy</a> 或 <a href="https://www.continuum.io/downloads" target="_blank" rel="external">Anaconda</a>，如果不想一次安装一堆可能一辈子都用不到的包，可以安装 <a href="http://conda.pydata.org/miniconda.html" target="_blank" rel="external">Miniconda</a>，因为我之前用过 Anaconda 觉得太臃肿，所以这里主要介绍 Miniconda 的安装和使用：</p>
<ul>
<li>下载安装文件，是 <code>.sh</code> 文件</li>
<li>执行 <code>bash Miniconda3-latest-MacOSX-x86_64.sh</code>，跟着步骤走即可</li>
<li>如果使用的是 zsh，那么把 <code>/Users/dawang/miniconda3/bin</code> 添加到 <code>~/.zshrc</code> 中的 PATH 变量中</li>
<li>输入 <code>conda list</code> 可以查看目前已经安装的包</li>
<li>安装 numpy <code>conda install numpy</code></li>
<li>安装 scipy <code>conda install scipy</code></li>
<li>安装 scikit-learn <code>conda install scikit-learn</code></li>
<li>如果想要了解更多，可以参考这里的 <a href="http://conda.pydata.org/docs/test-drive.html" target="_blank" rel="external">30 分钟入门教程</a></li>
<li>如果要更新，可以使用 <code>conda update conda</code></li>
<li>如果想要卸载，先删除整个文件夹 <code>rm -rf ~/miniconda</code>，然后更新 <code>.bash_profile</code>（或其他命令行）的 <code>PATH</code> 变量，最后删除临时文件 <code>rm -rf ~/.condarc ~/.conda ~/.continuum</code></li>
</ul>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>把 Octave 和 Scikit-Learn 官方的快速入门指南过一遍，有一个基础的认知</li>
<li>记下不太了解的名词，留着以后遇到了重点看</li>
<li>回忆一下大数定理和中心极限定理，那些年的高数还记得吗</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这节课中我们简单介绍了学习机器学习所需要的数学基础以及机器学习中常见的基本概念，最后搭建好了实验环境。接下来，我们就可以正式进入机器学习的世界了。</p>
<h2 id="附：缩写对照表"><a href="#附：缩写对照表" class="headerlink" title="附：缩写对照表"></a>附：缩写对照表</h2><ul>
<li>BP - Back Propagation 误差反向传播算法</li>
<li>BN - Bayes Network 贝叶斯网络</li>
<li>CRF - Conditional Random Field 条件随机场</li>
<li>DL - Deep Learning 深度学习</li>
<li>DT - Decision Tree 决策树</li>
<li>GBDT/GBRT/MART - Gradient Boosting Decision Tree </li>
<li>GDA - Gaussian Discriminative Analysis 高斯判别分析</li>
<li>GMM - Gaussian Mixture Model 高斯混合模型</li>
<li>KMEANS - K 均值</li>
<li>KNN - K Nearest Neighbor K 近邻</li>
<li>LDA 潜在狄利克雷分配 or 线性判别分析</li>
<li>LR 线性回归 or Logistic Regression</li>
<li>NB - Naive Bayes 朴素贝叶斯</li>
<li>NN - Neural Network 神经网络</li>
<li>RF - Random Forest 随机森林</li>
<li>SVM - Support Vector Machine 支持向量机</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://kubicode.me/2015/08/16/Machine%20Learning/Common-Interview/" target="_blank" rel="external">机器学习常见面试题整理</a></li>
<li><a href="http://www.cnblogs.com/TenosDoIt/p/3721074.html" target="_blank" rel="external">机器学习：判别模型与生成模型</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;欢迎走进机器学习的世界！『不周山之机器学习』系列会结合原理与实践，在弄懂机器学习理论的前提下，把抽象的机器学习理论给用起来。这一讲是系列正文的开端，主要介绍开始学习机器学习的预备知识和相关学习资料，以及搭建好我们用于实践算法的环境。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="基础" scheme="http://wdxtub.com/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="环境" scheme="http://wdxtub.com/tags/%E7%8E%AF%E5%A2%83/"/>
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
      <category term="机器学习" scheme="http://wdxtub.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>『不周山』研究作品集介绍</title>
    <link href="http://wdxtub.com/2016/11/24/buzhoushan-series-intro/"/>
    <id>http://wdxtub.com/2016/11/24/buzhoushan-series-intro/</id>
    <published>2016-11-24T01:09:09.000Z</published>
    <updated>2016-11-24T13:33:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>『不周山』作品集源于『通天塔』系列第一部作品完成度过半而萌生的想法，如果说『通天塔』作品集强调实践，那么『不周山』作品集则是强调理论。本文聊聊我做这事儿的『初心』。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.24: 初稿完成</li>
</ul>
<h2 id="为什么要写？"><a href="#为什么要写？" class="headerlink" title="为什么要写？"></a>为什么要写？</h2><p>不周山源于《山海经·大荒西经》：</p>
<blockquote>
<p>西北海之外，大荒之隅，有山而不合，名曰不周。</p>
</blockquote>
<p>相传不周山是人界唯一能够到达天界的路径，但终年寒冷，长年飘雪，非凡夫俗子所能徒步到达。而在《淮南子·天文训》中，是这样介绍共工怒撞不周山的：</p>
<blockquote>
<p>昔者共工与颛顼争为帝，怒而触不周之山，天柱折、地维绝，天倾西北，故日月星辰移焉；地不满东南，故水潦尘埃归焉。</p>
</blockquote>
<p>斗转星移，诗人毛泽东也曾提及不周山，因为历史背景的，他对共工的感情和传统史书中不太一样：</p>
<blockquote>
<p>唤起工农千百万，同心干，不周山下红旗乱。</p>
</blockquote>
<p>之所以为作品集取名『不周山』，除了表达在追求知识的道路上永远是『不周』之外，更多的是希望借用其中的隐喻，虽不能从人界到天界，但能『更上一层楼』，也算是为『欲穷千里目』做了一些努力。</p>
<p>兴趣是最好的老师，『不周山』作品集想成为的是对计算机学科感兴趣的同学的『助教』，至少让有志于此的朋友们在追求知识的道路上不太孤单。</p>
<p>我不喜欢培训班的短平快，也痛心于高校与业界的脱节，想找到一种方式，能够把原理和基础说明白的同时，通过实际可操作的案例来让大家更加深入理解数学之美与计算机科学之美。</p>
<p>于是便有了『不周山』这个系列。</p>
<h2 id="能给读者带来什么？"><a href="#能给读者带来什么？" class="headerlink" title="能给读者带来什么？"></a>能给读者带来什么？</h2><p>以下几点是我非常想要借助『不周山』系列带给读者的：</p>
<ul>
<li>学以致用的思维与能力</li>
<li>好奇心与打破沙锅问到底</li>
<li>数学基础知识与应用</li>
<li>概率论基础知识与应用</li>
<li>业界流行风潮背后的理论基础</li>
<li>发现问题解决问题的能力</li>
<li>安排计划，学会学习的能力</li>
</ul>
<p>当然，博客的形式还是有较多局限的，对读者的要求也比较高，不会有人催促，甚至也不会有及时的答疑，一切靠自己。另外，理论和原理很多时候比较枯燥，公式的记忆和推导也需要花费大量的时间。不过这是对自己的长远投资，想要提升自己的天花板，一定得熬过这个阶段。</p>
<h2 id="主要写什么？"><a href="#主要写什么？" class="headerlink" title="主要写什么？"></a>主要写什么？</h2><p>基于自己的工作和实践，目前的已经列入计划的有：</p>
<ul>
<li>机器学习：从公式推导开始，步步为营介绍常用的机器学习算法原理</li>
<li>推荐系统：从实际场景出发，介绍常见的推荐系统背后的算法原理</li>
<li>云计算：着重介绍虚拟化技术，也就是云计算的理论基础</li>
<li>算法与数据结构：计算机科学基础，从更高层次理解基础元素</li>
<li>读薄/读厚 CSAPP：老系列的翻新之作，结合全新的中文版进行增补</li>
</ul>
<p>如果大家有任何意见或者建议，欢迎以各种方式跟我聊聊，联系方式可以在<a href="http://wdxtub.com/thanks/">这里</a>找到</p>
<h2 id="写作格式"><a href="#写作格式" class="headerlink" title="写作格式"></a>写作格式</h2><p>每一篇都会包含：</p>
<ul>
<li>系列目录：方便查阅</li>
<li>任务目标：带着目的学习</li>
<li>试一试：实践部分</li>
<li>总结：回顾学过的内容</li>
</ul>
<p>这四个固定模块，完成每篇文章的任务后，都会有一个可交付可展示的东西，像打怪升级一样，以此鼓励大家。</p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>『不周山』作品集会是一个长期的项目，希望对此感兴趣的同学和朋友能够以远程合作的形式参与进来，众人拾柴火焰高嘛。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;『不周山』作品集源于『通天塔』系列第一部作品完成度过半而萌生的想法，如果说『通天塔』作品集强调实践，那么『不周山』作品集则是强调理论。本文聊聊我做这事儿的『初心』。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="原理" scheme="http://wdxtub.com/tags/%E5%8E%9F%E7%90%86/"/>
    
      <category term="不周山" scheme="http://wdxtub.com/tags/%E4%B8%8D%E5%91%A8%E5%B1%B1/"/>
    
  </entry>
  
  <entry>
    <title>『通天塔』技术作品集介绍</title>
    <link href="http://wdxtub.com/2016/11/19/babel-series-intro/"/>
    <id>http://wdxtub.com/2016/11/19/babel-series-intro/</id>
    <published>2016-11-19T04:12:12.000Z</published>
    <updated>2016-11-24T13:33:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>『通天塔』作品集源于我和华章两位老师的畅谈后萌生的想法，是我在计算机学科教育上一系列尝试的第一步。本文聊聊我做这事儿的『初心』。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.19: 初稿完成</li>
</ul>
<h2 id="为什么要写？"><a href="#为什么要写？" class="headerlink" title="为什么要写？"></a>为什么要写？</h2><p>通天塔也叫<a href="https://zh.wikipedia.org/wiki/%E5%B7%B4%E5%88%A5%E5%A1%94" target="_blank" rel="external">巴别塔</a> ，传说中的故事是这样的：</p>
<blockquote>
<p>当时地上的人们都说同一种语言，当人们离开东方之后，他们来到了示拿之地。在那里，人们想方设法烧砖好让他们能够造出一座城和一座高耸入云的塔来传播自己的名声，以免他们分散到世界各地。上帝来到人间后看到了这座城和这座塔，说一群只说一种语言的人以后便没有他们做不成的事了；于是上帝将他们的语言打乱，这样他们就不能听懂对方说什么了，还把他们分散到了世界各地，这座城市也停止了修建。这座城市就被称为“巴别城”。（来自维基百科）</p>
</blockquote>
<p>之所以为作品集取名『通天塔』，当然是希望能借用其中的隐喻，即使不能『通天』，能站得高一点，也许能看到更好的风景。</p>
<p>兴趣是最好的老师，『通天塔』作品集想成为的是对计算机学科感兴趣的同学的『助教』，至少让有志于此的朋友们在追求知识的道路上不太孤单。</p>
<p>我不喜欢培训班的短平快，也痛心于高校与业界的脱节，想找到一种方式，能够把原理和基础说明白的同时，通过实际可操作的案例来让大家意识到自己学习的东西是有用的，凭借自己的努力可以打造出不一样的东西。</p>
<p>于是便有了『通天塔』这个系列。</p>
<h2 id="能给读者带来什么？"><a href="#能给读者带来什么？" class="headerlink" title="能给读者带来什么？"></a>能给读者带来什么？</h2><p>以下几点是我非常想要借助『通天塔』系列带给读者的：</p>
<ul>
<li>学以致用的思维与能力</li>
<li>好奇心与打破沙锅问到底</li>
<li>计算机系统基础知识的理解</li>
<li>业界常见解决方案的使用</li>
<li>架构和系统设计的思路</li>
<li>发现问题解决问题的能力</li>
<li>安排计划，学会学习的能力</li>
</ul>
<p>当然，博客的形式还是有较多局限的，对读者的要求也比较高，不会有人催促，甚至也不会有及时的答疑，一切靠自己。另外，系统和解决方案的构建大多是从单机开始再拓展到集群，在流程和规范上肯定不如大公司来的专业，不过只要知道了原理，其实工作一段时间自然就会掌握。</p>
<h2 id="主要写什么？"><a href="#主要写什么？" class="headerlink" title="主要写什么？"></a>主要写什么？</h2><p>基于自己的工作和实践，目前的已经列入计划的有：</p>
<ul>
<li>日志分析平台：基于 ElasticStack</li>
<li>数据平台：存储采用 Elasticsearch，后端用 Go，前端用 jQuery</li>
<li>静态博客：打造自己的品牌</li>
<li>W.I.S.E：详情可见<a href="http://wdxtub.com/2016/10/17/wise-plan/">W.I.S.E 计划</a></li>
</ul>
<p>如果大家有任何意见或者建议，欢迎以各种方式跟我聊聊，联系方式可以在<a href="http://wdxtub.com/thanks/">这里</a>找到</p>
<h2 id="写作格式"><a href="#写作格式" class="headerlink" title="写作格式"></a>写作格式</h2><p>每一篇都会包含：</p>
<ul>
<li>系列目录：方便查阅</li>
<li>任务目标：带着目的学习</li>
<li>试一试：实践部分</li>
<li>总结：回顾学过的内容</li>
</ul>
<p>这四个固定模块，完成每篇文章的任务后，都会有一个可交付可展示的东西，像打怪升级一样，以此鼓励大家。</p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>『通天塔』作品集会是一个长期的项目，希望对此感兴趣的同学和朋友能够以远程合作的形式参与进来，众人拾柴火焰高嘛。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;『通天塔』作品集源于我和华章两位老师的畅谈后萌生的想法，是我在计算机学科教育上一系列尝试的第一步。本文聊聊我做这事儿的『初心』。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="原理" scheme="http://wdxtub.com/tags/%E5%8E%9F%E7%90%86/"/>
    
      <category term="实践" scheme="http://wdxtub.com/tags/%E5%AE%9E%E8%B7%B5/"/>
    
      <category term="尝试" scheme="http://wdxtub.com/tags/%E5%B0%9D%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】零 系列简介与环境配置</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/</id>
    <published>2016-11-19T03:11:11.000Z</published>
    <updated>2016-11-21T14:27:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>作为开篇，本文会介绍『日志分析平台』系列的内容梗概并完成基本的环境配置。作为『通天塔』这一技术主题合集的首个系列，我会尝试和『读薄/读厚 CSAPP』系列不一样的风格，但是目的是一致的，就是让感兴趣的朋友少走点弯路。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.19: 初稿完成</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>理解日志分析平台出现的背景</li>
<li>掌握日志从收集、传输到最终统一处理的基本流程中的重要概念</li>
<li>了解 ElasticStack 的各个组成部分及对应的角色</li>
<li>配置好 Linux 基本环境，为之后的工作打好基础</li>
</ol>
<h2 id="什么是日志分析平台"><a href="#什么是日志分析平台" class="headerlink" title="什么是日志分析平台"></a>什么是日志分析平台</h2><p>要回答这个问题，先得弄清楚什么是日志。于是让我们把记忆拉回刚学编程那会儿，想想当时我们是如何写程序运行程序的。具体很多细节我已经记不太清楚，但是把需要检测的变量用 <code>printf</code> 输出到命令行这个简单粗暴的方法，到现在我还时不时会用到。这其实就可以看做是一个『记日志』的行为，虽然非常不靠谱，但是仍提供给我们一些有用的信息。</p>
<p>代码多了之后，想要弄清楚程度到底在干嘛，干到哪一步了，最好的方法就是在每一步的时候输出一些信息，这样出了问题至少能够知道最后运行正常的部分。除了排错之外，日志本身也能给我们提供非常有价值的信息，比方说服务器提供了 100 个对外接口（假设这些接口是并行的，即关闭哪个都无所谓），忽然老板说我们不能提供这么多，只能保留 50 个。那怎么确定要关闭哪五十个呢？其中一个方法就是把访问次数最少的那些给干掉。这时候我们就可以把过去一个星期的日志找出来，统计一下各个接口的使用情况（假设每次接口被访问都会生成一条日志），然后就能排个序，确定需要去掉的接口了。</p>
<p>回顾一下这整个过程：</p>
<ol>
<li>我们提供一些服务，这些服务每被访问一次都会生成一条日志</li>
<li>一般来说我们会把程序产生的日志按日切割，也就是每天会生成一个新的日志文件</li>
<li>有的时候我们需要对大量日志进行统计以得到某些数据</li>
</ol>
<p>当我们的服务只部署在一台服务器上的时候，所有的日志都在同一个地方，基本的统计可以通过 shell 命令配合管道完成。比方说我们想知道接口每天被访问的次数，直接 <code>wc -l date.log</code> 即可，完全不需要费心去折腾什么日志分析平台。但是，随着服务量的增长，原来一行可以搞定的事情变得非常麻烦。</p>
<p>当我们的服务部署在十台服务器上的时候，日志分散在十个地方，基本的统计首先需要在每台服务器上进行，然后再汇总起来。用前面的例子，统计次数的过程就是把原来的命令在十个地方敲十次。这其实还不是最糟的，如果需要跨机器排个序什么的，就…</p>
<p>所以这个时候，日志分析平台应运而生，一般来说套路分三步：</p>
<ol>
<li>把分散在各个机器的日志汇总到一个地方(Shipper, Broker, Indexer)</li>
<li>把这些日志用某种方式保存并索引起来(Search &amp; Storage)</li>
<li>需要的时候直接在汇总的日志中查询(Web Interface)</li>
</ol>
<p><img src="/images/14795970842446.gif" alt=""></p>
<p>听起来没有很麻烦，因为原理大约总是简单的，但具体到做工程，就有各种问题各种坑了。我个人是不提倡自己重新造轮子的（确实没必要），除了现在很多现成的日志分析平台服务之外，我们也可以选择利用开源的力量自己搭建一个日志分析平台。</p>
<p>这也是正是这个系列想要教给大家的。我会从单机系统说起，最后扩展到集群和更复杂的解决方案。</p>
<h2 id="为什么选择-ElasticStack"><a href="#为什么选择-ElasticStack" class="headerlink" title="为什么选择 ElasticStack"></a>为什么选择 ElasticStack</h2><p>（开个玩笑）原因很简单：因为我在用。</p>
<p>（言归正传）ElasticStack 经过这几年的快速发展，版本号一路从 1.0 狂飙到 5.0（这个真的不是在黑），基本上形成了和 <a href="http://flume.apache.org/" target="_blank" rel="external">Flume</a>分庭抗礼的局面。至于为什么，可能是因为大家都喜欢简单粗暴颜高活好不粘人的解决方案吧。</p>
<p>ElasticStack 最初的核心是 ELK(Elasticsearch, Logstash, Kibana) 三兄弟。其中 Logstash 收集数据，Elasticsearch 索引数据，Kibana 展示数据。</p>
<ul>
<li>Elasticsearch 背靠 Lucene 这一老牌劲旅做到了准实时全文索引</li>
<li>Logstash 的配置直接是 Ruby DSL，非常灵活简单</li>
<li>Kibana 则自带各种查询聚合以及生成报表功能。</li>
</ul>
<p>再加上查询简单、扩展容易之类的特点，大受欢迎其实也在情理之中。官方也在不断吸收社区精华的同时开发了安全、报警、监控、报告等一系列功能，再加上能够轻松和 Hadoop 这类分布式计算框架配合，怎么看都是非常不错的选择。</p>
<h2 id="系列内容"><a href="#系列内容" class="headerlink" title="系列内容"></a>系列内容</h2><p>『通天塔之日志分析平台』这个系列的主要是内容是和大家一起一步一步搭建起来一个完整的日志分析平台，具体的内容通过前面的目录应该能够略知一二，会包含业界通用的解决方案，在介绍原理的同时，每一章都会有一定的产出，这样在学习的时候比较不容易懈怠。</p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>关于 ElasticStack 的更多详细介绍会在接下来的文章中继续，现在我们先把系统准备好吧。考虑到现在大部分服务器都在跑 Linux，所以本文会以 Ubuntu 64bit 14.04 这个长期支持版本来作为我们的操作系统。我目前在用的是 MacBook Pro(Retina, 13’, Late 2013)，8GB 内存 2.4 GHz 的 i5，在虚拟机里跑 Ubuntu。</p>
<p>ElasticStack 对系统和软件的配置要求并不高，我们只需要安装 JDK 即可。可以用如下的命令或者是我已经写好的脚本<a href="https://github.com/wdxtub/wdxtools/blob/master/linux-script/ubuntu-java-install.sh" target="_blank" rel="external"><code>ubuntu-java-install.sh</code></a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 添加源 </div><div class="line">sudo add-apt-repository -y ppa:webupd8team/java</div><div class="line"># 更新地址 </div><div class="line">sudo apt-get update</div><div class="line"># 安装 </div><div class="line">sudo apt-get -y install oracle-java8-installer</div></pre></td></tr></table></figure>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><p>因为是序章，所以实践的任务比较简单：</p>
<ul>
<li>在命令行中输入 <code>java -version</code>，看看输出是什么</li>
<li>访问 elastic 的<a href="https://www.elastic.co/" target="_blank" rel="external">官方网站</a>，并简单浏览各个产品的信息</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>相信大家现在已经对我们接下来要做的『日志分析平台』有基本的概念了，如果还有不明白的地方也不要担心，带着未知往前走，其实也是非常有意思的过程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为开篇，本文会介绍『日志分析平台』系列的内容梗概并完成基本的环境配置。作为『通天塔』这一技术主题合集的首个系列，我会尝试和『读薄/读厚 CSAPP』系列不一样的风格，但是目的是一致的，就是让感兴趣的朋友少走点弯路。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="环境" scheme="http://wdxtub.com/tags/%E7%8E%AF%E5%A2%83/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】壹 ELK 环境搭建</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/</id>
    <published>2016-11-19T03:11:10.000Z</published>
    <updated>2016-11-22T12:48:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>前一讲我们对 ElasticStack 进行了简要介绍并完成了基本的系统环境配置，这一次我们要把 Elasticsearch/Logstash/Kibana 安装配置好，并把 Linux 的系统日志导入进来。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.21: 完成初稿</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>掌握并完成 Elasticsearch, Logstash 和 Kibana 的安装配置</li>
<li>了解 Linux 系统日志的内容及保存位置，并利用 Logstash 导入到 Elasticsearch 中，最终由 Kibana 展示</li>
<li>掌握 Linux 的进程控制机制，学会如何启动和关闭前台/后台应用</li>
</ol>
<h2 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h2><p>无论是 Elasticsearch, Logstash 还是 Kibana，我们都推荐手动安装的方式，毕竟不涉及太多配置操作，用 <code>apt-get</code> 反而有些用牛刀杀鸡了。这里我们直接上命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 进入用户文件夹</span></div><div class="line"><span class="built_in">cd</span> ~</div><div class="line"><span class="comment"># 下载 Elasticsearch</span></div><div class="line">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.0.1.tar.gz</div><div class="line"><span class="comment"># 下载 Logstash</span></div><div class="line">wget https://artifacts.elastic.co/downloads/logstash/logstash-5.0.1.tar.gz</div><div class="line"><span class="comment"># 下载 Kibana</span></div><div class="line">wget https://artifacts.elastic.co/downloads/kibana/kibana-5.0.1-linux-x86_64.tar.gz</div><div class="line"></div><div class="line"><span class="comment"># 解压 </span></div><div class="line">tar -xvf elasticsearch-5.0.1.tar.gz</div><div class="line">tar -xvf logstash-5.0.1.tar.gz</div><div class="line">tar -xvf kibana-5.0.1-linux-x86_64.tar.gz</div><div class="line"></div><div class="line"><span class="comment"># 把安装包保存到固定文件夹中，这里叫 software</span></div><div class="line">mv elasticsearch-5.0.1.tar.gz software/mv kibana-5.0.1-linux-x86_64.tar.gz software/mv logstash-5.0.1.tar.gz software/</div></pre></td></tr></table></figure>
<p>解压完成之后，ElasticStack 运行前的准备就基本完成了。Logstash 可以在需要时再启用，这里我们先把 Elasticsearch 和 Kibana 给启动起来（这里我们继续用 tmux，关于 tmux 的使用可以参考我写的<a href="http://wdxtub.com/2016/03/30/tmux-guide/">tmux 指南</a>）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 新建 tmux session</span></div><div class="line">tmux</div><div class="line"><span class="comment"># 启动 Elasticsearch</span></div><div class="line"><span class="comment"># 这里注意，最好虚拟机有 4G 内存，不然很容易卡死</span></div><div class="line"><span class="built_in">cd</span> elasticsearch-5.0.1/bin; ./elasticsearch</div><div class="line"><span class="comment"># 启动 Kibana</span></div><div class="line"><span class="built_in">cd</span> kibana-5.0.1-linux-x86_64/; ./bin/kibana</div></pre></td></tr></table></figure>
<p>打开浏览器，访问 <code>localhost:5601</code>，如果看到如下所示的页面，Elasticsearch 和 Kibana 基本就没问题了。</p>
<p><img src="/images/14797108735042.jpg" alt=""></p>
<p>然后我们体验一下 Logstash，输入下列命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 进入文件夹</span></div><div class="line"><span class="built_in">cd</span> logstash-5.0.1/</div><div class="line"><span class="comment"># 启动 logstash，输入和输出均为命令行</span></div><div class="line">bin/logstash <span class="_">-e</span> <span class="string">'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;'</span></div></pre></td></tr></table></figure>
<p>然后我们随意输入一些内容，显示为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">parallels@ubuntu:~/logstash-5.0.1$ bin/logstash <span class="_">-e</span> <span class="string">'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;'</span>wdxtub.com updatedSending Logstash<span class="string">'s logs to /home/parallels/logstash-5.0.1/logs which is now configured via log4j2.propertiesThe stdin plugin is now waiting for input:[2016-11-20T22:54:20,014][INFO ][logstash.pipeline        ] Starting pipeline &#123;"id"=&gt;"main", "pipeline.workers"=&gt;2, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;5, "pipeline.max_inflight"=&gt;250&#125;[2016-11-20T22:54:20,038][INFO ][logstash.pipeline        ] Pipeline main started[2016-11-20T22:54:20,088][INFO ][logstash.agent           ] Successfully started Logstash API endpoint &#123;:port=&gt;9600&#125;2016-11-21T06:54:20.036Z ubuntu wdxtub.com updatedwdxtub.com is a personal blog2016-11-21T06:54:41.187Z ubuntu wdxtub.com is a personal blogwdxtub.com is created in 20152016-11-21T06:54:55.190Z ubuntu wdxtub.com is created in 2015</span></div></pre></td></tr></table></figure>
<p>至此，ElasticStack 三大组件都已经运行了一次，我们可以用一个实际的任务来上手了。不过开始之前，我们来简单了解一下 ElasticStack 的发展历程。</p>
<h2 id="ElasticStack-5-0-的变化"><a href="#ElasticStack-5-0-的变化" class="headerlink" title="ElasticStack 5.0 的变化"></a>ElasticStack 5.0 的变化</h2><p>ElasticStack 之所以版本一开始就是 5.0，主要原因是把各个产品进行版本统一。5.0 之前，Elasticsearch 的版本是 2.4，Logstash 的版本也是 2.4，但是 Kibana 是 4.5。这样一来开发者其实很难把各个组件对应起来，于是 Elastic 公司干脆直接统一到 5.0，皆大欢喜。</p>
<p>考虑到不是所有的朋友都有接触过 2.4 及之前版本的 ElasticStack，所以这部分会简明扼要介绍一下 5.0 版本的重大改变：</p>
<ul>
<li>Elasticsearch 的底层引擎是 Lucene，5.0 版本中集成了 Lucene6， 新增的多维浮点字段特性极大提高了对 <code>date</code>, <code>numeric</code>, <code>ip</code> 等类型字段的操作的性能。更直观一点说：磁盘空间少一半；索引时间少一半；查询性能提升25%（底层采用 k-d 树编码，更多信息可以在 <a href="http://lucene.apache.org/" target="_blank" rel="external">Lucene 官网</a>中查阅）</li>
<li>Instant Aggregations 特性在 Shard 层级提供了聚合结果的缓存，如果数据没有变化，Elasticsearch 可以直接返回上次的结果</li>
<li>Scliced Scroll 操作允许并发进行数据遍历，大大提升索引重建和遍历的速度</li>
<li>Profile API 可以帮助进行查询的优化，通过确定每个组件的性能消耗来进行优化（设置 <code>profile:true</code> 即可）</li>
<li>Shrink API 可以对分片(Shard)数量进行收缩（从前是不能更改的），利用这个特性，我们可以在写入压力非常大的收集阶段，设置足够多的索引，充分利用shard的并行写能力，索引写完之后收缩成更少的shard，提高查询性能（利用系统的 Hardlink 来进行链接，速度很快）</li>
<li>Rollover API 可以帮助我们按日切割日志，只需要简单的配置即可更加方便灵活分割日志，不用原来 <code>[YYYY-MM-DD]</code> 这样的模板了</li>
<li>Wait for Refresh 提供了文档级别的刷新</li>
<li>Ingest Node 可以直接在建立索引的时候对数据进行加工，这个功能还是很强大的</li>
<li>Task Manager 任务调度管理，来做离线任务</li>
</ul>
<p>总而言之，5.0 是目前最好的一个 ElasticStack 版本，增加了很多新功能，非常值得试一试，更加详细的变动可以查看如下链接，这里不再赘述</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes.html" target="_blank" rel="external">Elasticsearch 5.0 Breaking Changes</a></li>
<li><a href="https://www.elastic.co/guide/en/kibana/current/breaking-changes.html" target="_blank" rel="external">Kibana 5.0 Breaking Changes</a></li>
<li><a href="https://www.elastic.co/guide/en/logstash/current/breaking-changes.html" target="_blank" rel="external">Logstash 5.0 Breaking Changes</a></li>
</ul>
<p>接下来我们先简单了解一下 Elasticsearch 的基本概念，然后就可以上手来完成一个小小的实例了。</p>
<h2 id="Elasticsearch-快速入门"><a href="#Elasticsearch-快速入门" class="headerlink" title="Elasticsearch 快速入门"></a>Elasticsearch 快速入门</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>和 Mongodb 的思路类似，Elasticsearch 中保存的是整个文档(document)，并且还会根据文档的内容进行索引，于是我们得以进行搜索、排序和过滤等操作。在 Elasticsearch 中，利用 JSON 来表示文档。举个例子，下面的 JSON 文档就表示一个用户对象：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"email"</span>: <span class="string">"dawang@wdxtub.com"</span>,</div><div class="line">    <span class="attr">"name"</span>: <span class="string">"Da Wang"</span>,</div><div class="line">    <span class="attr">"info"</span>: &#123;</div><div class="line">        <span class="attr">"bio"</span>: <span class="string">"Sharp Blade, Shape Mind"</span>,</div><div class="line">        <span class="attr">"age"</span>: <span class="string">"25"</span>,</div><div class="line">        <span class="attr">"interests"</span>: [<span class="string">"games"</span>, <span class="string">"music"</span>]</div><div class="line">    &#125;,</div><div class="line">    <span class="attr">"birthday"</span>: <span class="string">"1990/09/11"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 Elasticsearch 中存储数据的行为就叫做索引(indexing)，而前面提到的文档，属于一种类型(type)，这里类型会存在索引(index)中，如果列一个表来和传统数据库比较，大概是这样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">关系型数据</th>
<th style="text-align:center">Elasticsearch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Databases</td>
<td style="text-align:center">Indices</td>
</tr>
<tr>
<td style="text-align:center">Tables</td>
<td style="text-align:center">Types</td>
</tr>
<tr>
<td style="text-align:center">Rows</td>
<td style="text-align:center">Documents</td>
</tr>
<tr>
<td style="text-align:center">Columns</td>
<td style="text-align:center">Fields</td>
</tr>
</tbody>
</table>
<p>一个 Elasticsearch 集群可以包含多个索引(indices，对应于『数据库』)，每个索引可以包含多个类型(types，对应于『表』)，每个类型可以包含多个文档(document，对应于『行』)，每个文档可以包含多个字段(fields，对应于『列』)</p>
<p>这里有一点需要强调一下，前面出现了两种『索引』，第一种，索引(indexing，动词，对应于关系型数据库的插入 insert)指的是把一个文档存储到索引(index，名词) 中；第二种的索引(index，名词）对应于关系型数据库的数据库，这里一定要根据上下文来进行理解。一般来说，我们会对数据库增加索引（这里是第三种意思，就是传统的索引的定义）来提高检索效率，Elasticsearch 和 Lucene 使用『倒排索引』的数据结构来完成这个工作（传统数据库一般用红黑树或者 B 树来完成）。默认情况下，文档中的每个字段都会拥有其对应的倒排索引，Elasticsearch 也是通过这个来进行检索的。</p>
<h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h3><p>我们用一个简单的例子来感受一下 Elasticsearch 的威力吧。设定一个场景，有一天我开了一家名为 “ohmywdx” 的公司，我需要为每个公司里的员工创建记录，我需要做的是：</p>
<ul>
<li>为每个员工的文档(document)建立索引，每个文档包含一个员工的各类信息，类型为 <code>wdxtuber</code></li>
<li><code>wdxtuber</code> 类型属于索引 <code>ohmywdx</code>（这里的索引对应于数据库）</li>
<li><code>ohmywdx</code> 索引存储在 Elasticsearch 集群中</li>
</ul>
<p>我们先来插入几条员工记录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">PUT /ohmywdx/wdxtuber/1</div><div class="line">&#123;</div><div class="line">    <span class="string">"name"</span>: <span class="string">"Da Wang"</span>,</div><div class="line">    <span class="string">"age"</span>: 25,</div><div class="line">    <span class="string">"about"</span>: <span class="string">"First one who is stupid enough to join this company"</span>,</div><div class="line">    <span class="string">"interests"</span>: [<span class="string">"game"</span>, <span class="string">"music"</span>]</div><div class="line">&#125;</div><div class="line"></div><div class="line">PUT /ohmywdx/wdxtuber/2</div><div class="line">&#123;</div><div class="line">    <span class="string">"name"</span>: <span class="string">"Tracy Bryant"</span>,</div><div class="line">    <span class="string">"age"</span>: 20,</div><div class="line">    <span class="string">"about"</span>: <span class="string">"First basketball robot for our company"</span>,</div><div class="line">    <span class="string">"interests"</span>: [<span class="string">"guard"</span>, <span class="string">"forward"</span>]</div><div class="line">&#125;</div><div class="line"></div><div class="line">PUT /ohmywdx/wdxtuber/3</div><div class="line">&#123;</div><div class="line">    <span class="string">"name"</span>: <span class="string">"Shadow Mouse"</span>,</div><div class="line">    <span class="string">"age"</span>: 50,</div><div class="line">    <span class="string">"about"</span>: <span class="string">"Secret agent for our company"</span>,</div><div class="line">    <span class="string">"interests"</span>: [<span class="string">"guitar"</span>, <span class="string">"sugar"</span>]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>具体怎么插入呢，我们可以使用 Kibana 5.0 自带的 Dev Tools 来折腾，把上面的命令粘贴到左边的输入框，然后点击绿色的运行按钮，就可以在右边看到结果了：</p>
<p><img src="/images/14797196056865.jpg" alt=""></p>
<p>按照这个套路，继续把其他两个人的资料插入 Elasticsearch。</p>
<p>有了数据之后，我们来看看如何搜索，简单来说，按照存储的方式来检索即可，不过这里我们使用 GET 方法，如下图所示：</p>
<p><img src="/images/14797197975730.jpg" alt=""></p>
<p>我们可以看到，原始文档内容包含在 <code>_source</code> 字段中。如果说这个搜索太明确了，啥都指定了没意思，我们可以来试试看下面几条搜索</p>
<ul>
<li><code>GET /ohmywdx/wdxtuber/_search</code></li>
<li><code>GET /ohmywdx/wdxtuber/_search?q=name:Da</code></li>
</ul>
<p>这里我们来看看第二个搜索的结果</p>
<figure class="highlight json"><table><tr><td class="code"><pre><div class="line">&#123;  <span class="attr">"took"</span>: <span class="number">11</span>,  <span class="attr">"timed_out"</span>: <span class="literal">false</span>,  <span class="attr">"_shards"</span>: &#123;    <span class="attr">"total"</span>: <span class="number">5</span>,    <span class="attr">"successful"</span>: <span class="number">5</span>,    <span class="attr">"failed"</span>: <span class="number">0</span>  &#125;,  <span class="attr">"hits"</span>: &#123;    <span class="attr">"total"</span>: <span class="number">1</span>,    <span class="attr">"max_score"</span>: <span class="number">0.25811607</span>,    <span class="attr">"hits"</span>: [      &#123;        <span class="attr">"_index"</span>: <span class="string">"ohmywdx"</span>,        <span class="attr">"_type"</span>: <span class="string">"wdxtuber"</span>,        <span class="attr">"_id"</span>: <span class="string">"1"</span>,        <span class="attr">"_score"</span>: <span class="number">0.25811607</span>,        <span class="attr">"_source"</span>: &#123;          <span class="attr">"name"</span>: <span class="string">"Da Wang"</span>,          <span class="attr">"age"</span>: <span class="number">25</span>,          <span class="attr">"about"</span>: <span class="string">"First one who is stupid enough to join this company"</span>,          <span class="attr">"interests"</span>: [            <span class="string">"game"</span>,            <span class="string">"music"</span>          ]        &#125;      &#125;    ]  &#125;&#125;</div></pre></td></tr></table></figure>
<p>除了 <code>_source</code> 的信息之外，我们可以看到有一个 <code>_score</code>，敏感的同学大概会意识到，Elasticsearch 是根据相关性来对结果进行排序的，这个得分就是相关性分数。</p>
<p>除了前面说明的搜索，我们还可以使用 DSL 语句来组合更加复杂的搜索，什么过滤、组合条件、全文、短语搜索，都不在话下，我们甚至可以高亮搜索结果。另外我们还可以利用『聚合』来实现关系型数据库中『Group By』类似的操作。其他诸如推荐、定位、渗透、模糊及部分匹配同样也支持。不过这一篇仅仅是一个简要介绍，就不继续深入了。</p>
<h2 id="实例：收集-Linux-系统日志"><a href="#实例：收集-Linux-系统日志" class="headerlink" title="实例：收集 Linux 系统日志"></a>实例：收集 Linux 系统日志</h2><p>万事俱备，只欠东风，我们现在就把  ElasticStack 用起来！第一个任务很简单，就是把本机的日志给监控起来，这样我们在查询系统发生的事件时，就不用再去 <code>/var/log/</code> 文件夹里『翻箱倒柜』了。</p>
<h3 id="系统日志介绍"><a href="#系统日志介绍" class="headerlink" title="系统日志介绍"></a>系统日志介绍</h3><p>这里简要介绍一下比较通用的系统日志及对应的内容，之后导入日志的时候我会挑选：</p>
<ul>
<li><code>/var/log/apport.log</code> 应用程序崩溃记录</li>
<li><code>/var/log/apt/</code> 用 apt-get 安装卸载软件的信息</li>
<li><code>/var/log/auth.log</code>  登录认证的日志</li>
<li><code>/var/log/boot.log</code>  系统启动时的日志。</li>
<li><code>/var/log/dmesg</code> 包含内核缓冲信息(kernel ringbuffer)。在系统启动时，显示屏幕上的与硬件有关的信息</li>
<li><code>/var/log/faillog</code> 包含用户登录失败信息。此外，错误登录命令也会记录在本文件中</li>
<li><code>/var/log/fsck</code> 文件系统日志</li>
<li><code>/var/log/kern.log</code> 包含内核产生的日志，有助于在定制内核时解决问题</li>
<li><code>/var/log/wtmp</code> 包含登录信息。使用 wtmp 可以找出谁正在登陆进入系统，谁使用命令显示这个文件或信息等</li>
</ul>
<h3 id="启动-Logstash"><a href="#启动-Logstash" class="headerlink" title="启动 Logstash"></a>启动 Logstash</h3><p>这一步的任务是利用 Logstash 把系统日志给导入 Elasticsearch 中。暂时没有使用最新的 Beats 组件，而是继续使用传统的 Logstash 来进行操作（比较重型），用法和之前有些不同，我们会把配置写在一个文件中，而不是直接在命令中输入。</p>
<p>Logstash 使用一个名叫 FileWatch 的 Ruby Gem 库来监听文件变化。这个库支持 glob 展开文件路径，而且会记录一个叫 <code>.sincedb</code> 的数据库文件来跟踪被监听的日志文件的当前读取位置。通过记录下来的 <code>inode</code>, <code>major number</code>, <code>minor number</code> 和 <code>pos</code> 就可以保证不漏过每一条日志。</p>
<p>具体配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 我的习惯是把配置文件统一放到名为 confs 的文件夹中</div><div class="line"># 本配置文件名为 syslog.conf</div><div class="line">input &#123;</div><div class="line">  file &#123;</div><div class="line">    # 确定需要检测的文件</div><div class="line">    path =&gt; [ &quot;/var/log/*.log&quot;, &quot;/var/log/messages&quot;, &quot;/var/log/syslog&quot;, &quot;/var/log/apt&quot;, &quot;/var/log/fsck&quot;, &quot;/var/log/faillog&quot;]</div><div class="line">    # 日志类型</div><div class="line">    type =&gt; &quot;syslog&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line">output &#123;</div><div class="line">  # 输出到命令行，一般用于调试</div><div class="line">  stdout &#123; </div><div class="line">    codec =&gt; rubydebug </div><div class="line">  &#125;</div><div class="line">  # 输出到 elasticsearch，这里指定索引名为 system-log</div><div class="line">  elasticsearch &#123; </div><div class="line">    hosts =&gt; &quot;localhost:9200&quot;</div><div class="line">    index =&gt; &quot;system-log&quot; </div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里说一下 File rotation 的情况，为了处理被 rotate 的情况，最好把 rotate 之后的文件名也加到 path 中（如上面所示），这里注意，如果 <code>start_position</code> 被设为 <code>beginning</code>，被 rotate 的文件因为会被认为是新文件，而重新导入。如果用默认值 <code>end</code>，那么在最后一次读之后到被 rotate 结束前生成的日志不会被采集。</p>
<p>有了配置文件，我们就可以把日志导入到 Elasticsearch 了，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># -f 表示从文件中读取配置</span></div><div class="line">bin/logstash <span class="_">-f</span> confs/syslog.conf</div></pre></td></tr></table></figure>
<p>大概的输出是如下：</p>
<p><img src="/images/14797212868418.jpg" alt=""></p>
<p>如果到这里一切正常，我们就可以去 Kibana 中查看导入的日志了。</p>
<h3 id="使用-Kibana"><a href="#使用-Kibana" class="headerlink" title="使用 Kibana"></a>使用 Kibana</h3><p>Kibana 相当于是 Elasticsearch 的一个可视化插件，所以我们需要在 Management 页面中告诉 Kibana 我们刚才创建的 <code>system-log</code> 索引，完成之后可以看到具体的条目及对应的信息（包括是否可被检索，是否能聚合，是否被分词等）</p>
<p><img src="/images/14797126705730.jpg" alt=""></p>
<p>创建完成后我们就可以在 Discover 面板里查看数据了。</p>
<p><img src="/images/14797126971993.jpg" alt=""></p>
<p>分别介绍下每个面板的作用（更加详细的介绍参见 <a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a>）</p>
<ul>
<li>Discover: 探索数据</li>
<li>Visualize: 可视化统计</li>
<li>Dashboard: 仪表盘</li>
<li>Timelion: 时序，这里我们暂时不用</li>
<li>Management: 设置</li>
<li>Dev Tools: 开发工具，可以方便的测试内置接口</li>
</ul>
<p>这里我简单给出两个实例，介绍一下 Visualize 和 Dashboard 的基本用法</p>
<p>如下图配置，我们可以轻松查看日志都是从哪些文件导入的，除了饼图外，柱状图折线图之类的都是支持的，大家可以自行尝试一下</p>
<p><img src="/images/14797163664177.jpg" alt=""></p>
<p>每个 Visualization 都可以保存，保存之后可以在 Dashboard 面板里集中显示，这样我们只需要看一眼，就对机器运行的状况有一个清晰的了解。</p>
<p><img src="/images/14797164257675.jpg" alt=""></p>
<p>至此，我们就完成了收集系统日志并展示的任务，本章内容到此基本结束。</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>搭建完成之后，看看自己的系统中最多出现的 log 是什么？</li>
<li>在 Kibana 中 Dev Tools 的使用 Elasticsearch 的 HTTP 接口来进行简单的查询</li>
<li>尝试 logstash 自己感兴趣的插件，看看能不能为系统日志添加更多字段</li>
<li>创建几个不同的 Visualization 并添加到 Dashboard 中，让内容更丰富一些</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本节中我们完成了 ElasticStack 核心的 ELK 安装，并用一个简单的实例熟悉了相关操作。刚开始接触，一定是会有很多陌生的概念，建议大家去浏览一下官方的快速入门文档，写得还是比较清晰的。下一讲我们会介绍整个日志处理流程中很重要的『缓冲区』- Kafka，有了它，我们的日志分析平台就有了基本的雏形了。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://www.elastic.co/guide/index.html" target="_blank" rel="external">Elastic Stack and Product Documentation</a></li>
<li><a href="http://www.infoq.com/cn/news/2016/08/Elasticsearch-5-0-Elastic" target="_blank" rel="external">大数据杂谈微课堂|Elasticsearch 5.0新版本的特性与改进</a></li>
<li><a href="http://www.infoq.com/cn/news/2016/11/Elasticsearch-5-0-publish" target="_blank" rel="external">开源搜索引擎Elasticsearch 5.0版本正式发布</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一讲我们对 ElasticStack 进行了简要介绍并完成了基本的系统环境配置，这一次我们要把 Elasticsearch/Logstash/Kibana 安装配置好，并把 Linux 的系统日志导入进来。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="环境" scheme="http://wdxtub.com/tags/%E7%8E%AF%E5%A2%83/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】贰 Kafka 缓冲区</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/</id>
    <published>2016-11-19T03:11:09.000Z</published>
    <updated>2016-11-22T12:48:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>前一讲我们已经搭建好了 ElasticStack 的核心组件，但是在日常使用中，一般会在 Logstash 和 Elasticsearch 之间加一层 Kafka 用来缓存和控制，这次我们就来看看如何实现这样的功能。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.21: 完成初稿</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>安装并配置好 Kafka</li>
<li>理解 Kafka 的工作机制</li>
<li>掌握 Kafka 的基本操作，学会基本的错误处理</li>
<li>完成 Logstash-Kafka-Elasticsearch 链路的构建，理解这种架构的优劣</li>
<li>通过实际操作，增加对 ElasticStack 的理解</li>
</ol>
<h2 id="Kafka-简介"><a href="#Kafka-简介" class="headerlink" title="Kafka 简介"></a>Kafka 简介</h2><p>作为云计算大数据的套件，Kafka 是一个分布式的、可分区的、可复制的消息系统。该有的功能基本都有，而且有自己的特色：</p>
<ul>
<li>以 topic 为单位进行消息归纳</li>
<li>向 topic 发布消息的是 producer</li>
<li>从 topic 获取消息的是 consumer</li>
<li>集群方式运行，每个服务叫 broker</li>
<li>客户端和服务器通过 TCP 进行通信</li>
</ul>
<p>在Kafka集群中，没有“中心主节点”的概念，集群中所有的服务器都是对等的，因此，可以在不做任何配置的更改的情况下实现服务器的的添加与删除，同样的消息的生产者和消费者也能够做到随意重启和机器的上下线。</p>
<p>对每个 topic 来说，Kafka 会对其进行分区，每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做 offset,用来在分区中唯一的标识这个消息。</p>
<p>发布消息通常有两种模式：队列模式(queuing)和发布-订阅模式(publish-subscribe)。队列模式中，consumers 可以同时从服务端读取消息，每个消息只被其中一个 consumer 读到；发布-订阅模式中消息被广播到所有的 consumer 中。更常见的是，每个 topic 都有若干数量的 consumer 组，每个组都是一个逻辑上的『订阅者』，为了容错和更好的稳定性，每个组由若干 consumer 组成。这其实就是一个发布-订阅模式，只不过订阅者是个组而不是单个 consumer。</p>
<p>通过分区的概念，Kafka 可以在多个 consumer 组并发的情况下提供较好的有序性和负载均衡。将每个分区分只分发给一个 consumer 组，这样一个分区就只被这个组的一个 consumer 消费，就可以顺序的消费这个分区的消息。因为有多个分区，依然可以在多个 consumer 组之间进行负载均衡。注意 consumer 组的数量不能多于分区的数量，也就是有多少分区就允许多少并发消费。</p>
<p>Kafka 只能保证一个分区之内消息的有序性，在不同的分区之间是不可以的，这已经可以满足大部分应用的需求。如果需要 topic 中所有消息的有序性，那就只能让这个 topic 只有一个分区，当然也就只有一个 consumer 组消费它。</p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>我们可以根据自己的需求来进行简单的配置，具体如下：</p>
<h3 id="1-下载-Kafka"><a href="#1-下载-Kafka" class="headerlink" title="(1) 下载 Kafka"></a>(1) 下载 Kafka</h3><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 美国主机</span></div><div class="line">wget http://www-us.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz</div><div class="line"><span class="comment"># 解压</span></div><div class="line">tar -xzf kafka_2.11-0.10.1.0.tgz</div><div class="line"><span class="comment"># 进入文件夹</span></div><div class="line"><span class="built_in">cd</span> kafka_2.11-0.10.1.0</div></pre></td></tr></table></figure>
<h3 id="2-配置-Zookeeper-及-Kafka"><a href="#2-配置-Zookeeper-及-Kafka" class="headerlink" title="(2) 配置 Zookeeper 及 Kafka"></a>(2) 配置 Zookeeper 及 Kafka</h3><p>Zookeeper 的配置在 <code>config/zookeeper.properties</code> 文件中，Kafka 的配置在 <code>config/server.properties</code> 文件中。</p>
<p>Zookeeper 的配置不需要特别更改，注意默认数据存放的位置是 <code>/zookeeper</code>，这里最好放到挂载磁盘上（如果使用云主机，一般来说系统盘比较小，具体可以用 <code>df -h</code> 查看）。Kafka 的默认数据存放位置是 <code>/tmp/kafka-logs</code>，我们把 zookeeper 和 kafka 的数据存放位置一并进行修改</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 在 zookeeper.properties 中</span></div><div class="line">/data/home/logger/kafka-data/zookeeper</div><div class="line"></div><div class="line"><span class="comment"># 在 server.properties 中</span></div><div class="line">log.dirs=/data/home/logger/kafka-data/kafka-logs</div></pre></td></tr></table></figure>
<p>其他配置这里推荐进行一些修改，具体如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># advertised.listerners 改为对外服务的地址</span></div><div class="line"><span class="comment"># 比如对外的 ip 地址是 xx.xx.xx.xx，端口是 8080，那么</span></div><div class="line">advertised.listeners=PLAINTEXT://xx.xx.xx.xx:8080</div><div class="line"></div><div class="line"><span class="comment"># 允许删除 topic</span></div><div class="line">delete.topic.enable=<span class="literal">true</span></div><div class="line"></div><div class="line"><span class="comment"># 不允许自动创建 topic，方便管理</span></div><div class="line">auto.create.topics.enable=<span class="literal">false</span></div><div class="line"></div><div class="line"><span class="comment"># 设定每个 topic 的分区数量，这里设为 100</span></div><div class="line">num.partitions=100</div><div class="line"></div><div class="line"><span class="comment"># 设定日志保留的时间，这里改为 72 小时</span></div><div class="line">log.retention.hours=72</div></pre></td></tr></table></figure>
<h3 id="3-启动-Zookeeper-及-Kafka"><a href="#3-启动-Zookeeper-及-Kafka" class="headerlink" title="(3) 启动 Zookeeper 及 Kafka"></a>(3) 启动 Zookeeper 及 Kafka</h3><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 可以使用 tmux 或 nohup &amp; 等方式来进行后台运行，这里使用 tmux</span></div><div class="line"><span class="comment"># 启动 Zookeeper</span></div><div class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</div><div class="line"></div><div class="line"><span class="comment"># 启动 Kafka</span></div><div class="line">bin/kafka-server-start.sh config/server.properties</div></pre></td></tr></table></figure>
<p>如果没有出现错误，则启动成功，接下来可以做一个简单的测试</p>
<h3 id="4-内部测试-Kafka"><a href="#4-内部测试-Kafka" class="headerlink" title="(4) 内部测试 Kafka"></a>(4) 内部测试 Kafka</h3><p>先创建一个叫做 wdxtub 的 topic，它只有一个分区和一个副本，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 创建 topic</span></div><div class="line">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic wdxtub</div></pre></td></tr></table></figure>
<p>然后我们可以使用 <code>bin/kafka-topics.sh --list --zookeeper localhost:2181</code> 命令来查看目前已有的 topic 列表，这时候应该能看到我们刚才创建的名为 wdxtub 的 topic。如果看到程序返回了 <code>wdxtub</code>，那么表示 topic 创建成功。</p>
<p>接下来我们创建一个简单的 producer，用来从标准输入中读取消息并发送给 Kafka，命令为 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 创建一个向 topic wdxtub 发送消息的 producer</span></div><div class="line"><span class="comment"># 按回车发送，ctrl+c 退出</span></div><div class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wdxtub</div></pre></td></tr></table></figure>
<p>另外新建一个窗口，启动一个 consumer，用来读取消息并输出到标准输出，命令为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 创建一个从 topic wdxtub 读取消息的 consumer</span></div><div class="line">bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic wdxtub --from-beginning</div></pre></td></tr></table></figure>
<p>启动成功后，我们在 producer 中输入的内容，就可以在 consumer 中看到：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># producer 窗口内容</span></div><div class="line">$&gt; ~/kafka_2.11-0.10.1.0$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic wdxtub</div><div class="line">abcdefu</div><div class="line">dalkdjflka^H^H^H^H^H^H^H</div><div class="line">wdxtub.com</div><div class="line">wdxtub.com is good</div><div class="line"></div><div class="line"><span class="comment"># consumer 窗口内容</span></div><div class="line">$&gt; ~/kafka_2.11-0.10.1.0$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic wdxtub --from-beginning</div><div class="line">Using the ConsoleConsumer with old consumer is deprecated and will be removed <span class="keyword">in</span> a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].</div><div class="line">abcdefu</div><div class="line">dalkdjflka</div><div class="line">wdxtub.com</div><div class="line">wdxtub.com is good</div></pre></td></tr></table></figure>
<h3 id="5-Nginx-配置"><a href="#5-Nginx-配置" class="headerlink" title="(5) Nginx 配置"></a>(5) Nginx 配置</h3><p>因为 Kafka 集群的通讯是走内网 ip，而外网访问的端口因为安全考虑只开了少数几个（这里是 8080），所以我们用 Nginx 反向代理来连通内外网</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">upstream mq_pool&#123;</div><div class="line">server ip1:9092 weight=1 max_fails=3 fail_timeout=30s;</div><div class="line">server localhost:9092 weight=1 max_fails=3 fail_timeout=30s;</div><div class="line">&#125;</div><div class="line"></div><div class="line">server&#123;</div><div class="line">listen 8080;</div><div class="line">allow all;</div><div class="line">proxy_pass mq_pool;</div><div class="line">proxy_connect_timeout 24h;</div><div class="line">proxy_timeout 24h;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个配置的意思大概是把所有 8080 端口的消息转发到 <code>mq_pool</code> 的两台机器上（负载均衡），其他的就是常规配置。</p>
<h3 id="6-外部测试-Kafka"><a href="#6-外部测试-Kafka" class="headerlink" title="(6) 外部测试 Kafka"></a>(6) 外部测试 Kafka</h3><p>现在我们的 Kafka 已经在运行了，但是刚才的测试程序是在本机，所以我们无法保证外部应用也能向 Kafka 发送消息（很多时候会用 Nginx 来控制），这里我们就来编写一段简单的 python 脚本来测试能否从其他服务器连接 Kafka。</p>
<p>这里我们采用的 python 包名为 <a href="https://github.com/dpkp/kafka-python" target="_blank" rel="external">dpkp/kafka-python</a>，如果已经有 <code>pip</code> 工具的话，直接 <code>pip install kafka-python</code> 即可。然后我们可以简单编写一个 producer 来进行测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 名为 kafka-test.py</span></div><div class="line"><span class="keyword">from</span> kafka <span class="keyword">import</span> KafkaProducer</div><div class="line"><span class="comment"># 设置 Kafka 地址</span></div><div class="line">producer = KafkaProducer(</div><div class="line">    bootstrap_servers=<span class="string">'your.host.name:8080'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 设置需要发送的 topic 及内容</span></div><div class="line">producer.send(<span class="string">'wdxtub'</span>, <span class="string">'Hello World! This is wdxtub.com.'</span>)</div></pre></td></tr></table></figure>
<p>执行一下 <code>python kafka-test.py</code>，如果能在第(4)步中打开的 consumer 中看到 Hello World 这行字儿，说明能够正确连接。</p>
<h2 id="Kafka-常用操作"><a href="#Kafka-常用操作" class="headerlink" title="Kafka 常用操作"></a>Kafka 常用操作</h2><p>所有的工具都可以在 <code>bin/</code> 文件夹下查看，如果不带任何参数，就会给出所有命令的列表说明，这里只简要说明一些常用的命令</p>
<h3 id="管理-topic"><a href="#管理-topic" class="headerlink" title="管理 topic"></a>管理 topic</h3><p>可以手动创建 topic，或在数据进来时自动创建不存在的 topic，如果是自动创建的话，可能需要根据<a href="http://kafka.apache.org/documentation.html#topic-config" target="_blank" rel="external">这里</a>来进行对应调整。</p>
<p><strong>创建 topic</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --create --topic my_topic_name --partitions 20 --replication-factor 3 --config x=y</code></p>
<p>replication-factor 控制复制的份数，建议 2-3 份来兼顾容错和效率。partitions 控制该 topic 将被分区的数目，partitions 的数目最好不要超过服务器的个数（因为分区的意义是增加并行效率，而服务器数量决定了并行的数量，假设只有 2 台服务器，分 4 个区和 2 个区其实差别不大）。另外，topic 的名称不能超过 249 个字符</p>
<p><strong>修改 topic</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name --partitions 40</code></p>
<p>这里需要注意，即使修改了分区的个数，已有的数据也不会进行变动，Kafka 不会做任何自动重分布</p>
<p><strong>增加配置</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name --config x=y</code></p>
<p><strong>移除配置</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic my_topic_name --delete-config x</code></p>
<p><strong>删除 topic</strong></p>
<p><code>bin/kafka-topics.sh --zookeeper zk_host:port/chroot --delete --topic my_topic_name</code></p>
<p>这个需要 <code>delete.topic.enable=true</code>，目前 Kafka 不支持减少 topic 的分区数目</p>
<h3 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h3><p>Kafka 会自动检测 broker 的状态并根据机器状态选举出新的 leader。但是如果需要进行配置更改停机的时候，我们就需要使用优雅关闭了，好处在于：</p>
<ol>
<li>会把所有的日志同步到磁盘上，避免重启之后的日志恢复，减少重启时间</li>
<li>会在关闭前把以这台机为 leader 的分区数据迁移到其他节点，会减少不可用的时间</li>
</ol>
<p>但是这个需要开启 <code>controlled.shutdown.enable=true</code>。</p>
<p>刚重启之后的节点不是任何分区的 leader，所以这时候需要进行重新分配：</p>
<p><code>bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot</code></p>
<p>这里需要开启 <code>auto.leader.rebalance.enable=true</code></p>
<p>然后可以使用脚本 <code>bin/kafka-server-stop.sh</code></p>
<p>注意，如果配置文件中没有 <code>auto.leader.rebalance.enable=true</code>，就还需要重新平衡。</p>
<h2 id="深入理解"><a href="#深入理解" class="headerlink" title="深入理解"></a>深入理解</h2><p>这里只是一部分摘录，更多内容可查阅参考链接（尤其是美团技术博客的那篇）</p>
<h3 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h3><p>Kafka 大量依赖文件系统去存储和缓存消息。而文件系统最终会放在硬盘上，不过不用担心，很多时候硬盘的快慢完全取决于使用它的方式。设计良好的硬盘架构可以和内存一样快。</p>
<p>所以与传统的将数据缓存在内存中然后刷到硬盘的设计不同，Kafka直接将数据写到了文件系统的日志中，因此也避开了 JVM 的劣势——Java 对象占用空间巨大，数据量增大后垃圾回收有困难。使用文件系统，即使系统重启了，也不需要刷新数据，也简化了维护数据一致性的逻辑。</p>
<p>对于主要用于日志处理的消息系统，数据的持久化可以简单的通过将数据追加到文件中实现，读的时候从文件中读就好了。这样做的好处是读和写都是 O(1) 的，并且读操作不会阻塞写操作和其他操作。这样带来的性能优势是很明显的，因为性能和数据的大小没有关系了。</p>
<p>既然可以使用几乎没有容量限制（相对于内存来说）的硬盘空间建立消息系统，就可以在没有性能损失的情况下提供一些一般消息系统不具备的特性。比如，一般的消息系统都是在消息被消费后立即删除，Kafka却可以将消息保存一段时间（比如一星期），这给consumer提供了很好的机动性和灵活性。</p>
<h3 id="事务定义"><a href="#事务定义" class="headerlink" title="事务定义"></a>事务定义</h3><p>数据传输的事务定义通常有以下三种级别：</p>
<ul>
<li>最多一次: 消息不会被重复发送，最多被传输一次，但也有可能一次不传输。</li>
<li>最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输.</li>
<li>精确的一次（Exactly once）: 不会漏传输也不会重复传输,每个消息都传输被一次而且仅仅被传输一次，这是大家所期望的。</li>
</ul>
<p>Kafka 的机制和 git 有点类似，有一个 commit 的概念，一旦提交且 broker 在工作，那么数据就不会丢失。如果 producer 发布消息时发生了网络错误，但又不确定实在提交之前发生的还是提交之后发生的，这种情况虽然不常见，但是必须考虑进去，现在Kafka版本还没有解决这个问题，将来的版本正在努力尝试解决。</p>
<p>并不是所有的情况都需要“精确的一次”这样高的级别，Kafka 允许 producer 灵活的指定级别。比如 producer 可以指定必须等待消息被提交的通知，或者完全的异步发送消息而不等待任何通知，或者仅仅等待 leader 声明它拿到了消息（followers没有必要）。</p>
<p>现在从 consumer 的方面考虑这个问题，所有的副本都有相同的日志文件和相同的offset，consumer 维护自己消费的消息的 offset。如果 consumer 崩溃了，会有另外一个 consumer 接着消费消息，它需要从一个合适的 offset 继续处理。这种情况下可以有以下选择：</p>
<ul>
<li>consumer 可以先读取消息，然后将 offset 写入日志文件中，然后再处理消息。这存在一种可能就是在存储 offset 后还没处理消息就 crash 了，新的 consumer 继续从这个 offset 处理，那么就会有些消息永远不会被处理，这就是上面说的『最多一次』</li>
<li>consumer 可以先读取消息，处理消息，最后记录o ffset，当然如果在记录 offset 之前就 crash 了，新的 consumer 会重复的消费一些消息，这就是上面说的『最少一次』</li>
<li>『精确一次』可以通过将提交分为两个阶段来解决：保存了 offset 后提交一次，消息处理成功之后再提交一次。但是还有个更简单的做法：将消息的 offset 和消息被处理后的结果保存在一起。比如用 Hadoop ETL 处理消息时，将处理后的结果和 offset 同时保存在 HDFS 中，这样就能保证消息和 offser 同时被处理了</li>
</ul>
<h3 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h3><p>Kafka 在提高效率方面做了很大努力。Kafka 的一个主要使用场景是处理网站活动日志，吞吐量是非常大的，每个页面都会产生好多次写操作。读方面，假设每个消息只被消费一次，读的量的也是很大的，Kafka 也尽量使读的操作更轻量化。</p>
<p>线性读写的情况下影响磁盘性能问题大约有两个方面：太多的琐碎的 I/O 操作和太多的字节拷贝。I/O 问题发生在客户端和服务端之间，也发生在服务端内部的持久化的操作中。</p>
<p><strong>消息集(message set)</strong></p>
<p>为了避免这些问题，Kafka 建立了<strong>消息集(message set)</strong>的概念，将消息组织到一起，作为处理的单位。以消息集为单位处理消息，比以单个的消息为单位处理，会提升不少性能。Producer 把消息集一块发送给服务端，而不是一条条的发送；服务端把消息集一次性的追加到日志文件中，这样减少了琐碎的 I/O 操作。consumer 也可以一次性的请求一个消息集。</p>
<p>另外一个性能优化是在字节拷贝方面。在低负载的情况下这不是问题，但是在高负载的情况下它的影响还是很大的。为了避免这个问题，Kafka 使用了标准的二进制消息格式，这个格式可以在 producer, broker 和 producer 之间共享而无需做任何改动。</p>
<p><strong>zero copy</strong></p>
<p>Broker 维护的消息日志仅仅是一些目录文件，消息集以固定队的格式写入到日志文件中，这个格式 producer 和 consumer 是共享的，这使得 Kafka 可以一个很重要的点进行优化：消息在网络上的传递。现代的 unix 操作系统提供了高性能的将数据从页面缓存发送到 socket 的系统函数，在 linux 中，这个函数是 <code>sendfile</code></p>
<p>为了更好的理解 <code>sendfile</code> 的好处，我们先来看下一般将数据从文件发送到 socket 的数据流向：</p>
<ul>
<li>操作系统把数据从文件拷贝内核中的页缓存中</li>
<li>应用程序从页缓存从把数据拷贝自己的内存缓存中</li>
<li>应用程序将数据写入到内核中 socket 缓存中</li>
<li>操作系统把数据从 socket 缓存中拷贝到网卡接口缓存，从这里发送到网络上。</li>
</ul>
<p>这显然是低效率的，有 4 次拷贝和 2 次系统调用。<code>sendfile</code> 通过直接将数据从页面缓存发送网卡接口缓存，避免了重复拷贝，大大的优化了性能。</p>
<p>在一个多consumers的场景里，数据仅仅被拷贝到页面缓存一次而不是每次消费消息的时候都重复的进行拷贝。这使得消息以近乎网络带宽的速率发送出去。这样在磁盘层面你几乎看不到任何的读操作，因为数据都是从页面缓存中直接发送到网络上去了。</p>
<p><strong>数据压缩</strong></p>
<p>很多时候，性能的瓶颈并非CPU或者硬盘而是网络带宽，对于需要在数据中心之间传送大量数据的应用更是如此。当然用户可以在没有 Kafka 支持的情况下各自压缩自己的消息，但是这将导致较低的压缩率，因为相比于将消息单独压缩，将大量文件压缩在一起才能起到最好的压缩效果。</p>
<p>Kafka 采用了端到端的压缩：因为有『消息集』的概念，客户端的消息可以一起被压缩后送到服务端，并以压缩后的格式写入日志文件，以压缩的格式发送到 consumer，消息从 producer 发出到 consumer 拿到都被是压缩的，只有在 consumer 使用的时候才被解压缩，所以叫做『端到端的压缩』。Kafka支持GZIP和Snappy压缩协议。</p>
<h2 id="实例：把系统日志通过-Kafka-接入-Elasticsearch"><a href="#实例：把系统日志通过-Kafka-接入-Elasticsearch" class="headerlink" title="实例：把系统日志通过 Kafka 接入 Elasticsearch"></a>实例：把系统日志通过 Kafka 接入 Elasticsearch</h2><p>现在我们就把上一讲搭建好的架构中加入 Kakfa 作为缓冲区，具体分两步</p>
<h3 id="1-Logstash-gt-Kafka"><a href="#1-Logstash-gt-Kafka" class="headerlink" title="(1) Logstash -&gt; Kafka"></a>(1) Logstash -&gt; Kafka</h3><p>让我们回想一下之前的架构，Logstash 会直接把日志发送给 Elasticsearch，再由 Kibana 进行展示。因为 Logstash 是同步把日志发送给 Elasticsearch 的，所以等于这俩耦合在了一起，Elasticsearch 一旦挂掉，可能就会丢失数据。</p>
<p>于是，我们考虑利用 Kafka 作为缓冲区，让 Logstash 不受 Elasticsearch 的影响。所以第一步就是让 Logstash 把日志发送到 Kafka，这里 Logstash 相当于 producer。</p>
<p>不过在开始之前，我们先启动 Kafka</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 启动 Zookeeper</span></div><div class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</div><div class="line"><span class="comment"># 启动 Kafka</span></div><div class="line">bin/kafka-server-start.sh config/server.properties</div></pre></td></tr></table></figure>
<p>我们之前的 Logstash 配置文件是把日志直接发送到 Elasticsearch 的，这里我们需要更新为发送到 Kafka</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 我的习惯是把配置文件统一放到名为 confs 的文件夹中</div><div class="line"># 本配置文件名为 log-to-kafka.conf</div><div class="line">input &#123;</div><div class="line">  file &#123;</div><div class="line">    # 确定需要检测的文件</div><div class="line">    path =&gt; [ &quot;/var/log/*.log&quot;, &quot;/var/log/messages&quot;, &quot;/var/log/syslog&quot;, &quot;/var/log/apt&quot;, &quot;/var/log/fsck&quot;, &quot;/var/log/faillog&quot;]</div><div class="line">    # 日志类型</div><div class="line">    type =&gt; &quot;syslog&quot;</div><div class="line">    add_field =&gt; &#123; &quot;service&quot; =&gt; &quot;system-log&quot;&#125;</div><div class="line">    # stat_interval =&gt; 1800</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line">output &#123;</div><div class="line">  # 输出到命令行，一般用于调试</div><div class="line">  stdout &#123; </div><div class="line">    codec =&gt; rubydebug </div><div class="line">  &#125;</div><div class="line">  # 输出到 Kafka，topic 名称为 logs，地址为默认的端口号</div><div class="line">  kafka &#123;</div><div class="line">    topic_id =&gt; &quot;logs&quot;</div><div class="line">    bootstrap_servers =&gt; &quot;localhost:9092&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>file 插件其他一些配置设定原因</p>
<ul>
<li><code>add_field</code> 添加一个 topic 字段，用作之后导入 elasticsearch 的索引标识</li>
<li><code>stat_interval</code> 单位是秒，这里 30 分钟进行一次检测，不过测试的时候需要去掉这个配置</li>
</ul>
<p>kafka 插件其他一些需要注意的配置</p>
<ul>
<li><code>acks</code> 可以选的值为 <code>0</code>, <code>1</code>, <code>all</code>，这里解释一下，0 表示不需要 server 返回就认为请求已完成；1 表示需要 leader 返回才认为请求完成；all 表示需要所有的服务器返回才认为请求完成</li>
<li><code>batch_size</code> 单位是字节，如果是发送到同一分区，会攒够这个大小才发送一次请求</li>
<li><code>block_on_buffer_full</code> 这个设置在缓冲区慢了之后阻塞还是直接报错</li>
<li><code>buffer_memory</code> 发送给服务器之前的缓冲区大小，单位是字节</li>
<li><code>client_id</code> 可以在这里设定有意义的名字，就不一定要用 ip 和 端口来区分</li>
<li><code>compression_type</code> 压缩方式，默认是 <code>none</code>，其他可选的是 <code>gzip</code> 和 <code>snappy</code></li>
</ul>
<h3 id="2-Kafka-gt-Elasticsearch"><a href="#2-Kafka-gt-Elasticsearch" class="headerlink" title="(2) Kafka -&gt; Elasticsearch"></a>(2) Kafka -&gt; Elasticsearch</h3><p>利用 Logstash 从 Kafka 导出数据到 Elasticsearch。这一步就比较简单了，先从 Kafka 中读取，然后写入到 elasticsearch，这里 Logstash 作为 consumer。唯一需要注意的地方是要保证 topic 名称一致</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 文件名 kafka-to-es.conf</div><div class="line">input &#123;</div><div class="line">  kafka &#123;</div><div class="line">    bootstrap_servers =&gt; &quot;localhost:9092&quot;</div><div class="line">    topics =&gt; [&quot;logs&quot;]</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">output &#123;</div><div class="line">  # for debugging</div><div class="line">  stdout &#123;</div><div class="line">     codec =&gt; rubydebug</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  elasticsearch &#123; </div><div class="line">    hosts =&gt; &quot;localhost:9200&quot;</div><div class="line">    index =&gt; &quot;system-log&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>至此，我们完成了从 Logstash 到 Kafka 再到 Elasticsearch 的连接，下一步就可以用 kibana 来展示日志的监控分析结果了。</p>
<p><img src="/images/14797818253620.jpg" alt=""></p>
<p>如上图所示，打开 Kibana，即可见到我们使用 Logstash 通过 Kafka 再发送到 Elasticsearch 的日志。至此，我们就成功把 Kafka 加入到日志分析平台的架构中了。</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>查阅 Logstash 的 Kafka 插件的文档，了解其他的配置选项</li>
<li>Logstash 能够处理 json 格式的日志，试着把系统日志转换成 json，并进行处理</li>
<li>更新 Logstash 配置，看看能不能多记录一些系统事件</li>
<li>随着日志的增多，使用 Kibana 多创建一些图表并添加到 Dashboard 中</li>
</ol>
<p>一个可能的例子如下：</p>
<p><img src="/images/14797829019725.jpg" alt=""></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这一讲我们主要学习了 Kafka 的相关内容，并在了解原理的基础上更新了日志分析平台的架构，这样我们的日志在发送到 Elasticsearch 之前。下一讲我们会在单机的状态下完成监控、安全、报警与通知的功能。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="http://kafka.apache.org/quickstart" target="_blank" rel="external">Kafka 快速入门</a></li>
<li><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz" target="_blank" rel="external">Kafka 2.11-0.10.1.0 下载</a></li>
<li><a href="http://blog.csdn.net/LOUISLIAOXH/article/details/51567515" target="_blank" rel="external">Kafka学习整理六(server.properties配置实践)</a></li>
<li><a href="http://kafka.apache.org/" target="_blank" rel="external">Apache Kafka</a></li>
<li><a href="http://kafka.apache.org/documentation.html#quickstart" target="_blank" rel="external">Quick Start</a></li>
<li><a href="http://www.aboutyun.com/thread-12882-1-1.html" target="_blank" rel="external">Kafka入门经典教程</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-kafka/" target="_blank" rel="external">Apache kafka 工作原理介绍</a></li>
<li><a href="http://www.coderli.com/setup-kafka-cluster-step-by-step/" target="_blank" rel="external">事无巨细 Apache Kafka 0.9.0.1 集群环境搭建</a></li>
<li><a href="http://blog.csdn.net/dhtx_wzgl/article/details/46892231" target="_blank" rel="external">kafka集群搭建</a></li>
<li><a href="http://tech.meituan.com/kafka-fs-design-theory.html" target="_blank" rel="external">Kafka文件存储机制那些事</a></li>
<li><a href="http://kaimingwan.com/post/kafka/kafkayuan-li-yi-ji-she-ji-shi-xian-si-xiang" target="_blank" rel="external">kafka原理以及设计实现思想</a></li>
<li><a href="http://www.dexcoder.com/dexcoder/article/2194" target="_blank" rel="external">kafka设计原理介绍</a></li>
<li><a href="http://blog.jobbole.com/99195/" target="_blank" rel="external">Kafka集群操作指南</a></li>
<li><a href="https://www.quora.com/What-is-the-actual-role-of-ZooKeeper-in-Kafka" target="_blank" rel="external">What is the actual role of ZooKeeper in Kafka?</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前一讲我们已经搭建好了 ElasticStack 的核心组件，但是在日常使用中，一般会在 Logstash 和 Elasticsearch 之间加一层 Kafka 用来缓存和控制，这次我们就来看看如何实现这样的功能。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="Kafka" scheme="http://wdxtub.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】叁 监控、安全、报警与通知</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/</id>
    <published>2016-11-19T03:11:08.000Z</published>
    <updated>2016-11-23T14:11:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们已经把 ELK 和 Kafka 组合成了一个比较稳定的系统，这次我们来看看，如何通过 X-Pack 插件包来完成日常运维监控的各项任务。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.22: 完成初稿</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>完成 X-Pack 的安装</li>
<li>了解 Security, Alerting, Monitoring, Reporting 几大组件的功能</li>
<li>自己编写一个报警规则</li>
<li>自己生成一个数据报表</li>
</ol>
<h2 id="X-Pack-简介与安装"><a href="#X-Pack-简介与安装" class="headerlink" title="X-Pack 简介与安装"></a>X-Pack 简介与安装</h2><p>在 Elasticsearch 2.4 时代，如果想对其进行监控和管理，除了五花八门的开源解决方案外，还可以使用 elastic 官方的配套插件。但是从前的名字比较乱，从 Shield 到 Watcher 再到 Marvel，还要一个一个安装配置。</p>
<p>不过在 ElasticStack 5.0 时代，所有的功能得到了统一，统称为 X-Pack，包含：</p>
<ul>
<li>安全：用户权限管理</li>
<li>警报：自动报警</li>
<li>监控：监控 Elasticsearch 集群状态</li>
<li>报告：发送报表、导出数据</li>
<li>图表：可视化数据</li>
</ul>
<p>这些功能基本上涵盖了日常应用的方方面面，接下来我们就来简单了解一下各项功能。不过开始之前，我们先把 X-Pack 装好。</p>
<p>安装需要先停止 Kibana 和 Elasticsearch，这个时候就体现出 Kafka 的优势了：我们可以对 Elasticsearch 进行修改，因为缓存到了 Kafka<br>，所以不必担心日志服务停止。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># 为 Elasticsearch 安装 X-Pack</span></div><div class="line">bin/elasticsearch-plugin install x-pack</div><div class="line"><span class="comment"># 启动 Elasticsearch</span></div><div class="line">bin/elasticsearch</div><div class="line"></div><div class="line"><span class="comment"># 为 Kibana 安装 X-Pack</span></div><div class="line">bin/kibana-plugin install x-pack</div><div class="line"><span class="comment"># 启动 Kibana</span></div><div class="line">bin/kibana</div></pre></td></tr></table></figure>
<p>命令完成之后，安装就算完成了。这里需要额外提一点，因为加上了安全认证，所以原先我们的 Logstash 脚本就不能用了，初始的用户名为 <code>user</code> 和密码为 <code>changeme</code>，对应的配置文件需要更新为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 文件名：kafka-to-es.conf</div><div class="line">input &#123;</div><div class="line">  kafka &#123;</div><div class="line">    bootstrap_servers =&gt; &quot;localhost:9092&quot;</div><div class="line">    topics =&gt; [&quot;logs&quot;]</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">output &#123;</div><div class="line">  # for debugging</div><div class="line">  stdout &#123;</div><div class="line">     codec =&gt; rubydebug</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  elasticsearch &#123; </div><div class="line">    hosts =&gt; &quot;localhost:9200&quot;</div><div class="line">    index =&gt; &quot;system-log&quot;</div><div class="line">    # 用户名和密码如果变更需要更改</div><div class="line">    user =&gt; &quot;elastic&quot;</div><div class="line">    password =&gt; &quot;changeme&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="安全-Security"><a href="#安全-Security" class="headerlink" title="安全 Security"></a>安全 Security</h2><p>再次打开之前的 Kibana 页面，就会发现我们需要登录了：</p>
<p><img src="/images/14797879133833.jpg" alt=""></p>
<p>输入一开始预置的用户名与密码(elastic:changeme)，就可以进入 Kibana 了。然后我们在 Management 面板中可以看到一个新的 Elasticsearch 的栏目，可以在这里进行用户和角色的定制。</p>
<p><img src="/images/14797883023338.jpg" alt=""></p>
<p>这里我们暂时使用默认的帐号和角色进行操作，更多关于安全方面的问题可以参考下面的链接：</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/x-pack/current/encrypting-communications.html" target="_blank" rel="external">通讯加密</a></li>
<li><a href="https://www.elastic.co/guide/en/x-pack/current/ip-filtering.html" target="_blank" rel="external">IP 过滤</a></li>
</ul>
<h2 id="监控-Monitoring"><a href="#监控-Monitoring" class="headerlink" title="监控 Monitoring"></a>监控 Monitoring</h2><p>点击左侧的 Monitoring 面板，便可以清楚查阅 Elasticsearch 和 Kibana 的状态。</p>
<p><img src="/images/14797888163112.jpg" alt=""></p>
<p>点击进入 Overview，便可以清晰了解整体的使用状况：</p>
<p><img src="/images/14797951637038.jpg" alt=""></p>
<p>而在 Indices 分页中点击具体的索引，便可以看到索引的详细：</p>
<p><img src="/images/14797952970767.jpg" alt=""></p>
<p>监控功能在遇到问题的时候进行问题查找和确定非常有用，也可以结合报警和报告功能实现自动化通知。</p>
<h2 id="报警-Alerting"><a href="#报警-Alerting" class="headerlink" title="报警 Alerting"></a>报警 Alerting</h2><p>Elasticsearch 中报警功能的实现目前还不算特别智能，这里我们只简单了解一下其工作机制，具体在需要的时候可以根据文档来进行设置。</p>
<p>简单来说，我们需要自己设定触发条件，并指定条件触发之后的动作。一个实际的例子就是，如果发现近十分钟内某个接口一直返回 503 错误，那么就发送邮件通知。分解一下，一个可能的逻辑是：</p>
<ol>
<li>Trigger: 每十分钟执行一次</li>
<li>Input: 对某个 index 进行检索，查询日志中状态为 error 的条目</li>
<li>Condition: 如果 error 的次数超过 5 次，则认为触发了条件</li>
<li>Transform: 触发之后会再次进行检索，检索的结果可以被之后的动作访问</li>
<li>Actions: 执行具体的操作，可以是通知第三方系统或发送邮件等</li>
</ol>
<p>上面的套路对应到配置文件就是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">PUT _xpack/watcher/watch/log_errors</div><div class="line">&#123;</div><div class="line">  &quot;metadata&quot; : &#123; </div><div class="line">    &quot;color&quot; : &quot;red&quot;</div><div class="line">  &#125;,</div><div class="line">  &quot;trigger&quot; : &#123; </div><div class="line">    &quot;schedule&quot; : &#123;</div><div class="line">      &quot;interval&quot; : &quot;5m&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;,</div><div class="line">  &quot;input&quot; : &#123; </div><div class="line">    &quot;search&quot; : &#123;</div><div class="line">      &quot;request&quot; : &#123;</div><div class="line">        &quot;indices&quot; : &quot;log-events&quot;,</div><div class="line">        &quot;body&quot; : &#123;</div><div class="line">          &quot;size&quot; : 0,</div><div class="line">          &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;status&quot; : &quot;error&quot; &#125; &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;,</div><div class="line">  &quot;condition&quot; : &#123; </div><div class="line">    &quot;compare&quot; : &#123; &quot;ctx.payload.hits.total&quot; : &#123; &quot;gt&quot; : 5 &#125;&#125;</div><div class="line">  &#125;,</div><div class="line">  &quot;transform&quot; : &#123; </div><div class="line">    &quot;search&quot; : &#123;</div><div class="line">        &quot;request&quot; : &#123;</div><div class="line">          &quot;indices&quot; : &quot;log-events&quot;,</div><div class="line">          &quot;body&quot; : &#123;</div><div class="line">            &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;status&quot; : &quot;error&quot; &#125; &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;,</div><div class="line">  &quot;actions&quot; : &#123; </div><div class="line">    &quot;my_webhook&quot; : &#123;</div><div class="line">      &quot;webhook&quot; : &#123;</div><div class="line">        &quot;method&quot; : &quot;POST&quot;,</div><div class="line">        &quot;host&quot; : &quot;mylisteninghost&quot;,</div><div class="line">        &quot;port&quot; : 9200,</div><div class="line">        &quot;path&quot; : &quot;/&#123;&#123;watch_id&#125;&#125;&quot;,</div><div class="line">        &quot;body&quot; : &quot;Encountered &#123;&#123;ctx.payload.hits.total&#125;&#125; errors&quot;</div><div class="line">      &#125;</div><div class="line">    &#125;,</div><div class="line">    &quot;email_administrator&quot; : &#123;</div><div class="line">      &quot;email&quot; : &#123;</div><div class="line">        &quot;to&quot; : &quot;sys.admino@host.domain&quot;,</div><div class="line">        &quot;subject&quot; : &quot;Encountered &#123;&#123;ctx.payload.hits.total&#125;&#125; errors&quot;,</div><div class="line">        &quot;body&quot; : &quot;Too many error in the system, see attached data&quot;,</div><div class="line">        &quot;attachments&quot; : &#123;</div><div class="line">          &quot;attached_data&quot; : &#123;</div><div class="line">            &quot;data&quot; : &#123;</div><div class="line">              &quot;format&quot; : &quot;json&quot;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;,</div><div class="line">        &quot;priority&quot; : &quot;high&quot;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>以上也可以在 Dev Tools 中的面板中执行试试看。</p>
<h2 id="报告-Reporting"><a href="#报告-Reporting" class="headerlink" title="报告 Reporting"></a>报告 Reporting</h2><p>简单来说，这个功能就是一个输出搜索结果和图表的按钮。我们进入 Dashboard 页面，保存当前的图表后，点击右上角的 Reporting 按钮，就会出现一个下载按钮：</p>
<p><img src="/images/14797974158340.jpg" alt=""></p>
<p>点击之后我们会发现并不能够直接下载，因为这个按钮只是给系统发送了一个生成报表的请求，具体的文件我们需要在 Management 面板的 Kibana/Reporting 部分查看：</p>
<p><img src="/images/14797975688905.jpg" alt=""></p>
<p>除了手动生成，我们也可以设置自动生成（使用上面图片中的 Generation URL）并通过给出的 api 来进行下载，具体可以参照 <a href="https://www.elastic.co/guide/en/x-pack/current/automating-report-generation.html" target="_blank" rel="external">Automating Report Generation</a></p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>阅读 <a href="https://www.elastic.co/guide/en/x-pack/current/xpack-settings.html" target="_blank" rel="external">X-Pack Settings</a> 来了解各种设置</li>
<li>阅读 <a href="https://www.elastic.co/guide/en/x-pack/current/xpack-api.html" target="_blank" rel="external">X-Pack APIs</a> 来了解相关 API，以实现自动化设置</li>
<li>阅读 <a href="https://www.elastic.co/guide/en/x-pack/current/xpack-limitations.html" target="_blank" rel="external">Limitaions</a> 来了解 X-Pack 的限制</li>
<li>自己设定一个报警条件，并在触发之后自动发送邮件给自己</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>经历了前两节辛苦的环境搭建，本节内容还是比较轻松的，把环境基本搭建完成之后，就可以真刀真枪做一些实际的项目了。下一讲我们会学习从单机到集群的迁移操作和需要注意的地方，为扩展系统做好准备。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面我们已经把 ELK 和 Kafka 组合成了一个比较稳定的系统，这次我们来看看，如何通过 X-Pack 插件包来完成日常运维监控的各项任务。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="监控" scheme="http://wdxtub.com/tags/%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】肆 从单机到集群</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/</id>
    <published>2016-11-19T03:11:07.000Z</published>
    <updated>2016-11-24T13:33:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前所做的工作都是在单机上运行，随着业务数据量的增长和维持服务高可用的需要，基本都得进行从单机到集群的变迁。从单机到集群主要需要克服的问题和多人合作一样，就是沟通问题。如果需要向全球提供服务，全球部署带来的高延迟和不稳定同样也是一个难题。本文会简单介绍一些解决这些问题的思路。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.23: 完成初稿</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>完成 Elasticsearch 集群的搭建</li>
<li>完成 Kafka 集群的搭建</li>
</ol>
<h2 id="Elasticsearch-集群配置"><a href="#Elasticsearch-集群配置" class="headerlink" title="Elasticsearch 集群配置"></a>Elasticsearch 集群配置</h2><p>开发人员花了很多心思，尽量让一台机上运行和集群上运行的体验一致，具体说，Elasticsearch 在底层主要做的工作有：</p>
<ul>
<li>根据配置进行数据分片(sharding)，并且保存在一个或者多个节点中</li>
<li>将分片均匀分配到各个节点，对索引和搜索做负载均衡</li>
<li>分片冗余，提高容错性</li>
<li>将集群中任意一个节点上的请求路由到相应数据所在的节点</li>
<li>增加或者移除节点时无缝迁移数据</li>
</ul>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>虽然 Elasticsearch 的安装比较简单，不过我还是写了一个安装脚本，可以在<a href="https://github.com/wdxtub/wdxtools/tree/master/linux-script" target="_blank" rel="external">这里</a>查看，具体来说其实就两步，下载和解压，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.4.0/elasticsearch-2.4.0.tar.gz</div><div class="line">tar -xvzf elasticsearch-2.4.0.tar.gz</div></pre></td></tr></table></figure>
<p>分别在集群中每台机器中完成安装即可，具体的启动也非常简单，如果要在前台，直接 <code>./bin/elasticsearch</code> 即可，如果要放到后台，则使用 <code>nohup ./bin/elasticsearch &amp;</code>。</p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>配置文件位于 <code>config</code> 文件夹中，其中 <code>elasticsearch.yml</code> 是 elasticsearch 的配置，而 <code>logging.yml</code> 是输出日志相关的设置。配置文件的内容有很多，不过因为默认值基本都够用了，所以我们只需要配置很少的内容。假设现在有两个节点，内部 IP 地址分别为 <code>A: 10.1.1.0</code> 和 <code>B: 10.1.1.1</code>，那么配置为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 节点 A</div><div class="line">cluster.name: wdxtubes</div><div class="line">node.name: &quot;es01&quot;</div><div class="line">bootstrap.mlockall: true</div><div class="line">network.host: 10.1.1.0</div><div class="line">network.publish_host: 10.1.1.0</div><div class="line">discovery.zen.ping.unicast.hosts: [&quot;10.1.1.1&quot;]</div><div class="line">discovery.zen.fd.ping_timeout: 120s</div><div class="line">discovery.zen.fd.ping_retries: 6</div><div class="line">discovery.zen.fd.ping_interval: 30s</div><div class="line"></div><div class="line"></div><div class="line"># 节点 B</div><div class="line">cluster.name: wdxtubes</div><div class="line">node.name: &quot;es02&quot;</div><div class="line">bootstrap.mlockall: true</div><div class="line">network.host: 10.1.1.1</div><div class="line">network.publish_host: 10.1.1.1</div><div class="line">discovery.zen.ping.unicast.hosts: [&quot;10.1.1.0&quot;]</div><div class="line">discovery.zen.fd.ping_timeout: 120s</div><div class="line">discovery.zen.fd.ping_retries: 6</div><div class="line">discovery.zen.fd.ping_interval: 30s</div></pre></td></tr></table></figure>
<p>这里需要注意的是我们采用单播的方式来进行集群中机器的查找，因为 elasticsearch 已经尽量帮我们做好了集群相关的工作，只要保证 <code>cluster.name</code> 一致，就可以自动发现。另外，我们调大了超时的间隔和互相 ping 发送的频率以及重试次数，防止某台机器在 Full GC 的时候因未能及时响应而造成的连锁反应（后面会详细说明）</p>
<p>多说一句，机器配置的时候，最好确保两台机器可以互相 ping 通，并开放所有端口的内部访问（如果是用云主机的话，尤其需要注意这一点）</p>
<p>如果需要扩展的话，只需要保证 <code>cluster.name</code> 一致即可，比如说现在新加入一台 <code>C: 10.1.1.2</code>，那么配置可以这么写</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 节点 C</div><div class="line">cluster.name: wdxtubes</div><div class="line">node.name: &quot;es03&quot;</div><div class="line">bootstrap.mlockall: true</div><div class="line">network.host: 10.1.1.2</div><div class="line">network.publish_host: 10.1.1.2</div><div class="line">discovery.zen.ping.unicast.hosts: [&quot;10.1.1.0&quot;]</div><div class="line">discovery.zen.fd.ping_timeout: 120s</div><div class="line">discovery.zen.fd.ping_retries: 6</div><div class="line">discovery.zen.fd.ping_interval: 30s</div></pre></td></tr></table></figure>
<p>这里 <code>discovery.zen.ping.unicast.hosts</code> 中只需要填写原有集群中任意一台机器的地址即可。</p>
<p>然后我们可以在集群中的机器上使用 <code>curl http://10.1.1.0:9200/_cluster/health</code> 来查看集群状态。比如：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"cluster_name"</span>:<span class="string">"wdxtub-es"</span>,</div><div class="line">    <span class="attr">"status"</span>:<span class="string">"green"</span>,</div><div class="line">    <span class="attr">"timed_out"</span>:<span class="literal">false</span>,</div><div class="line">    <span class="attr">"number_of_nodes"</span>:<span class="number">2</span>,</div><div class="line">    <span class="attr">"number_of_data_nodes"</span>:<span class="number">2</span>,</div><div class="line">    <span class="attr">"active_primary_shards"</span>:<span class="number">821</span>,</div><div class="line">    <span class="attr">"active_shards"</span>:<span class="number">1642</span>,</div><div class="line">    <span class="attr">"relocating_shards"</span>:<span class="number">0</span>,</div><div class="line">    <span class="attr">"initializing_shards"</span>:<span class="number">0</span>,</div><div class="line">    <span class="attr">"unassigned_shards"</span>:<span class="number">0</span>,</div><div class="line">    <span class="attr">"delayed_unassigned_shards"</span>:<span class="number">0</span>,</div><div class="line">    <span class="attr">"number_of_pending_tasks"</span>:<span class="number">0</span>,</div><div class="line">    <span class="attr">"number_of_in_flight_fetch"</span>:<span class="number">0</span>,</div><div class="line">    <span class="attr">"task_max_waiting_in_queue_millis"</span>:<span class="number">0</span>,</div><div class="line">    <span class="attr">"active_shards_percent_as_number"</span>:<span class="number">100.0</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果状态是 green，那就没有问题啦。下面我们会结合不同的实例进行介绍</p>
<h3 id="重启"><a href="#重启" class="headerlink" title="重启"></a>重启</h3><p>Elasticsearch 的重启是一个非常需要按规矩操作的过程，否则会带来一系列的意想不到的问题，所以一定要按照官方建议的步骤来进行。</p>
<p>首先，因为 Elasticsearch 自带的高可用机制，一旦一个节点下线，就会在集群内部进行数据的重分配，会带来很多不必要的开销，所以需要先关闭，关闭方法是给集群发送一个请求，这个请求可以动态修改集群的设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">PUT /_cluster/settings</div><div class="line">&#123;</div><div class="line">  &quot;persistent&quot;: &#123;</div><div class="line">    &quot;cluster.routing.allocation.enable&quot;: &quot;none&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>而在重启之后需要进行数据恢复，如果停止索引并发送一个同步刷新请求，这个过程就会快很多，需要注意的是，如果此时有任何正在进行的索引操作，这个 flush 操作会失败，因此必要时我们可以重试多次，这是安全的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">POST /_flush/synced</div></pre></td></tr></table></figure>
<p>现在我们可以停止集群中的各个节点，完成重启或升级的操作。具体单台机器的操作可以看<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/rolling-upgrades.html#upgrade-node" target="_blank" rel="external">这里</a></p>
<p>完成之后，我们最好先启动那些 <code>node.master</code> 设置为 true 的节点（这也是默认设置），等到集群选举出了 master 节点，就可以继续添加数据节点了（即那些 <code>node.master</code> 为 false 且 <code>node.data</code> 为 true 的），这里我们可以用以下方式进行监控</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">GET _cat/health</div><div class="line"></div><div class="line">GET _cat/nodes</div></pre></td></tr></table></figure>
<p>每个节点加入集群之后，就会开始恢复本地保存的首要分片，一开始 <code>_cat/health</code> 查询的结果是 red，之后会变成 yellow，也就意味着所有的首要分片已经恢复了，但是其他的复制分片还没有恢复，因为我们一开始已经设置不恢复复制分片。</p>
<p>最后一步，我们需要重新开启集群的数据重分配，以保证集群的高可用性，操作也很简单：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">PUT /_cluster/settings</div><div class="line">&#123;</div><div class="line">  &quot;persistent&quot;: &#123;</div><div class="line">    &quot;cluster.routing.allocation.enable&quot;: &quot;all&quot;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当使用 <code>_cat/health</code> 的结果为 green 时，则重启和恢复顺利完成。</p>
<h3 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h3><p>无论是 Elasticsearch 官方还是社区，有很多插件可以完成监控的任务，但是本文只介绍默认的 API，主要是 <code>_cat</code> 和 <code>_cluster</code> 这两个接口，具体的文档可以在 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html" target="_blank" rel="external">cat API</a> 和 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html" target="_blank" rel="external">cluster API</a> 中查看，这里简要介绍一下。</p>
<p>对于 <code>_cat</code> 接口，在请求后面加上 <code>?v</code> 就会输出详细信息，例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">wdxtub:~$ curl 10.1.1.10:9200/_cat/master?v</div><div class="line">id                     host      ip        node   </div><div class="line">AoVFmiU4Q2SAHNVcMGPsWQ 10.1.1.11 10.1.1.11 node-2</div></pre></td></tr></table></figure>
<p>如果对于字段的名字有疑问，可以使用 <code>?help</code>，例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">wdxtub:~$ curl 10.1.1.10:9200/_cat/master?<span class="built_in">help</span></div><div class="line">id   |   | node id    </div><div class="line">host | h | host name  </div><div class="line">ip   |   | ip address </div><div class="line">node | n | node name</div></pre></td></tr></table></figure>
<p>如果只想要查看指定字段，可以利用 <code>?h=</code> 来进行指定，例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">wdxtub:~$ curl 10.1.1.10:9200/_cat/nodes?h=ip,port,heapPercent,name</div><div class="line">10.1.1.11 9300 64 node-2 </div><div class="line">10.1.1.10 9300 71 node-1</div></pre></td></tr></table></figure>
<p>对于带数字的输出，可以利用管道来进行排序，比如下面的命令就可以按照索引大小来进行排序（这里的 <code>-rnk8</code> 指的是按照第八列排序）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">wdxtub:~$ curl 10.1.1.10:9200/_cat/indices?bytes=b | sort -rnk8</div><div class="line">green open slog-2016-09-11   5 1 9729152      0 11128793222 5564396611 </div><div class="line">green open slog-2016-09-12   5 1 8355880      0  9539380440 4769690220 </div><div class="line">green open slog-2016-09-25   5 1 6720954      0  7415719218 3707859609 </div><div class="line">green open slog-2016-09-19   5 1 5840177      0  6575155002 3287577501 </div><div class="line">green open slog-2016-09-10   5 1 5858916      0  6504251544 3252125772</div></pre></td></tr></table></figure>
<p>其他比较常用的命令如下所示，具体的可以参阅文档，这里不再赘述：</p>
<ul>
<li><code>_cat/count</code> 文档总数</li>
<li><code>_cat/count/[index_name]</code> 某个索引的文档总数</li>
<li><code>_cat/fielddata?v</code> 显示每个节点的字段的堆内存使用量</li>
<li><code>_cat/health?v</code> 节点的健康状况<ul>
<li>可以使用下面的命令来自动检查集群状况</li>
<li><code>while true; do curl localhost:9200/_cat/health; sleep 120; done</code></li>
</ul>
</li>
<li><code>_cat/indices?v</code> 查看每个索引的详细信息，配合管道命令可以有很多应用，比如<ul>
<li>找出所有状态为 yellow 的索引 <code>curl localhost:9200/_cat/indices | grep ^yell</code></li>
<li>排序 <code>curl &#39;localhost:9200/_cat/indices?bytes=b&#39; | sort -rnk8</code></li>
<li>指定列及内存使用状况 <code>curl &#39;localhost:9200/_cat/indices?v&amp;h=i,tm&#39;</code></li>
</ul>
</li>
<li><code>_cat/nodes</code> 展示集群的拓扑结构</li>
<li><code>_cat/pending_tasks?v</code> 显示正在排队的任务</li>
<li><code>_cat/recovery?v</code> 显示分片恢复的过程</li>
<li><code>_cat/thread_pool?v</code> 显示线程池相关信息，有很多信息，可以根据需要进行查询</li>
<li><code>_cat/shards?v</code> 显示分片的相关信息</li>
<li><code>_cat/shards/[index-name]</code> 显示指定索引的分片信息  </li>
</ul>
<p><code>_cluster</code> 的接口的用法和 <code>_cat</code> 类似，这里就不再赘述了。</p>
<h2 id="Kafka-集群配置"><a href="#Kafka-集群配置" class="headerlink" title="Kafka 集群配置"></a>Kafka 集群配置</h2><p>kafka 使用 ZooKeeper 用于管理、协调代理。每个 Kafka 代理通过 Zookeeper 协调其他 Kafka 代理。当 Kafka 系统中新增了代理或某个代理失效时，Zookeeper 服务将通知生产者和消费者。生产者与消费者据此开始与其他代理协调工作。</p>
<h3 id="安装-Java"><a href="#安装-Java" class="headerlink" title="安装 Java"></a>安装 Java</h3><p>先给两台机子安装 Java</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">sudo add-apt-repository -y ppa:webupd8team/java</div><div class="line">sudo apt-get update</div><div class="line">sudo apt-get -y install oracle-java8-installer</div></pre></td></tr></table></figure>
<h3 id="更新-Hosts"><a href="#更新-Hosts" class="headerlink" title="更新 Hosts"></a>更新 Hosts</h3><p>这里用两台机器做例子（理论上最好是 3 台起步，偶数个不是不可以的，但是zookeeper集群是以宕机个数过半才会让整个集群宕机的，所以奇数个集群更佳），分别配置 <code>/etc/hosts</code> 文件为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">127.0.0.1 localhost</div><div class="line">10.1.1.164 bi03</div><div class="line">10.1.1.44 bi02</div></pre></td></tr></table></figure>
<h3 id="修改-Zookeeper-配置文件"><a href="#修改-Zookeeper-配置文件" class="headerlink" title="修改 Zookeeper 配置文件"></a>修改 Zookeeper 配置文件</h3><p>修改 <code>config/zookeeper.properties</code> 为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">dataDir=/data/home/logger/kafka_2.11-0.10.0.0/zookeeper-logs/</div><div class="line">clientPort=2181</div><div class="line"># maxClientCnxns=0</div><div class="line">tickTime=2000</div><div class="line">initLimit=5</div><div class="line">syncLimit=2</div><div class="line"></div><div class="line">server.1=bi03:13645:13646</div><div class="line">server.2=bi02:13645:13646</div></pre></td></tr></table></figure>
<p>参数的意义为：</p>
<ul>
<li>initLimit: zookeeper集群中的包含多台 server，其中一台为 leader，集群中其余的 server 为 follower。initLimit 参数配置初始化连接时，follower 和 leader 之间的最长心跳时间。此时该参数设置为 5，说明时间限制为 5 倍 tickTime，即 <code>5*2000=10000ms=10s</code></li>
<li>syncLimit: 该参数配置 leader 和 follower 之间发送消息，请求和应答的最大时间长度。此时该参数设置为 2，说明时间限制为 2 倍 tickTime，即 4000ms</li>
<li>server.X=A:B:C 其中 X 是一个数字, 表示这是第几号 server。A 是该 server 所在的 IP 地址。B 配置该 server 和集群中的 leader 交换消息所使用的端口。C 配置选举 leader 时所使用的端口。</li>
</ul>
<h3 id="给服务器编号"><a href="#给服务器编号" class="headerlink" title="给服务器编号"></a>给服务器编号</h3><p>在 dataDir 目录下建立一个 myid 文件，分别为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># server.1</div><div class="line">echo 1 &gt; myid</div><div class="line"></div><div class="line"># server.2</div><div class="line">echo 2 &gt; myid</div></pre></td></tr></table></figure>
<h3 id="启动-Zookeeper"><a href="#启动-Zookeeper" class="headerlink" title="启动 Zookeeper"></a>启动 Zookeeper</h3><p>然后在每台机子上启动 zookeeper 服务 </p>
<p><code>bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</code></p>
<p>所有机子的 zookeeper 都启动之前会报错，这都是正常的</p>
<p>如果不想要任何输出</p>
<p><code>nohup bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</code></p>
<h3 id="修改-Kafka-配置文件"><a href="#修改-Kafka-配置文件" class="headerlink" title="修改 Kafka 配置文件"></a>修改 Kafka 配置文件</h3><p>修改 <code>config/server.properties</code>，几个要改的部分是 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># 允许删除 topic</div><div class="line">delete.topic.enable=true</div><div class="line">broker.id=0 # 这里不能重复</div><div class="line">listeners=PLAINTEXT://bi03:13647 # 这里要配置成本机的 host name</div><div class="line"># 这里需要配置成外网能够访问的地址及端口</div><div class="line">advertised.listeners=PLAINTEXT://external.ip:8080</div><div class="line"></div><div class="line">log.dirs=/data/home/logger/kafka_2.11-0.10.0.0/kafka-logs</div><div class="line">num.partitions=2</div><div class="line"></div><div class="line">zookeeper.connect=bi03:2181,bi02:2181</div></pre></td></tr></table></figure>
<h3 id="启动-Kafka"><a href="#启动-Kafka" class="headerlink" title="启动 Kafka"></a>启动 Kafka</h3><p>在每个节点上执行</p>
<p><code>bin/kafka-server-start.sh config/server.properties &amp;</code></p>
<p>如果不想要任何输出</p>
<p><code>nohup bin/kafka-server-start.sh config/server.properties &amp;</code></p>
<h3 id="验证安装"><a href="#验证安装" class="headerlink" title="验证安装"></a>验证安装</h3><p>创建一个 topic</p>
<p><code>bin/kafka-topics.sh --create --zookeeper bi03:2181,bi02:2181 --replication-factor 2 --partitions 1 --topic test</code></p>
<p>查看集群状态</p>
<p><code>bin/kafka-topics.sh --describe --zookeeper bi03:2181,bi02:2181 --topic test</code></p>
<p>生产消息，这里注意要生产到前面设置的监听端口，而不是 zookeeper 的端口</p>
<p><code>bin/kafka-console-producer.sh --broker-list bi03:13647,bi02:13647 --topic test</code></p>
<p>消费消息，这里注意是 zookeeper 的端口，而不是 kafka 的端口</p>
<p><code>bin/kafka-console-consumer.sh --zookeeper bi03:2181,bi02:2181 --from-beginning --topic test</code></p>
<p>显示 topic 列表</p>
<p><code>bin/kafka-topics.sh --zookeeper bi03:2181,bi02:2181 --list</code></p>
<p>删除 topic</p>
<p><code>bin/kafka-topics.sh --zookeeper bi03:2181,bi02:2181 --delete --topic hello</code></p>
<h3 id="其他配置"><a href="#其他配置" class="headerlink" title="其他配置"></a>其他配置</h3><p>Kafka 使用键值对的属性文件格式来进行配置，比如 <code>config/server.properties</code>，具体的值可以从文件中读取，或者在代码中进行指定。最重要的三个属性是：</p>
<ul>
<li><code>broker.id</code>: broker 的编号，不能相同</li>
<li><code>log.dirs</code>: 日志保存的文件夹，默认为 <code>/tmp/kafka-logs</code></li>
<li><code>zookeeper.connect</code>: zookeeper 的 host</li>
</ul>
<p>其他一些我觉得比较有用的属性为</p>
<ul>
<li><code>auto.create.topics.enable</code> 是否允许自动创建 topic，boolean 值，默认为 <code>true</code></li>
<li><code>auto.leader.rebalance.enable</code> 是否允许 leader 进行自动平衡，boolean 值，默认为 <code>true</code></li>
<li><code>background.threads</code> 后台进程数目，int 值，默认为 10 个</li>
<li><code>compression.type</code> 指定 topic 的压缩方式，string 值，可选有<ul>
<li><code>gzip</code>, <code>snappy</code>, <code>lz4</code> 压缩方法</li>
<li><code>uncompressed</code> 不压缩</li>
<li><code>producer</code> 跟随 producer 的压缩方式</li>
</ul>
</li>
<li><code>delete.topic.enable</code> 是否允许删除 topic，boolean 值，默认为 false（主要用于控制 admin 界面中的控制）</li>
<li><code>leader.imbalance.check.interval.seconds</code> 检查是否平衡的时间间隔，long 值，默认为 300</li>
<li><code>leader.imbalance.per.broker.percentage</code> 允许的不平衡的百分比，超出则会进行重平衡，int 值，默认为 10</li>
<li><code>log.flush.interval.messages</code> 攒了多少条消息之后会把数据刷入磁盘，long 值，默认是 9223372036854775807</li>
<li><code>log.flush.interval.ms</code> 每条消息在保存到磁盘中前会在内存中待多久，单位毫秒，long 值，如果不设定，默认使用 <code>log.flush.scheduler.interval.ms</code>，也就是 9223372036854775807</li>
</ul>
<p>更多的配置可以参考<a href="http://kafka.apache.org/documentation.html#brokerconfigs" target="_blank" rel="external">这里</a>，以上的配置均针对 broker，因为目前我只用 broker 的部分</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>尝试主动关闭集群中某一台机器，看看整个集群是如何保证高可用的</li>
<li>Zookeeper 在 Kafka 集群中起到了什么作用？尝试使用单独的 zookeeper 而不是 Kafka 自带的</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本节中的配置需要多台机器或者是一台机器开多个实例进行测试，对于没有条件的同学来说可能会比较麻烦，不过现在 AWS 有一年的免费使用时间（当然只有最基础的机器），所以大家其实可以申请一个 AWS 帐号，在上面进行学习和测试（而且速度也会更快）。接下来的几篇文章会具体介绍 Logstash, Elasticsearch 和 Kibana 的相关使用。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前所做的工作都是在单机上运行，随着业务数据量的增长和维持服务高可用的需要，基本都得进行从单机到集群的变迁。从单机到集群主要需要克服的问题和多人合作一样，就是沟通问题。如果需要向全球提供服务，全球部署带来的高延迟和不稳定同样也是一个难题。本文会简单介绍一些解决这些问题的思路。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="集群" scheme="http://wdxtub.com/tags/%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】伍 Logstash 技巧指南</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/</id>
    <published>2016-11-19T03:11:06.000Z</published>
    <updated>2016-11-24T13:33:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们已经把系统搭建完成，但是具体的应用都比较简单。这一次我们来详细了解一下 Logstash，就可以处理各种各样的输入源及格式了。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.24: 完成初稿（插件详情之后会补充）</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>了解 Logstash 的工作流程</li>
<li>熟悉 Input 阶段的常用插件</li>
<li>熟悉 Filter 阶段的常用插件</li>
<li>熟悉 Output 阶段的常用插件</li>
<li>了解如何监控 Logstash 运行状况</li>
</ol>
<h2 id="Logstash-简介"><a href="#Logstash-简介" class="headerlink" title="Logstash 简介"></a>Logstash 简介</h2><p>Logstash 最打动我的是整个社区的风格，而这个风格和作者 Jordan Sissel 本人分不开，虽然现在最初的 Google groups 已经搬迁到 elastic 官方的论坛，但是还是能看到这么一句话：</p>
<blockquote>
<p>Remember: if a new user has a bad time, it’s a bug in logstash</p>
</blockquote>
<p>这是什么精神，这是白求恩精神，做一个高尚的人，一个纯粹的人，一个有道德的人，一个脱离了低级趣味的人，一个有益于人民的人。嗯，就是这样。</p>
<p>简单的入门可以参考我的 <a href="./2016/07/24/logstash-guide/">Logstash 入门指南</a>，这里重点介绍一些中高级用法。</p>
<p>Logstash 支持的数据值类型有 bool, string, number, array 和 hash，和 Redis 一样，支持得不多，但是完全够用。支持的条件判断和表达式则比较丰富，如：</p>
<ul>
<li>基本条件判断 <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code></li>
<li><code>=~</code> 匹配正则, <code>!~</code> 不匹配正则</li>
<li><code>in</code> 包含, <code>not in</code> 不包含</li>
<li><code>and</code> 与, <code>or</code> 或, <code>nand</code> 非与, <code>xor</code> 非或</li>
<li><code>()</code> 复合表达式, <code>!()</code> 表达式结果取反</li>
</ul>
<p>比方说我们有一个字段是 <code>type</code>，我们想要过滤一下做指定操作的话，可以</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">if &quot;good&quot; in [type] &#123;</div><div class="line">    // do something</div><div class="line">&#125; else &#123;</div><div class="line">    // do something</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从 Logstash 5.0 开始，可以在 <code>$LS_HOME/config/logstash.yml</code> 文件进行所有的命令行参数配置，例如</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><div class="line"><span class="attr">pipeline:</span></div><div class="line"><span class="attr">    workers:</span> <span class="number">24</span></div><div class="line"><span class="attr">    batch:</span></div><div class="line"><span class="attr">        size:</span> <span class="number">125</span></div><div class="line"><span class="attr">        delay:</span> <span class="number">5</span></div></pre></td></tr></table></figure>
<h2 id="插件-Plugin"><a href="#插件-Plugin" class="headerlink" title="插件 Plugin"></a>插件 Plugin</h2><p>使用之前我们先要安装一下 ruby，命令为 <code>sudo apt install ruby</code>，然后我们可以运行 <code>logstash-plugin list</code> 来看看本机中目前有多少插件可以用，这里会有一个警告，不过查阅 github issue 中说没有问题，那就暂时忽略。插件很多，这里就不一一介绍，简单贴一下 help 文档应该就一目了然了：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">dawang@dawang-Parallels-Virtual-Platform:~$ logstash-plugin -hUsage:    bin/logstash-plugin [OPTIONS] SUBCOMMAND [ARG] ...Parameters:    SUBCOMMAND                    subcommand    [ARG] ...                     subcommand argumentsSubcommands:    install                       Install a plugin    uninstall                     Uninstall a plugin    update                        Update a plugin    pack                          Package currently installed plugins    unpack                        Unpack packaged plugins    list                          List all installed pluginsOptions:    -h, --help                    <span class="built_in">print</span> <span class="built_in">help</span></div></pre></td></tr></table></figure>
<h2 id="输入-Input"><a href="#输入-Input" class="headerlink" title="输入 Input"></a>输入 Input</h2><p>我们的配置文件中一定需要有一个 input，如果没有的话，就会默认使用 <code>input/stdin</code>。这里只记录一些最常用和最基本的插件，更多的插件可以参考官方文档或参考链接中的教程。</p>
<ul>
<li>读取文件 File</li>
<li>读取 Syslog 数据</li>
<li>编码插件 Codec: JSON</li>
</ul>
<h2 id="过滤-Filter"><a href="#过滤-Filter" class="headerlink" title="过滤 Filter"></a>过滤 Filter</h2><p>这部分是 Logstash 最具特色和扩展性的部分（但并不一定是必须的），这里只记录一些最常用和最基本的插件，更多的插件可以参考官方文档或参考链接中的教程。</p>
<ul>
<li>时间处理 Date，包括 <code>ISO8601</code>, <code>UNIX</code>, <code>UNIX_MS</code>, <code>TAI64N</code> 和 <code>Joda-Time</code></li>
<li>正则捕获 Grok，这个插件可以摆弄出非常多的黑魔法，可以考虑重点应用，记得使用 Grok Debugger 来调试 grok 表达式</li>
<li>GeoIP 地址查询，用于统计区域活着可视化地图</li>
<li>Mutate 数据修改，可以用来转换类型、处理字符串以及处理字段（重命名、更新、替换等）</li>
<li>split 拆分事件</li>
</ul>
<h2 id="输出-Output"><a href="#输出-Output" class="headerlink" title="输出 Output"></a>输出 Output</h2><p>我们的配置文件中一定需要有一个 input，如果没有的话，就会默认使用 <code>output/stdout</code>。这里只记录一些最常用和最基本的插件，更多的插件可以参考官方文档或参考链接中的教程。</p>
<ul>
<li>保存到 Elasticsearch 中，注意几个参数： <code>flush_size</code> 是攒够这个大小才写入，<code>idle_flush_time</code> 是隔这么多时间写入一次，这俩都会影响 ES 的写入性能</li>
<li>发邮件 Email</li>
<li>调用命令执行 exec，比方说可以发短信，最好只用于少量的信息处理场景</li>
<li>保存成文件 file</li>
<li>发送到 HDFS 可以使用 <code>hadoop_webhdfs</code> 插件</li>
</ul>
<h2 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h2><p>从 Logstash 5.0 开始提供了监控 API，就不再像以前那样比较黑盒了，具体有</p>
<ul>
<li>events <code>curl -s localhost:9600/_node/stats/events?pretty=true</code></li>
<li>jvm <code>curl -s localhost:9600/_node/stats/jvm?pretty=true</code></li>
<li>process <code>curl -s localhost:9600/_node/stats/process?pretty=true</code></li>
<li>热线程统计 <code>curl -s localhost:9600/_node/stats/hot_threads?human=true</code></li>
</ul>
<h2 id="疑难杂症"><a href="#疑难杂症" class="headerlink" title="疑难杂症"></a>疑难杂症</h2><p>Logstash 的字段中不能出现 <code>.</code> 这个问题，可以通过 <code>de_dot</code> 这个插件解决，安装命令 <code>bin/logstash-plugin install logstash-filter-de_dot</code>，然后在 logstash 的配置文件中添加如下一段代码即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">filter &#123;</div><div class="line">  de_dot &#123; &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>尝试利用 Rsyslog 的方式向 Logstash 提供日志数据</li>
<li>尝试 Filter 插件把系统日志细化成更多的字段</li>
<li>试着执行一个比较长时间的任务，用监控 API 来看看 Logstash 运行的状况</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体来说，Logstash 的使用还是比较简单的，只要配置好规则，基本都能够保证正常执行。对于日志收集来说，最大的难点在于日志形式的不规范，如果整个系统各个模块的日志拥有统一的规范的话，收集起来会轻松不少。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面我们已经把系统搭建完成，但是具体的应用都比较简单。这一次我们来详细了解一下 Logstash，就可以处理各种各样的输入源及格式了。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="Logstash" scheme="http://wdxtub.com/tags/Logstash/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】陆 Elasticsearch 技巧指南</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/</id>
    <published>2016-11-19T03:11:05.000Z</published>
    <updated>2016-11-24T13:33:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们已经把系统搭建完成，但是具体的应用都比较简单。这一次我们来详细了解一下 Elasticsearch，尤其是如何利用其内置的各种强大功能来完成我们的需求。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.24: 完成初稿（部分内容后续会陆续完善）</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>了解 Elasticsearch 的底层存储</li>
<li>熟悉 HTTP 接口和 RESTful API</li>
<li>了解 Elasticsearch 的调优与运维</li>
</ol>
<h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>这部分内容虽然不一定对工程有立竿见影的帮助，但是知其然还知其所以然，才是高手的做事风格。那么问题来了</p>
<blockquote>
<p>写入的数据是如何变成 Elasticsearch 里可以被检索和聚合的索引内容的？</p>
</blockquote>
<p>关键在于『倒排索引』，新收到的数据会被写入到内存的 buffer 中，然后在一定的时间间隔后刷到磁盘中，成为一个新的 segment，然后另外使用一个 commit 文件来记录所有的 segment，数据只有在成为 segment 之后才能被检索。默认的从 buffer 到 segment 的时间间隔是 1 秒，基本已经是『实时』了，如果需要更改，也可以调用 <code>/_refresh</code> 接口。不过很多时候我们不需要这么『实时』，所以可以加大这个时间间隔，以获得更快的写入性能。导入历史数据时甚至可以关闭，导入完成再重新开启。</p>
<p>为了保证数据从 buffer 到 segment 的一致性，Elasticsearch 还会有一个名为 Translog 的记录，至于  Translog 的一致性则是通过定期保存到磁盘中来实现的</p>
<p>前面说过 Lucene 会不断开新文件，这样磁盘上就会有一堆小文件，所以 ES 会在后台把这些零散的 segment 做数据归并，归并完成后就可以把小的 segment 删掉，也就减少了 segment 的数量了。为了不影响 IO 和 CPU，会对归并线程做一定的限制，我们可以根据硬件的不同来调整 <code>indices.store.throttle.max_bytes_per_sec</code> 来提高性能。与此同时，我们也有不同的归并策略，不过总体来说就是让我们加大 flush 的间隔，尽量让每次新生成的 segment 本身就比较大。</p>
<p>ES 的分布式处理主要是通过 sharding 机制，也会保留副本进行冗余备份，具体采用的是 gossip 协议，配置也不算复杂，这里就不赘述，如果有机会专门写一篇实例教程。</p>
<h2 id="操作管理"><a href="#操作管理" class="headerlink" title="操作管理"></a>操作管理</h2><p>配置文件在 <code>/etc/elasticsearch/elasticsearch.yml</code>，重启命令 <code>sudo service elasticsearch restart</code></p>
<h2 id="增删改查"><a href="#增删改查" class="headerlink" title="增删改查"></a>增删改查</h2><p>ES 虽然不是数据库，不过其特性决定了，这就是一个很好的 NoSQL 数据库嘛，因为 ELK stack 的缘故，写入由 Logstash 负责，查询由 Kibana 负责，不过修改和删除就有些无能为力了（毕竟为什么要简单修改和删除日志？），可是修改和删除是数据库必须的功能，好在 ES 提供了 RESTful 接口来处理 JSON 请求，最简单的用 <code>curl</code> 就可以完成各类操作。这里推荐一个 Chrome 的插件 Postman，可以很方便进行各类测试。具体如何发送请求请参考文档，这里不赘述了。</p>
<h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>前面的增删改查针对的是单条记录，ES 中更重要的是搜索。这里回顾一下：刚写入的数据，可以通过 translog 立刻获取；但是直到其成为一个 segment 之后，才能被搜索到</p>
<p>可以利用 <code>/_search?q=</code> 这种 querystring 的简单语法，或者发送完整的 json 来进行查询。具体可以依据版本查阅文档，这里不赘述。</p>
<p>另外，聚合、管道聚合</p>
<h2 id="其他功能"><a href="#其他功能" class="headerlink" title="其他功能"></a>其他功能</h2><p>Elasticsearch 目前已经可以和 Hadoop, HDFS, Spark Streaming 等大数据工具连接使用。如果需要配置权限，可以使用 Elastic 官方的 Shield，如果想用开源的话，可以使用 <a href="https://github.com/floragunncom/search-guard" target="_blank" rel="external">search-guard</a>，这样不同的用户可以访问不同的索引，达到权限控制。</p>
<p>监控集群健康状态也可以通过接口访问，比如 <code>curl -XGET 127.0.0.1:9200/_cluster/health?pretty</code>，更多监控信息请参阅文档，这里不赘述。</p>
<p>需要提的一点就是 GC 是非常影响性能的，所以我们来简单介绍一下 JVM 的机制。启动 JVM 虚拟机的时候，会分配固定大小的内存块，也就是堆 heap。堆又分成两组，Young 组是为新实例化的对象所分配的空间，比较小，一般来说几百 MB，Young 组内又分为两个 survivor 空间。Young 空间满了后，就垃圾回收一次，还存活的对象放到幸存空间中，失效的就被移除。Old 组就是保存那些重启存活且一段时间不会变化的内容，对于 ES 来说可能有 30 GB 内存是 Old 组，同样，满了之后就垃圾回收。</p>
<p>垃圾回收的时候，JVM 采用的是 STW(Stop The World) 机制，Young 组比较小还好，但是 Old 组可能需要几秒十几秒，那就是服务器无响应啊！所以我们必须非常关注 GC 性能。</p>
<p>如果 ES 集群中经常有很耗时的 GC，说明内存不足，如果影响集群之间 ping 的话，就会退出集群，然后因为分片缘故导致更大的影响。我们可以在节点状态中的 <code>jvm</code> 部分查看对应的数值，最重要是 <code>heap_used_percent</code>，如果大于 75，那么就要垃圾回收了，如果长期在 75 以上，那就是内存不足。</p>
<p>注：节点状态可以通过 <code>curl -XGET http://127.0.0.1:9200/_nodes/stats</code> 查看，下面是一个例子（省略了部分内容）：</p>
<figure class="highlight"><table><tr><td class="code"><pre><div class="line">&#123;</div><div class="line">  "cluster_name" : "wdxtub",</div><div class="line">  "nodes" : &#123;</div><div class="line">    "M-OzSwFBTc6uU8ndWU1SFw" : &#123;</div><div class="line">      "timestamp" : 1470310258934,</div><div class="line">      "name" : "Kleinstocks",</div><div class="line">      "transport_address" : "127.0.0.1:9302",</div><div class="line">      "host" : "127.0.0.1",</div><div class="line">      "ip" : [ "127.0.0.1:9302", "NONE" ],</div><div class="line">      "indices" : &#123;</div><div class="line">        "docs" : &#123;</div><div class="line">          "count" : 7240861,</div><div class="line">          "deleted" : 257</div><div class="line">        &#125;,</div><div class="line">        "store" : &#123;</div><div class="line">          "size_in_bytes" : 1836976476,</div><div class="line">          "throttle_time_in_millis" : 0</div><div class="line">        &#125;,</div><div class="line">        ...</div><div class="line">      "fs" : &#123;</div><div class="line">        "timestamp" : 1470310262388,</div><div class="line">        "total" : &#123;</div><div class="line">          "total_in_bytes" : 316934193152,</div><div class="line">          "free_in_bytes" : 32878755840,</div><div class="line">          "available_in_bytes" : 16755851264</div><div class="line">        &#125;,</div><div class="line">        "data" : [ &#123;</div><div class="line">          "path" : "/data2/active2/dji-active/nodes/0",</div><div class="line">          "mount" : "/data2 (/dev/xvdf)",</div><div class="line">          "type" : "ext4",</div><div class="line">          "total_in_bytes" : 316934193152,</div><div class="line">          "free_in_bytes" : 32878755840,</div><div class="line">          "available_in_bytes" : 16755851264,</div><div class="line">          "spins" : "false"</div><div class="line">        &#125; ]</div><div class="line">      &#125;,</div><div class="line">      "transport" : &#123;</div><div class="line">        "server_open" : 0,</div><div class="line">        "rx_count" : 30,</div><div class="line">        "rx_size_in_bytes" : 8193,</div><div class="line">        "tx_count" : 36,</div><div class="line">        "tx_size_in_bytes" : 13202</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>状态比较多，这里挑几个说一下，首先是 <code>gc</code> 部分，显示的是 young 和 old gc 的耗时，一般来说 young 会比较大，这是正常的。一次 young gc 大概在 1-2ms，old gc 在 100 ms 左右，如果有量级上的差距，建议打开 slow-gc 日志，具体研究原因。</p>
<p><code>thread_pool</code> 是线程池信息，我们主要看 <code>rejected</code> 的数据，如果这个数值很大，就说明 ES 忙不过来了。</p>
<p>其他的基本就是系统和文件系统的数据如果 <code>fielddata_breaker.tripped</code> 数值太高，那么就需要优化了。</p>
<p>其他一些监控接口</p>
<ul>
<li><code>hot_threads</code> 状态 <code>curl -XGET &#39;http://127.0.0.1:9200/_nodes/_local/hot_threads?interval=60s&#39;</code></li>
<li>等待执行的任务列表 <code>curl -XGET http://127.0.0.1:9200/_cluster/pending_tasks{ &quot;tasks&quot;: [] }</code></li>
<li>可以用 <code>/_cat</code> 接口，具体参考文档<ul>
<li>集群状态 <code>curl -XGET http://127.0.0.1:9200/_cat/health?v</code></li>
<li>节点状态 <code>curl -XGET http://127.0.0.1:9200/_cat/nodes?v</code> </li>
</ul>
</li>
</ul>
<p>Elasticsearch 的日志在 <code>$ES_HOME/logs/</code> 中，或者可以使用 官方自己的监控工具 - marvel。如果在生产环境中，最好使用 nagios, zabbix, ganglia, collectd 这类监控系统。</p>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="合理计划服务器"><a href="#合理计划服务器" class="headerlink" title="合理计划服务器"></a>合理计划服务器</h3><p>在 Elasticsearch 的配置文件中，可以根据两个配置(<code>node.master</code> 和 <code>node.data</code>)选项来分配不同节点的角色，以达到提高服务器性能的目的。</p>
<ul>
<li><code>node.master: false; node.data: true</code> - 该节点只作为数据节点，用于存储和查询，资源消耗会较低</li>
<li><code>node.master: true; node.data: false</code> - 该节点只作为 master 节点，不存储数据，主要负责协调索引请求和查询请求</li>
<li><code>node.master: false; node.data: falst</code> - 该节点不作为 master 节点，也不存储数据，主要用于查询时的负载均衡（做结果汇总等工作）</li>
</ul>
<p>另外，一台服务器最好只部署一个节点以维持服务器稳定，毕竟资源是有限的，多开也没啥</p>
<h3 id="数据节点就是数据节点"><a href="#数据节点就是数据节点" class="headerlink" title="数据节点就是数据节点"></a>数据节点就是数据节点</h3><p>如果有配置数据节点，那么可以关闭其 http 功能，让它专注于索引的操作。插件之类的也最好安装到非数据节点服务器上，这样是一个兼顾数据安全和服务器性能的考虑。具体的配置项是 <code>http.enabled: false</code></p>
<h3 id="线程池配置"><a href="#线程池配置" class="headerlink" title="线程池配置"></a>线程池配置</h3><p>针对 Elasticsearch 的不同操作，可以配置不同大小的线程池，这个需要根据业务需求确定最佳值，场景的操作有：index, search, suggest, get, bulk, percolate, snapshot, snapshot_data, warmer, refresh。</p>
<p>这里以 index(创建/更新/删除索引数据)和 search(搜索操作)为例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">threadpool:</div><div class="line">     index:</div><div class="line">         type: fixed</div><div class="line">         size: 24（逻辑核心数*3）</div><div class="line">         queue_ size: 1000</div><div class="line"> </div><div class="line">     search:</div><div class="line">         type: fixed</div><div class="line">         size: 24（逻辑核心数*3）</div><div class="line">         queue_ size: 1000</div></pre></td></tr></table></figure>
<h3 id="分片与副本"><a href="#分片与副本" class="headerlink" title="分片与副本"></a>分片与副本</h3><p>默认的参数是 5 个分片(shard)和 1 个副本(replica)，碎片数目越多，索引速度越快；副本数目越多，搜索能力及可用性更高。分片的数目是在一开始就设定好的，但是副本的数目是可以后期修改的。</p>
<p>而在恢复数据的时候，可以先减少分片刷新索引的时间间隔，如</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">curl -XPUT <span class="string">'http://10.1.1.0:9200/_settings'</span> <span class="_">-d</span> <span class="string">'&#123; </span></div><div class="line">    "index" : &#123; </div><div class="line">        "refresh_interval" : "-1" </div><div class="line">    &#125; </div><div class="line">&#125;'</div></pre></td></tr></table></figure>
<p>完成插入之后再恢复</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">curl -XPUT <span class="string">'http://10.1.1.0:9200/_settings'</span> <span class="_">-d</span> <span class="string">'&#123; </span></div><div class="line">    "index" : &#123; </div><div class="line">        "refresh_interval" : "1s" </div><div class="line">    &#125; </div><div class="line">&#125;'</div></pre></td></tr></table></figure>
<h3 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h3><p>查询中最重要的思路就是 routing，尽量减少慢查询的次数。而当索引越来越大的时候，每个分片也会增大，查询速度就会变慢。一个可行的解决思路就是分索引，比方说不同类型的数据利用不同的 routing 进行分离。</p>
<p>还有一个从业务出发的思路，就是不索引不需要的字段，这样就可以减小集群所需资源的量。</p>
<h3 id="JVM-设置"><a href="#JVM-设置" class="headerlink" title="JVM 设置"></a>JVM 设置</h3><p>关于 JVM 的设置我还在摸索中，不过有几个技巧：</p>
<ul>
<li>JVM 的堆大小不要超过 32G，来源 <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops" target="_blank" rel="external">Don’t Cross 32 GB!</a></li>
<li>使用 <code>bootstrap.mlockall: true</code>，启动时就锁定内存</li>
<li>用较小的 heapsize 配合 SSD</li>
</ul>
<h3 id="Full-GC-问题"><a href="#Full-GC-问题" class="headerlink" title="Full GC 问题"></a>Full GC 问题</h3><p>这里以一个实例来介绍我是如何在生产环境中排查和修复 Elasticsearch 集群忽然响应时间剧增的问题的。</p>
<p>情况是这样的，随着接入 Elasticsearch 的数据量增大，忽然有一个周末出问题了 - ES 集群的查询和插入都变得巨慢无比。监控报警都把邮箱和手机发爆炸了。</p>
<p>那么问题来了，究竟是哪里出了乱子？</p>
<p>因为发送数据的客户端和服务器近期并没有特别大的改动，我检查了 Kafka 队列也一切正常，于是可以锁定问题出在 Elasticsearch 身上。</p>
<p>第一反应就是先去看 Elasticsearch 的日志，发现根据日志显示，一致在不停的垃圾回收。因此对症下药，把 JVM 的堆内存改大。但是在集群重启之后仍然会出现性能急剧下降的状况，于是继续检查日志，发现是因为 JVM 进行 Full GC 的时间过长，导致 ES 集群认为拓扑结构改变，开始迁移数据所导致。而迁移数据本身又会导致 Full GC，让情况更糟的是，在 Full GC 结束之后，集群的拓扑结构又再次改变，于是就陷入了这样的死循环。</p>
<p>破局的方法其实非常简单粗暴，把检测集群拓扑的时间间隔和超时次数加大一点，留足够的时间给 JVM 进行 Full GC 即可。</p>
<h3 id="导入数据过慢问题"><a href="#导入数据过慢问题" class="headerlink" title="导入数据过慢问题"></a>导入数据过慢问题</h3><p>最近在从 MySQL 数据库中导入大量数据到 Elasticsearch 的时候，出现写入极其缓慢，甚至在使用了 bulk（批量）接口之后也没有改善的问题。奇怪的是，从 MySQL 的表 A 和表 B 中导入甚至会有几十倍的速度差距，这是为什么呢？</p>
<p>经过一步一步排查，基本上 ES 的文档和可以配置的参数都调整过之后并没有改善，于是开始从数据源入手，最后发现表 A 和 表 B 的数据顺序是不太一样的。表 A 中基本是顺序递增的数据，主键（自增长 ID）基本对应于时间顺序；而表 B 中则基本是随机插入的，所以按照数据库中的 ID 进行顺序导出，就会发现相邻记录对应的日期可能相差很大，而正好我们在 ES 中又是根据日期来进行索引的切割的，导致每次都需要在不同的索引中进行切换，速度自然上不去。</p>
<p>所以我们把从 MySQL 数据库中选择数据的语句利用 timestamp 作为 order by 的标准，导入速度就很快了。</p>
<p>这里有一点需要注意每次除了 ID 之外，还需要记录 timestamp 的值，这样才能保证是顺序导入的 <code>where id &gt; xxx and timestamp &gt; xxx</code>，其中 timestamp 每次需要归 0。</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><ol>
<li>通过 JVM 命令查看 Elasticsearch 的运行状况</li>
<li>尝试不同的检索，通过 Profile API 来判断到底是哪一步最耗时</li>
<li>有没有办法快速重启 Elasticsearch 集群，尽量减少分区恢复的时间？</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Elasticsearch 的调优其实是一个玄学问题，这里的建议是直接把 SSD 和 内存申请够，不然出现各种奇葩问题非常头疼。好消息是，一旦稳定下来，Elasticsearch 还是非常好用的，所以学会这一套，性价比是不低的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面我们已经把系统搭建完成，但是具体的应用都比较简单。这一次我们来详细了解一下 Elasticsearch，尤其是如何利用其内置的各种强大功能来完成我们的需求。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="Elasticsearch" scheme="http://wdxtub.com/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】柒 Kibana 技巧指南</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/</id>
    <published>2016-11-19T03:11:04.000Z</published>
    <updated>2016-11-24T13:33:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面我们已经把系统搭建完成，但是具体的应用都比较简单。这一次我们来详细了解一下 Kibana，看看如何把数据可视化弄得更加绚丽一些，毕竟人靠衣装嘛。</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<ul>
<li>2016.11.24: 完成初稿（具体介绍后面会陆续添加）</li>
</ul>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><h2 id="老介绍"><a href="#老介绍" class="headerlink" title="老介绍"></a>老介绍</h2><p>Kibana3 和 Kibana4 基本还处于并行的状态（想到了 Python），这里主要介绍 Kibana4（因为主要在用这个版本）</p>
<p>任何需要展示的数据都需要现在 Settings 中进行索引配置，注意可以选择配置时间索引，这样在 Discover 页面会多出来时间的选项。默认情况下，Discover 页面会显示匹配搜索条件的前 500 个文档。Visualization 用来为搜索结果做可视化。每个可视化都是跟一个搜索关联着的。Dashboard 可以创建定值自己的仪表盘。</p>
<p>要应用到生产环境的话，具体对于 Nginx, shield 和 SSL 的配置请参考官方文档。使用 Shield 的话，可以做到索引级别的访问控制，这对多团队管理很有帮助。</p>
<h3 id="Discover"><a href="#Discover" class="headerlink" title="Discover"></a>Discover</h3><p>Discover 标签用于交互式探索数据。基本上常用的功能应有尽有，具体就要自己慢慢摸索。</p>
<ul>
<li>右上角的时间过滤器、中间的直方图都可以选择时间范围</li>
<li>搜索的时候可以使用 Lucene 查询语法，可以用完整的基于 JSON 的 Elasticsearch 查询 DSL</li>
<li>按字段过滤包含正反两种过滤器，尝试一下即可</li>
<li>JSON 中可以灵活应用 bool query 组合中各种 <code>should</code>, <code>must</code>, <code>must not</code> 条件</li>
<li>可以使用任何已建立索引的字段排序文档表哥中的数据。如果当前索引模式配置了时间字段，默认会使用该字段倒序排列文档</li>
</ul>
<h3 id="Visualize"><a href="#Visualize" class="headerlink" title="Visualize"></a>Visualize</h3><p>几个不同的大类</p>
<ul>
<li>Area chart: 用区块图来可视化多个不同序列的总体共享</li>
<li>Data table: 用数据表来显示聚合的原始数据。其他可视化可以通过点击底部的方式显示数据表</li>
<li>Line char: 用折线图来比较不同序列</li>
<li>Markdown widget: 用 Markdown 显示自定义格式的信息或和仪表盘有关的用法说明</li>
<li>Metric: 用指标在仪表盘上显示单个数字</li>
<li>Pie char: 用饼图来显示每个来源对总体的贡献</li>
<li>Tile map: 用瓦片地图将聚合结果和经纬度联系起来</li>
<li>Vertical bar chart: 用垂直条形图作为一个通用图形</li>
</ul>
<p>Y 轴的数值维度有以下聚合：</p>
<ul>
<li>Count 原始计数</li>
<li>Average 平均值</li>
<li>Sum 总和</li>
<li>Min 最小值</li>
<li>Max 最大值</li>
<li>Unique Count 不重复的值</li>
<li>Standard Deviation 标准差</li>
<li>Percentile 百分比</li>
<li>Percentile Rank 百分比排名</li>
</ul>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>Kibana 服务器在启动的时候会从 <code>kibana.yml</code> 文件读取属性。常见的属性有</p>
<ul>
<li><code>port</code></li>
<li><code>host</code></li>
<li><code>elasticsearch_url</code></li>
<li><code>kibana_index</code></li>
<li><code>default_app_id</code></li>
<li><code>request_timeout</code></li>
<li><code>shard_timeout</code></li>
<li><code>verify_ssl</code></li>
<li><code>ca</code></li>
<li><code>ssl_key_file</code></li>
<li><code>ssl_cert_file</code></li>
<li><code>pid_file</code></li>
</ul>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面我们已经把系统搭建完成，但是具体的应用都比较简单。这一次我们来详细了解一下 Kibana，看看如何把数据可视化弄得更加绚丽一些，毕竟人靠衣装嘛。&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="Kibana" scheme="http://wdxtub.com/tags/Kibana/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】捌 实例：接入外部应用日志</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/</id>
    <published>2016-11-19T03:11:03.000Z</published>
    <updated>2016-11-24T13:33:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>施工中：简单的接口访问次数统计和 IP 统计，爬虫与博客统计</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><h2 id="自动运行"><a href="#自动运行" class="headerlink" title="自动运行"></a>自动运行</h2><p>通常来说，我们需要 logstash 在后台长期运行，否则每次需要去各台机器上手动操作，会很麻烦。</p>
<p>有两种方法，一种是配合 <code>crontab</code>，定期执行指定命令，另一种是让 logstash 以服务或者守护进程的形式运行，配合配置文件中的 schedule 即可。其中 <code>crontab</code> 的方法可以参阅 <a href="http://wdxtub.com/2016/07/26/crontab-guide/">Crontab 指南</a>，这里主要介绍另外四种方法。</p>
<p><strong>方法一：标准的 service 方式</strong></p>
<p>在 <code>/etc/init.d/logstash</code> 脚本中，会加载 <code>/etc/init.d/functions</code> 库文件，利用其中的 daemon 函数，将 logstash 进程作为后台程序运行。</p>
<p>我们要做的是把配置文件都放到 <code>/etc/logstsh/</code> 目录下，必须以 <code>.conf</code> 结尾，然后我们执行 <code>service logstash start</code> 即可（注意要在配置文件中设定好 schedule，这样就可以按照要求自动执行了）</p>
<p><strong>方法二：nohup 方式</strong></p>
<p>简单来说，一句话就可以搞定，如果想让某命令在后台长期运行，需要在命令前加 <code>nohup</code>，后面加 <code>&amp;</code></p>
<p><strong>方法三：用 tmux/screen</strong></p>
<p>一般来说，如果我需要让服务器跑一堆命令又不想挂着 ssh 连接的话，直接用 tmux/screen 运行命令即可，这样即使退出，命令也依然在执行，具体的使用可以参考 <a href="./2016/03/30/tmux-guide/">tmux 指南</a></p>
<p><strong>方法四：daemontools 方式</strong></p>
<p>如果需要长期在后台运行大量程序，建议使用 daemontools 工具，可以通过配置文件来管理操作程序，类似于自动化的 tmux，比方说 python 实现的 <code>supervisord</code>，perl 实现的 <code>ubic</code> 或者 ruby 实现的 <code>god</code>，具体的用法之后会写日志进行说明</p>
<h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://www.gitbook.com/book/chenryn/kibana-guide-cn/details" target="_blank" rel="external">ELKstack 中文指南</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;施工中：简单的接口访问次数统计和 IP 统计，爬虫与博客统计&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="实例" scheme="http://wdxtub.com/tags/%E5%AE%9E%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>【通天塔之日志分析平台】玖 业界：大厂实践</title>
    <link href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/"/>
    <id>http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/</id>
    <published>2016-11-19T03:11:02.000Z</published>
    <updated>2016-11-20T15:34:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>施工中：看各个大厂是如何做这个事情的</p>
<a id="more"></a>
<hr>
<p>更新历史</p>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><ul>
<li><a href="http://wdxtub.com/2016/11/19/babel-series-intro/">『通天塔』技术作品合集介绍</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-0/">零 系列简介与环境配置</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-1/">壹 ELK 环境搭建</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-2/">贰 Kafka 缓冲区</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-3/">叁 监控、安全、报警与通知</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-4/">肆 从单机到集群</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-5/">伍 Logstash 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-6/">陆 Elasticsearch 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-7/">柒 Kibana 技巧指南</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-8/">捌 实例：接入外部应用日志</a></li>
<li><a href="http://wdxtub.com/2016/11/19/babel-log-analysis-platform-9/">玖 业界：大厂实践</a></li>
</ul>
<h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><h2 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;施工中：看各个大厂是如何做这个事情的&lt;/p&gt;
    
    </summary>
    
      <category term="Technique" scheme="http://wdxtub.com/categories/Technique/"/>
    
    
      <category term="通天塔" scheme="http://wdxtub.com/tags/%E9%80%9A%E5%A4%A9%E5%A1%94/"/>
    
      <category term="日志" scheme="http://wdxtub.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="平台" scheme="http://wdxtub.com/tags/%E5%B9%B3%E5%8F%B0/"/>
    
      <category term="业界" scheme="http://wdxtub.com/tags/%E4%B8%9A%E7%95%8C/"/>
    
  </entry>
  
  <entry>
    <title>第二十三周 - 少年壮志不言愁</title>
    <link href="http://wdxtub.com/2016/11/18/youth-dream/"/>
    <id>http://wdxtub.com/2016/11/18/youth-dream/</id>
    <published>2016-11-18T11:20:51.000Z</published>
    <updated>2016-11-18T15:15:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>几度风雨几度春秋，风霜雪雨博激流。历尽苦难痴心不改，少年壮志不言愁。</p>
<a id="more"></a>
<hr>
<p>这周发生的事情有点多，多到我已经有点记不太清楚顺序。在回家的动车上写周记，就写到哪儿是哪儿吧。如果要一句话总结的话就是：我很满意，我已经使出了洪荒之力!!!</p>
<p>周一去看了传说中的『超级月亮』，虽然兴致勃勃带着新买的相机，但是发现镜头吃紧并没有办法拍出像样的月亮，于是『赏月』之旅变成了『自拍』练习。遗憾的是在暗光场景下我对相机的使用简直直逼『智障』，只能恬不知耻地说一句：技术不够颜值来凑。</p>
<p>周二迎来了公司篮球赛小组赛的最后一场，带着两战皆负提前出局的心态，我们部门的六个人迎来了最后一场的荣誉之战。大家没有看错，就是六个人的球队要打全场。前三节比分紧紧咬住甚至还略微有些领先，最后一节我们深陷犯规泥潭。早早领到第四次犯规的我（五次就罚下场）防守基本靠眼神和吼叫，最后一次快攻后我的得分定格在 12 分，最终我们以 19 比 23 败北。但是这已经是我们得分最多且失分最少的一场了，大家能一起在场上奔跑，本身就是很美好的事情，只是希望以后部门的活动大家都能参与进来，不能聚餐积极打球就当逃兵啊。</p>
<p>写到这儿忽然想起来周末沿着海边跑了 13 公里时候的感觉，不轻松但是特别畅快。生活中的很多感悟，其实都能在一次跑步中一次兑现。从一开始的斗志满满到临近极限的艰难再到突破自己的喜悦，不由得想，原地踏步的人看不到新的风景，眼里只有终点的人无暇欣赏一路的风景，只有方向坚定但是又不拘泥于此的人才能真正享受探索和发现的旅程。</p>
<p>前两天博客的访问量突破 20 万了，因为是在 10 万的时候才记下了日子，所以只知道第二个十万花了 74 天，希望第三个十万能少花几天（慢慢进步，我不贪心）。这个周末终于腾出了一点时间来打理一下博客，把几个系列都开了出来：</p>
<ul>
<li>小土刀玩摄影：用来记录我学习摄影的经历</li>
<li>小土刀的剪报本：用来取代之前的『一周读报』系列，收集我平时看到的比较有价值的信息</li>
</ul>
<p>周末应该会把最近构造日志系统的各种经验汇总成一个新的系列，把之前若干分散的文章整合成一个可操作易上手的系列，争取让读者看完这个系列之后也能快速搭建起这样一套集中化日志监控系统。</p>
<p>之所以想要把之前的技术文章系列化作品化，其实也是受机械工业出版社华章分社（学计算机的应该非常熟悉了）温姚二位老师的启发和影响。回想半年前因创作《读厚/读薄 CSAPP》系列结缘，到今天拿到还未上市的 CSAPP 第三版中文版，能和二位奋斗在计算机教育第一线的先锋聊聊人生聊聊理想，真的倍感荣幸，也确实的感受到，自己能为这片我深爱的土地和在这之上生活着的人们做些什么。期间聊到计算机基础教育的现状，不由得提到了母校中山大学。作为和 CMU 联合办学的第一批走向社会的学生，真的是非常幸运，赶上了『好时候』。曾经软件学院的李文军院长其实一直非常注重培养学生的软硬结合能力，尤其在 CMU 进行了深入学习之后，才更能够理解院长的一片苦心。可惜的是最近因为各种架构和人员的调整，风雨飘摇，原先计划的若干合作也不得不转为其他的形式。</p>
<p><img src="/images/14794821052176.jpg" alt=""></p>
<p>深入理解计算机系统（即 CSAPP）是一本非常值得每个计算机软硬件的同学学习的教材，二位老师和我都希望能尽自己的努力，想各种办法弥补暂时和国外一流大学师资上的差距，让更多想要迈入计算机学科大门的同学，能有一个更好的起跑。这里先偷偷剧透一下，之后可能会联合北大清华复旦大学的教授以在线公开课或者学习社区的形式来和大家一起学习 CSAPP，我也会以『助教』的角色陪大家一起成长。欢迎更多对此感兴趣的同学能够参与进来，毕竟少年强则国强嘛。</p>
<p>最后说说工作，近一个月来一直在开发的保密项目终于完成了核心系统的开发，一个人把产品经理项目经理架构师前端后端运维的事儿都做了个遍（后来有了小伙伴轻松了不少），算是强行把前端的基本技能给点亮了。临下班前跟二老板说了一下自己之后的想法，因为保密项目的开发接近尾声，后面会空出来一些时间，会继续把 UTM 和数据平台的事情做起来。不过对我来说最大的收获反而算是第一次真正跟二老板沟通自己的想法，让老大知道自己在做什么，还是很重要滴。</p>
<p>在一个飞速发展的公司，能够参与到最前沿的工作，这种没有任何攻略的挑战，其实是非常刺激的。我可能不是一个愿意在轨道上安安稳稳前进的人，我不想要有这样一条轨道，我想要尝试各种可能。在老板眼皮底下干活，逼着自己完成工作的同时，眼界要更宽，思维要更严密。虽然谈不上轻松，但是被逼迫着快速成长，也是很有意思的经历。再说了，我也相信自己一定能搞定。</p>
<p>峥嵘岁月，何惧风流，危难之处显身手。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;几度风雨几度春秋，风霜雪雨博激流。历尽苦难痴心不改，少年壮志不言愁。&lt;/p&gt;
    
    </summary>
    
      <category term="Gossip" scheme="http://wdxtub.com/categories/Gossip/"/>
    
    
      <category term="周记" scheme="http://wdxtub.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="工作" scheme="http://wdxtub.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
</feed>
